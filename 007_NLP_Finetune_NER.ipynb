{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 NER Fine-Tuning\n",
    "## (part of Lab 2)\n",
    "\n",
    "In this notebook, you'll use the NVIDIA TAO (Train, Adapt, and Optimize) Toolkit to fine-tune a [BERT](https://arxiv.org/abs/1810.04805)-based model for a named entity recognition (NER) task in a restaurant context using the [MIT Restaurant Corpus](https://groups.csail.mit.edu/sls/downloads/restaurant) dataset. To do so, you will use the [Token Classification](https://docs.nvidia.com/metropolis/TAO/tao-user-guide/text/nlp/token_classification.html) task in TAO.\n",
    "\n",
    "**[7.1 Named Entity Recognition](#7.1-Named-Entity-Recognition)<br>**\n",
    "**[7.2 TAO Toolkit `token_classification` Task](#7.2-TAO-Toolkit-token_classification-Task)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.2.1 Path Setup](#7.2.1-Path-Setup)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.2.2 Specification Files](#7.2.2-Specification-Files)<br>\n",
    "**[7.3 General NER Inference](#7.3-General-NER-Inference)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.3.1 NER Inference with a GMB Context](#7.3.1-NER-Inference-with-a-GMB-Context)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.3.2 Exercise: NER Inference with a Restaurant Context](#7.3.2-Exercise:-NER-Inference-with-a-Restaurant-Context)<br>\n",
    "**[7.4 NER Training](#7.4-NER-Training)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.4.1 Restaurant Data Exploration](#7.4.1-Restaurant-Data-Exploration)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.4.2 `train` Command](#7.4.2-train-Command)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.4.3 Faster Training with AMP](#7.4.3-Faster-Training-with-AMP)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.4.4 Change the Language Model](#7.4.4-Change-the-Language-Model)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.4.5 Evaluate the Trained Model](#7.4.5-Evaluate-the-Trained-Model)<br>\n",
    "**[7.5 NER Fine-Tuning](#7.5-NER-Fine-Tuning)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.5.1 Exercise: Evaluate the Fine-Tuned Model](#7.5.1-Exercise:-Evaluate-the-Fine-Tuned-Model)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.5.2 Inference on the Fine-Tuned Model](#7.5.2-Inference-on-the-Fine-Tuned-Model)<br>\n",
    "**[7.6 Export for Deployment](#7.6-Export-for-Deployment)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[7.6.1 Exercise: NER Model Export to ONNX](#7.6.1-Exercise:-NER-Model-Export-to-ONNX)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "The steps in this notebook assume that you have:\n",
    "\n",
    "1. **NGC Credentials**<br>Be sure you have added your NGC credential as described in the [NGC Setup notebook](003_Intro_NGC_Setup.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                             COMMAND       CREATED              STATUS              PORTS     NAMES\n",
      "bedf6b714a3c   nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3   \"/bin/bash\"   About a minute ago   Up About a minute             brave_curie\n",
      "0bb3cab74930   nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3   \"/bin/bash\"   About a minute ago   Up About a minute             flamboyant_goodall\n"
     ]
    }
   ],
   "source": [
    "# Check running docker containers. This should be empty.\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedf6b714a3c\n",
      "0bb3cab74930\n",
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# If not empty, clear Docker containers\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7.1 Named Entity Recognition\n",
    "\n",
    "NER, also referred to as entity chunking, identification, token classification, or extraction, is the task of detecting and classifying key information (entities) in text.  In the general example you used for the Riva Contact app, the entities classified were person, location, organization, time, and miscellaneous. \n",
    "For example, in a sentence: `Mary lives in Santa Clara and works at NVIDIA`, we should detect that `Mary` is a person, `Santa Clara` is a location and `NVIDIA` is an organization.\n",
    "\n",
    "Using TAO, we can train a new model it to recognize different entities, such as cuisine, dish, hours, or restaurant_name for a new domain context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `tao info`, review the tasks TAO can perform.  In our ASR examples we used the `speech_to_text` task.  For NER, we will use the `token_classification` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia_tao in /usr/local/lib/python3.8/dist-packages (0.1.24)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from nvidia_tao) (0.8.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from nvidia_tao) (1.15.0)\n",
      "Requirement already satisfied: docker==4.3.1 in /usr/local/lib/python3.8/dist-packages (from nvidia_tao) (4.3.1)\n",
      "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/local/lib/python3.8/dist-packages (from docker==4.3.1->nvidia_tao) (2.24.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from docker==4.3.1->nvidia_tao) (0.57.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests!=2.18.0,>=2.14.2->docker==4.3.1->nvidia_tao) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests!=2.18.0,>=2.14.2->docker==4.3.1->nvidia_tao) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests!=2.18.0,>=2.14.2->docker==4.3.1->nvidia_tao) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests!=2.18.0,>=2.14.2->docker==4.3.1->nvidia_tao) (2020.6.20)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nvidia_tao --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of the TAO Toolkit Instance\n",
      "\n",
      "dockers: \t\t\n",
      "\tnvidia/tao/tao-toolkit-tf: \t\t\t\n",
      "\t\tv3.22.05-tf1.15.5-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. augment\n",
      "\t\t\t\t2. bpnet\n",
      "\t\t\t\t3. classification\n",
      "\t\t\t\t4. dssd\n",
      "\t\t\t\t5. faster_rcnn\n",
      "\t\t\t\t6. emotionnet\n",
      "\t\t\t\t7. efficientdet\n",
      "\t\t\t\t8. fpenet\n",
      "\t\t\t\t9. gazenet\n",
      "\t\t\t\t10. gesturenet\n",
      "\t\t\t\t11. heartratenet\n",
      "\t\t\t\t12. lprnet\n",
      "\t\t\t\t13. mask_rcnn\n",
      "\t\t\t\t14. multitask_classification\n",
      "\t\t\t\t15. retinanet\n",
      "\t\t\t\t16. ssd\n",
      "\t\t\t\t17. unet\n",
      "\t\t\t\t18. yolo_v3\n",
      "\t\t\t\t19. yolo_v4\n",
      "\t\t\t\t20. yolo_v4_tiny\n",
      "\t\t\t\t21. converter\n",
      "\t\tv3.22.05-tf1.15.4-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. detectnet_v2\n",
      "\tnvidia/tao/tao-toolkit-pyt: \t\t\t\n",
      "\t\tv3.22.05-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. speech_to_text\n",
      "\t\t\t\t2. speech_to_text_citrinet\n",
      "\t\t\t\t3. speech_to_text_conformer\n",
      "\t\t\t\t4. action_recognition\n",
      "\t\t\t\t5. pointpillars\n",
      "\t\t\t\t6. pose_classification\n",
      "\t\t\t\t7. spectro_gen\n",
      "\t\t\t\t8. vocoder\n",
      "\t\tv3.21.11-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. text_classification\n",
      "\t\t\t\t2. question_answering\n",
      "\t\t\t\t3. token_classification\n",
      "\t\t\t\t4. intent_slot_classification\n",
      "\t\t\t\t5. punctuation_and_capitalization\n",
      "\tnvidia/tao/tao-toolkit-lm: \t\t\t\n",
      "\t\tv3.22.05-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. n_gram\n",
      "format_version: 2.0\n",
      "toolkit_version: 3.22.05\n",
      "published_date: 05/25/2022\n"
     ]
    }
   ],
   "source": [
    "# Check the token_classification capability in your TAO version\n",
    "!tao info --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7.2 TAO Toolkit `token_classification` Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [token_classification](https://docs.nvidia.com/tao/tao-toolkit/text/nlp/token_classification.html) task provides commands to run data preprocessing, training, fine-tuning, evaluation, inference, and export. All configurations happen through YAML spec files. The `tao token_classification --help` usage information output is as follows:\n",
    "\n",
    "```\n",
    "usage: token_classification [-h] -r RESULTS_DIR [-k KEY]\n",
    "                            [-e EXPERIMENT_SPEC_FILE] [-g GPUS]\n",
    "                            [-m RESUME_MODEL_WEIGHTS] [-o OUTPUT_SPECS_DIR]\n",
    "                            {dataset_convert,evaluate,export,finetune,infer,infer_onnx,train,download_specs}\n",
    "\n",
    "Train Adapt Optimize Toolkit\n",
    "\n",
    "positional arguments:\n",
    "  {dataset_convert,evaluate,export,finetune,infer,infer_onnx,train,download_specs}\n",
    "                        Subtask for a given task/model.\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -r RESULTS_DIR, --results_dir RESULTS_DIR\n",
    "                        Path to a folder where the experiment outputs should\n",
    "                        be written. (DEFAULT: ./)\n",
    "  -k KEY, --key KEY     User specific encoding key to save or load a .tlt\n",
    "                        model.\n",
    "  -e EXPERIMENT_SPEC_FILE, --experiment_spec_file EXPERIMENT_SPEC_FILE\n",
    "                        Path to the experiment spec file.\n",
    "  -g GPUS, --gpus GPUS  Number of GPUs to use. The default value is 1.\n",
    "  -m RESUME_MODEL_WEIGHTS, --resume_model_weights RESUME_MODEL_WEIGHTS\n",
    "                        Path to a pre-trained model or model to continue\n",
    "                        training.\n",
    "  -o OUTPUT_SPECS_DIR, --output_specs_dir OUTPUT_SPECS_DIR\n",
    "                        Path to a target folder where experiment spec files\n",
    "                        will be downloaded.\n",
    "```                        \n",
    "\n",
    "This should look pretty familiar as it is almost identical in form to the `speech_to_text` task!  As before, additional arguments can be added to the end of the command to override values in the spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.1 Path Setup\n",
    "\n",
    "Define some folder locations and an encryption key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from shutil import rmtree\n",
    "\n",
    "# The source mount is our workspace on the host (this lab instance)\n",
    "source_mount = \"/dli/task/tao\"\n",
    "# The destination mount is our mapped workspace within the TAO docker container's file structure\n",
    "destination_mount = \"/workspace/mount\"\n",
    "\n",
    "# The following paths are set relative to the TAO docker container\n",
    "# The path to the specification yaml files\n",
    "SPECS_DIR=os.path.join(destination_mount, 'specs')\n",
    "\n",
    "# The results are saved at this path by default\n",
    "RESULTS_DIR=os.path.join(destination_mount, 'results')\n",
    "\n",
    "# The data are located at this path by default\n",
    "DATA_DIR=os.path.join(destination_mount, 'data')\n",
    "\n",
    "# The models are located at this path by default\n",
    "MODELS_DIR=os.path.join(destination_mount, 'models')\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands. Please use \"tlt_encode\" if you'd like to deploy the models later with NVIDIA Riva.\n",
    "KEY='tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.2 Specification Files\n",
    "Fetch the example specification YAML files for the `token_classification` task. We can load example files with the [download_specs subtask](https://docs.nvidia.com/tao/tao-toolkit/text/nlp/token_classification.html#downloading-sample-spec-files), then modify them or override them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:10:26,253 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:10:26,399 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:10:26,436 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:10:31 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:10:31 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:10:31 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:10:31 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:10:31 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:10:31 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:10:32 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:10:32 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:10:32 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2022-06-22 12:10:35 tlt_logging:20] Experiment configuration:\n",
      "    exp_manager:\n",
      "      task_name: download_specs\n",
      "      explicit_log_dir: /workspace/mount/results\n",
      "    source_data_dir: /opt/conda/lib/python3.8/site-packages/nlp/token_classification/experiment_specs\n",
      "    target_data_dir: /workspace/mount/specs/token_classification\n",
      "    workflow: nlp\n",
      "    \n",
      "[NeMo W 2022-06-22 12:10:35 exp_manager:26] Exp_manager is logging to `/workspace/mount/results``, but it already exists.\n",
      "[NeMo I 2022-06-22 12:10:35 download_specs:73] Default specification files for nlp downloaded to '/workspace/mount/specs/token_classification'\n",
      "[NeMo I 2022-06-22 12:10:35 download_specs:74] Experiment logs saved to '/workspace/mount/results'\n",
      "\u001b[0m2022-06-22 12:10:36,353 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n",
      "CPU times: user 142 ms, sys: 29 ms, total: 171 ms\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, TAO takes about 3 minutes to load and run\n",
    "\n",
    "# Delete the token_classification specification directory if it already exists\n",
    "folder = source_mount + '/specs/token_classification'\n",
    "if os.path.exists(folder):\n",
    "    rmtree(folder)\n",
    "    \n",
    "# Download specification files for token_classification \n",
    "!tao token_classification download_specs \\\n",
    "    -o $SPECS_DIR/token_classification \\\n",
    "    -r $RESULTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7.3 General NER Inference\n",
    "\n",
    "For our general model, we will use the [Named Entity Recognition Bert Model](https://ngc.nvidia.com/catalog/models/nvidia:tlt-riva:namedentityrecognition_english_bert), a TAO compatible NER pretrained model available on NGC.\n",
    "\n",
    "The model was trained on the [Groningen Meaning Bank (GMB) corpus](https://gmb.let.rug.nl/) for entity recognition. The GMB dataset is a fairly large corpus with annotations. Note that GMB is not completely human-annotated and it is not considered 100% correct.  The following entity classes appear in the dataset:\n",
    "```\n",
    "LOC = Geographical Entity\n",
    "ORG = Organization\n",
    "PER = Person\n",
    "GPE = Geopolitical Entity\n",
    "TIME = Time indicator\n",
    "ART = Artifact           --|\n",
    "EVE = Event              --|-- combined as MISC\n",
    "NAT = Natural Phenomenon --|\n",
    "\n",
    "```\n",
    "For this model, the classes ART, EVE, and NAT were combined into a MISC class due to the small number of examples for these classes. \n",
    "This NER classifier achieves a 74.21 F1 macro score on the GMB dataset. The macro score computes the F1 score for each label and averages without taking any label imbalance into account. \\begin{array}{rcl} \\text{Macro F1-score} & = & \\frac{1}{N} \\sum_{i=0}^{N} {\\text{F1-score}_i} \\\\ \\end{array} where N the number of labels and i label index.\n",
    "\n",
    "This model is already available in the `tao/models/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/tao/models/namedentityrecognition_english_bert.tlt\n"
     ]
    }
   ],
   "source": [
    "# check model on /tao/models\n",
    "MODEL_DOWNLOAD_DIR=os.path.join(source_mount, 'models')\n",
    "!ls $MODEL_DOWNLOAD_DIR/namedentityrecognition_english_bert.tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3.1 NER Inference with a GMB Context\n",
    "\n",
    "We need to use the `tao token_classification infer` command for inference.  <br> The corresponding [infer.yaml](tao/specs/token_classification/infer.yaml) file is straightforward and includes some \"simulated\" user input: \n",
    "\n",
    "```yaml\n",
    "input_batch:\n",
    "  - 'We bought four shirts from the Nvidia gear store in Santa Clara.'\n",
    "  - 'Nvidia is a company.'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try querying the general NER model. Feel free to try out custom inputs as an exercise by changing the data and running the inference command again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:11:03,030 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:11:03,164 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:11:03,200 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:11:07 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:07 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:07 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:07 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:11:08 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:11:08 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:11:08 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:11:08 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:08 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:13 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:13 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:13 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:13 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:11:14 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:11:14 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:11:14 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:11:14 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:14 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:11:15 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/infer.py:84: UserWarning: \n",
      "    'infer.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:11:15 tlt_logging:20] Experiment configuration:\n",
      "    exp_manager:\n",
      "      task_name: infer\n",
      "      explicit_log_dir: /workspace/mount/results/bert-base/\n",
      "    restore_from: /workspace/mount/models/namedentityrecognition_english_bert.tlt\n",
      "    input_batch:\n",
      "    - We bought four shirts from the Nvidia gear store in Santa Clara.\n",
      "    - Nvidia is a company.\n",
      "    encryption_key: '*********'\n",
      "    \n",
      "[NeMo W 2022-06-22 12:11:15 exp_manager:26] Exp_manager is logging to `/workspace/mount/results/bert-base/``, but it already exists.\n",
      "[NeMo I 2022-06-22 12:11:18 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmp3r3zdneh/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 140043850391312 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 35.7kB/s]\n",
      "Lock 140043850391312 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 140043850197936 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 647kB/s]\n",
      "Lock 140043850197936 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 140043850686624 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 53.9MB/s]\n",
      "Lock 140043850686624 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140043850207824 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 74.4MB/s]\n",
      "Lock 140043850207824 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:11:18 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2022-06-22 12:11:18 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 140043780750496 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:04<00:00, 90.3MB/s]\n",
      "Lock 140043780750496 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:11:31 token_classification_dataset:121] Setting Max Seq length to: 17\n",
      "[NeMo I 2022-06-22 12:11:31 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-06-22 12:11:31 data_preprocessing:360] Min: 9 |                  Max: 17 |                  Mean: 13.0 |                  Median: 13.0\n",
      "[NeMo I 2022-06-22 12:11:31 data_preprocessing:366] 75 percentile: 15.00\n",
      "[NeMo I 2022-06-22 12:11:31 data_preprocessing:367] 99 percentile: 16.92\n",
      "[NeMo W 2022-06-22 12:11:31 token_classification_dataset:150] 0 are longer than 17\n",
      "[NeMo I 2022-06-22 12:11:31 token_classification_dataset:153] *** Example ***\n",
      "[NeMo I 2022-06-22 12:11:31 token_classification_dataset:154] i: 0\n",
      "[NeMo I 2022-06-22 12:11:31 token_classification_dataset:155] subtokens: [CLS] we bought four shirts from the n ##vid ##ia gear store in santa clara . [SEP]\n",
      "[NeMo I 2022-06-22 12:11:31 token_classification_dataset:156] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-06-22 12:11:31 token_classification_dataset:157] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-06-22 12:11:31 token_classification_dataset:158] subtokens_mask: 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0\n",
      "[NeMo I 2022-06-22 12:11:32 infer:76] Query  : We bought four shirts from the Nvidia gear store in Santa Clara.\n",
      "[NeMo I 2022-06-22 12:11:32 infer:77] Results: We bought four shirts from the Nvidia[B-ORG] gear store in Santa[B-LOC] Clara[I-LOC].\n",
      "[NeMo I 2022-06-22 12:11:32 infer:76] Query  : Nvidia is a company.\n",
      "[NeMo I 2022-06-22 12:11:32 infer:77] Results: Nvidia[B-ORG] is a company.\n",
      "[NeMo I 2022-06-22 12:11:32 infer:80] Experiment logs saved to '/workspace/mount/results/bert-base'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:11:34,089 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# TAO inference NER general model with text in the general domain\n",
    "!tao token_classification infer \\\n",
    "    -e $SPECS_DIR/token_classification/infer.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $MODELS_DIR/namedentityrecognition_english_bert.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/bert-base/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find results like the following towards the end of the output.  They are also available in the results log at [tao/results/bert-base/infer.log](tao/results/bert-base/infer.log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-06-22 11:41:31 infer:77] Results: We bought four shirts from the Nvidia[B-ORG] gear store in Santa[B-LOC] Clara[I-LOC].\n",
      "[NeMo I 2022-06-22 11:41:31 infer:77] Results: Nvidia[B-ORG] is a company.\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: I would like to order a pizza for 6pm[B-TIME]\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: what sauce in your burger .\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: mhh nice food.\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: any good cheap german[B-GPE] restaurants nearby\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: any good ice cream parlors around\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: any good place to get a pie at an affordable price\n",
      "[NeMo I 2022-06-22 12:11:32 infer:77] Results: We bought four shirts from the Nvidia[B-ORG] gear store in Santa[B-LOC] Clara[I-LOC].\n",
      "[NeMo I 2022-06-22 12:11:32 infer:77] Results: Nvidia[B-ORG] is a company.\n"
     ]
    }
   ],
   "source": [
    "!grep Results $source_mount/results/bert-base/infer.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3.2 Exercise: NER Inference with a Restaurant Context\n",
    "Now try querying with sentences we might find in a restaurant context. Execute the following cell to populate a new YAML file,  `infer_restaurant.yaml`.  Then run NER inference as before and check the output.  What do you expect to see?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /dli/task/tao/specs/token_classification/infer_restaurant.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $source_mount/specs/token_classification/infer_restaurant.yaml\n",
    "\n",
    "# Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.\n",
    "# TAO Spec file for inference using a previously pretrained BERT model for a text classification task.\n",
    "\n",
    "# \"Simulate\" user input: batch with four samples.\n",
    "input_batch:\n",
    "  - \"I would like to order a pizza for 6pm\"\n",
    "  - \"what sauce in your burger .\"\n",
    "  - \"mhh nice food.\"\n",
    "  - \"any good cheap german restaurants nearby\"\n",
    "  - \"any good ice cream parlors around\"\n",
    "  - \"any good place to get a pie at an affordable price\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you've learned previously, run inference using the new `infer_restaurant.yaml` configuration file. If you get stuck, you can take a look at the [solution](solutions/ex7.3.2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:12:04,828 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:12:04,965 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:12:04,997 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:12:09 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:09 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:09 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:09 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:12:10 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:12:10 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:12:10 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:12:10 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:10 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:15 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:15 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:15 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:15 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:12:15 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:12:16 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:12:16 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:12:16 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:16 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:12:17 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/infer.py:84: UserWarning: \n",
      "    'infer_restaurant.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:12:17 tlt_logging:20] Experiment configuration:\n",
      "    exp_manager:\n",
      "      task_name: infer\n",
      "      explicit_log_dir: /workspace/mount/results/bert-base/\n",
      "    restore_from: /workspace/mount/models/namedentityrecognition_english_bert.tlt\n",
      "    input_batch:\n",
      "    - I would like to order a pizza for 6pm\n",
      "    - what sauce in your burger .\n",
      "    - mhh nice food.\n",
      "    - any good cheap german restaurants nearby\n",
      "    - any good ice cream parlors around\n",
      "    - any good place to get a pie at an affordable price\n",
      "    encryption_key: '********'\n",
      "    \n",
      "[NeMo W 2022-06-22 12:12:17 exp_manager:26] Exp_manager is logging to `/workspace/mount/results/bert-base/``, but it already exists.\n",
      "[NeMo I 2022-06-22 12:12:20 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmpmi45006f/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 139780661464752 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 39.4kB/s]\n",
      "Lock 139780661464752 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 139780661422352 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 583kB/s]\n",
      "Lock 139780661422352 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 139780661477184 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 57.8MB/s]\n",
      "Lock 139780661477184 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 139780661205792 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 70.5MB/s]\n",
      "Lock 139780661205792 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:12:20 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2022-06-22 12:12:20 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 139780591536544 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|████████████████████████████| 440M/440M [00:04<00:00, 106MB/s]\n",
      "Lock 139780591536544 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:12:32 token_classification_dataset:121] Setting Max Seq length to: 13\n",
      "[NeMo I 2022-06-22 12:12:32 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-06-22 12:12:32 data_preprocessing:360] Min: 7 |                  Max: 13 |                  Mean: 9.5 |                  Median: 8.5\n",
      "[NeMo I 2022-06-22 12:12:32 data_preprocessing:366] 75 percentile: 11.25\n",
      "[NeMo I 2022-06-22 12:12:32 data_preprocessing:367] 99 percentile: 12.95\n",
      "[NeMo W 2022-06-22 12:12:32 token_classification_dataset:150] 0 are longer than 13\n",
      "[NeMo I 2022-06-22 12:12:32 token_classification_dataset:153] *** Example ***\n",
      "[NeMo I 2022-06-22 12:12:32 token_classification_dataset:154] i: 0\n",
      "[NeMo I 2022-06-22 12:12:32 token_classification_dataset:155] subtokens: [CLS] i would like to order a pizza for 6 ##pm [SEP]\n",
      "[NeMo I 2022-06-22 12:12:32 token_classification_dataset:156] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "[NeMo I 2022-06-22 12:12:32 token_classification_dataset:157] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "[NeMo I 2022-06-22 12:12:32 token_classification_dataset:158] subtokens_mask: 0 1 1 1 1 1 1 1 1 1 0 0 0\n",
      "[NeMo I 2022-06-22 12:12:33 infer:76] Query  : I would like to order a pizza for 6pm\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: I would like to order a pizza for 6pm[B-TIME]\n",
      "[NeMo I 2022-06-22 12:12:33 infer:76] Query  : what sauce in your burger .\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: what sauce in your burger .\n",
      "[NeMo I 2022-06-22 12:12:33 infer:76] Query  : mhh nice food.\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: mhh nice food.\n",
      "[NeMo I 2022-06-22 12:12:33 infer:76] Query  : any good cheap german restaurants nearby\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: any good cheap german[B-GPE] restaurants nearby\n",
      "[NeMo I 2022-06-22 12:12:33 infer:76] Query  : any good ice cream parlors around\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: any good ice cream parlors around\n",
      "[NeMo I 2022-06-22 12:12:33 infer:76] Query  : any good place to get a pie at an affordable price\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: any good place to get a pie at an affordable price\n",
      "[NeMo I 2022-06-22 12:12:33 infer:80] Experiment logs saved to '/workspace/mount/results/bert-base'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:12:35,084 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# TODO infer on NER model with the infer_restaurant.yaml examples\n",
    "!tao token_classification infer \\\n",
    "    -e $SPECS_DIR/token_classification/infer_restaurant.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $MODELS_DIR/namedentityrecognition_english_bert.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/bert-base/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-06-22 11:41:31 infer:77] Results: We bought four shirts from the Nvidia[B-ORG] gear store in Santa[B-LOC] Clara[I-LOC].\n",
      "[NeMo I 2022-06-22 11:41:31 infer:77] Results: Nvidia[B-ORG] is a company.\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: I would like to order a pizza for 6pm[B-TIME]\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: what sauce in your burger .\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: mhh nice food.\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: any good cheap german[B-GPE] restaurants nearby\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: any good ice cream parlors around\n",
      "[NeMo I 2022-06-22 11:43:18 infer:77] Results: any good place to get a pie at an affordable price\n",
      "[NeMo I 2022-06-22 12:11:32 infer:77] Results: We bought four shirts from the Nvidia[B-ORG] gear store in Santa[B-LOC] Clara[I-LOC].\n",
      "[NeMo I 2022-06-22 12:11:32 infer:77] Results: Nvidia[B-ORG] is a company.\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: I would like to order a pizza for 6pm[B-TIME]\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: what sauce in your burger .\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: mhh nice food.\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: any good cheap german[B-GPE] restaurants nearby\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: any good ice cream parlors around\n",
      "[NeMo I 2022-06-22 12:12:33 infer:77] Results: any good place to get a pie at an affordable price\n"
     ]
    }
   ],
   "source": [
    "!grep Results $source_mount/results/bert-base/infer.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job running the inference!  Is the result useful?  \n",
    "\n",
    "The current labels are not well suited for the restaurant context!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7.4 NER Training\n",
    "\n",
    "To get useful information in a restaurant context, we need to train a robust classifier on a dataset that has the appropriate entities labeled. We will begin with a pretrained BERT language model to encode the text, as it already inherently understands word relationships. By default, TAO Toolkit uses the `bert-base-uncased` language model (110M parameters). Then, we will train a classifier to recognize restaurant entities.  Fortunately, we have an annotated dataset for restaurants that we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.1 Restaurant Data Exploration\n",
    "\n",
    "The [Restaurant dataset](https://groups.csail.mit.edu/sls/downloads/restaurant) is labeled using the [IOB format](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) (short for \"inside\", \"outside\", and \"beginning\"). The following entity classes appear in the dataset:\n",
    "\n",
    "```\n",
    "Amenity, Cuisine, Dish, Hours, Location, Price, Rating, Restaurant_Name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached_text_dev.txt_BertTokenizer_128_30522_-1\t  labels_train_label_stats.tsv\n",
      "cached_text_train.txt_BertTokenizer_128_30522_-1  labels_train_weights.p\n",
      "label_ids.csv\t\t\t\t\t  restauranttest.bio\n",
      "labels_dev.txt\t\t\t\t\t  restauranttrain.bio\n",
      "labels_dev_label_stats.tsv\t\t\t  text_dev.txt\n",
      "labels_train.txt\t\t\t\t  text_train.txt\n"
     ]
    }
   ],
   "source": [
    "# set data path and explore data\n",
    "DATA_DOWNLOAD_DIR = os.path.join(source_mount, 'data/restaurant')\n",
    "!ls $DATA_DOWNLOAD_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\ta\n",
      "B-Rating\tfour\n",
      "I-Rating\tstar\n",
      "O\trestaurant\n",
      "B-Location\twith\n",
      "I-Location\ta\n",
      "B-Amenity\tbar\n"
     ]
    }
   ],
   "source": [
    "# print first test example\n",
    "!head -7 $DATA_DOWNLOAD_DIR/restauranttest.bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOB Tagging\n",
    "\n",
    "The files in the dataset, `restauranttrain.bio` and `restauranttest.bio` must be converted to an IOB format that is compatible with [TAO Token Classification module](https://docs.nvidia.com/metropolis/TAO/tao-user-guide/text/nlp/token_classification.html#data-input-for-token-classification-model). TAO Toolkit requires the input to be in two files:\n",
    "-  `text.txt`: Each line of the text.txt file contains text sequences, where words are separated with spaces.\n",
    "-  `labels.txt`: Each line contains corresponding labels for each word in text.txt; the labels are separated with spaces.\n",
    "\n",
    "For the first test example printed previously, the TAO input format should be a `text.txt` file mapped to a `labels.txt` as follows:\n",
    "```text\n",
    "  text.txt:   a four     star     restaurant  with        a          bar\n",
    "labels.txt:   O B-Rating I-Rating O           B-Location  I-Location B-Amenity\n",
    "```\n",
    "To generate the TAO-compatible dataset, we can use the [conversion script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/token_classification/data/import_from_iob_format.py) from NVIDIA NeMo toolkit.  \n",
    "\n",
    "We don't need to do that here, as the preprocessed dataset is already available on `data/restaurant` directory for this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a four star restaurant with a bar \n",
      "any asian cuisine around \n",
      "any bbq places open before 5 nearby \n",
      "any dancing establishments with reasonable pricing \n",
      "any good cheap german restaurants nearby \n",
      "any good ice cream parlors around \n",
      "any good place to get a pie at an affordable price \n",
      "any good vegan spots nearby \n",
      "any mexican places have a tameles special today \n",
      "any place along the road has a good beer selection that also serves ribs \n"
     ]
    }
   ],
   "source": [
    "# show test text samples\n",
    "!head $DATA_DOWNLOAD_DIR/text_dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O B-Rating I-Rating O B-Location I-Location B-Amenity \n",
      "O B-Cuisine O B-Location \n",
      "O B-Cuisine O B-Hours I-Hours I-Hours B-Location \n",
      "O B-Location I-Location O B-Price O \n",
      "O O B-Price B-Cuisine O B-Location \n",
      "O B-Rating B-Cuisine I-Cuisine I-Cuisine B-Location \n",
      "O B-Rating O O O O B-Dish O O B-Price O \n",
      "O O B-Cuisine O B-Location \n",
      "O B-Cuisine O O O B-Dish B-Amenity I-Amenity \n",
      "O O B-Location I-Location I-Location O O B-Rating B-Dish O O O O B-Dish \n"
     ]
    }
   ],
   "source": [
    "# show test labels samples\n",
    "!head $DATA_DOWNLOAD_DIR/labels_dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.2 `train` Command\n",
    "\n",
    "To train a model using TAO, we must configure the spec file and run the `tao token_classification train` command. More details about the command can be found in the [documentation](https://docs.nvidia.com/tao/tao-toolkit/text/nlp/token_classification.html#training-a-token-classification-model), including [required arguments](https://docs.nvidia.com/tao/tao-toolkit/text/nlp/token_classification.html#required-arguments-for-training) and an example command:\n",
    "\n",
    "```yaml\n",
    "REQUIRED ARGUMENTS\n",
    "-e: The experiment specification file to set up training.\n",
    "\n",
    "-r: Path to the directory to store the results/logs. Note, the trained-model.tlt would be saved in this specified folder under a subfolder checkpoints; in our case it will be saved here: /results/token_classification/train/checkpoints/trained-model.tlt\n",
    "\n",
    "-k: Encryption key\n",
    "\n",
    "data_dir: Path to the data_dir with the processed data files.\n",
    "\n",
    "model.label_ids: Path to the label_ids.csv file, usually stored at data_dir\n",
    "\n",
    "```\n",
    "\n",
    "```sh\n",
    "EXAMPLE COMMAND\n",
    "tao token_classification train [-h] \\\n",
    "    -e /specs/nlp/token_classification/train.yaml \\\n",
    "    -r /results/token_classification/train/ \\\n",
    "    -g 1 \\\n",
    "    -k $KEY\n",
    "    data_dir=/path/to/data_dir \\\n",
    "    model.label_ids=/path/to/label_ids.csv \\\n",
    "    trainer.max_epochs=5 \\\n",
    "    training_ds.num_samples=-1 \\\n",
    "    validation_ds.num_samples=-1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command relies on the [train.yaml](tao/specs/token_classification/train.yaml) specification file. Through the spec file, you can tune many knobs such as the model, dataset, hyperparameters, and optimizers.\n",
    "Each `token_classification` command (`download_and_convert`, `train`, `finetune`, `evaluate`, `infer`, and so on) has a dedicated spec file with configurations pertinent to it. \n",
    "\n",
    "Take a look at the training spec file you downloaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
      "# TLT Spec file for training of the BERT model on a Token Classification task:\n",
      "# Named Entity Recognition on GMB dataset\n",
      "\n",
      "trainer:\n",
      "  max_epochs: 5\n",
      "\n",
      "model:\n",
      "  tokenizer:\n",
      "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n",
      "      vocab_file: null # path to vocab file\n",
      "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
      "      special_tokens: null\n",
      "\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null # json file, precedence over config\n",
      "    config: null\n",
      "\n",
      "  head:\n",
      "    num_fc_layers: 2\n",
      "    fc_dropout: 0.5\n",
      "    activation: 'relu'\n",
      "    use_transformer_init: True\n",
      "\n",
      "  # Path to file with label_ids, generated with dataset_convert.py.\n",
      "  # Those labels are used by the model as labels (names of target classes, their number).\n",
      "  label_ids: ???\n",
      "\n",
      "# Path to directory containing both finetuning and validation data.\n",
      "data_dir: ???\n",
      "\n",
      "training_ds:\n",
      "  text_file: text_train.txt\n",
      "  labels_file: labels_train.txt\n",
      "  batch_size: 64\n",
      "\n",
      "validation_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  batch_size: 64\n",
      "\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 5e-5\n",
      "  weight_decay: 0.00\n",
      "\n",
      "  # scheduler setup\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    # Scheduler params\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    # pytorch lightning args\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This line will print the entire training config\n",
    "!cat $source_mount/specs/token_classification/train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below uses the default `train.yaml`. It is configured to use the `bert-base-uncased` pretrained model. Additionally, these configurations can be overridden by adding the overrides to the `tao` command. Here, we override the `data_dir`, `model.label_ids`, `trainer.max_epochs`, `training_ds.num_samples`, and `validation_ds.num_samples` configurations to suit our needs. <br>\n",
    "\n",
    "In order to get good results, try training for a few epochs (depends on the size of the data). \n",
    "\n",
    "*NOTE: All file paths correspond to the destination-mount directory that is visible in the TAO docker container and used in the backend.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:14:43,483 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:14:43,619 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:14:43,651 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:14:48 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:48 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:48 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:48 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:14:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:14:49 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:14:49 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:14:49 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:49 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:54 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:54 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:54 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:54 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:14:54 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:14:54 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:14:54 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:14:54 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:54 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:14:55 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/train.py:139: UserWarning: \n",
      "    'train.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:14:55 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: ???\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/mount/results/bert-base_ner\n",
      "      exp_dir: null\n",
      "      name: trained-model\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: true\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: min\n",
      "        period: null\n",
      "        every_n_val_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .tlt\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "      files_to_copy: null\n",
      "    model:\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      label_ids: /workspace/mount/data/restaurant/label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: null\n",
      "        class_balancing: null\n",
      "        max_seq_length: 128\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 3\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 32\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O0\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices: null\n",
      "    training_ds:\n",
      "      batch_size: 64\n",
      "      text_file: text_train.txt\n",
      "      labels_file: labels_train.txt\n",
      "      shuffle: true\n",
      "      num_samples: -1\n",
      "    validation_ds:\n",
      "      batch_size: 64\n",
      "      text_file: text_dev.txt\n",
      "      labels_file: labels_dev.txt\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "    data_dir: /workspace/mount/data/restaurant\n",
      "    optim:\n",
      "      name: adam\n",
      "      lr: 5.0e-05\n",
      "      weight_decay: 0.0\n",
      "      sched:\n",
      "        name: WarmupAnnealing\n",
      "        warmup_steps: null\n",
      "        warmup_ratio: 0.1\n",
      "        last_epoch: -1\n",
      "        monitor: val_loss\n",
      "        reduce_on_plateau: false\n",
      "    encryption_key: '**********'\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-06-22 12:14:55 exp_manager:414] Exp_manager is logging to /workspace/mount/results/bert-base_ner, but it already exists.\n",
      "[NeMo W 2022-06-22 12:14:55 exp_manager:332] There was no checkpoint folder at checkpoint_dir :/workspace/mount/results/bert-base_ner/checkpoints. Training from scratch.\n",
      "[NeMo I 2022-06-22 12:14:55 exp_manager:220] Experiments will be logged at /workspace/mount/results/bert-base_ner\n",
      "[NeMo W 2022-06-22 12:14:55 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:240: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2022-06-22 12:14:55 token_classification_model:58] Reusing label_ids file found at /workspace/mount/data/restaurant/label_ids.csv.\n",
      "[NeMo I 2022-06-22 12:14:55 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: None, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 139973840000912 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 20.5kB/s]\n",
      "Lock 139973840000912 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 139973840002880 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 603kB/s]\n",
      "Lock 139973840002880 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 139973839967616 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 56.9MB/s]\n",
      "Lock 139973839967616 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 139973839952192 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 76.9MB/s]\n",
      "Lock 139973839952192 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:14:56 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 139973839902416 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|████████████████████████████| 440M/440M [00:04<00:00, 106MB/s]\n",
      "Lock 139973839902416 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:15:03 token_classification_model:103] Setting model.dataset.data_dir to /workspace/mount/data/restaurant.\n",
      "[NeMo I 2022-06-22 12:15:03 token_classification_utils:54] Processing /workspace/mount/data/restaurant/labels_train.txt\n",
      "[NeMo I 2022-06-22 12:15:03 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:15:03 token_classification_utils:90] Labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16} saved to : /workspace/mount/data/restaurant/label_ids.csv\n",
      "[NeMo I 2022-06-22 12:15:03 token_classification_utils:99] Three most popular labels in /workspace/mount/data/restaurant/labels_train.txt:\n",
      "[NeMo I 2022-06-22 12:15:03 data_preprocessing:194] label: 0, 43670 out of 70525 (61.92%).\n",
      "[NeMo I 2022-06-22 12:15:03 data_preprocessing:194] label: 5, 3817 out of 70525 (5.41%).\n",
      "[NeMo I 2022-06-22 12:15:03 data_preprocessing:194] label: 13, 3658 out of 70525 (5.19%).\n",
      "[NeMo I 2022-06-22 12:15:03 token_classification_utils:101] Total labels: 70525. Label frequencies - {0: 43670, 5: 3817, 13: 3658, 2: 2839, 9: 2676, 1: 2541, 8: 1901, 16: 1668, 3: 1475, 12: 1283, 7: 1070, 4: 990, 11: 767, 6: 730, 10: 630, 15: 527, 14: 283}\n",
      "[NeMo I 2022-06-22 12:15:03 token_classification_utils:107] Class weights restored from /workspace/mount/data/restaurant/labels_train_weights.p\n",
      "[NeMo I 2022-06-22 12:15:04 token_classification_dataset:277] features restored from /workspace/mount/data/restaurant/cached_text_train.txt_BertTokenizer_128_30522_-1\n",
      "[NeMo I 2022-06-22 12:15:04 token_classification_utils:54] Processing /workspace/mount/data/restaurant/labels_dev.txt\n",
      "[NeMo I 2022-06-22 12:15:04 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:15:04 token_classification_utils:96] /workspace/mount/data/restaurant/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-06-22 12:15:04 token_classification_dataset:277] features restored from /workspace/mount/data/restaurant/cached_text_dev.txt_BertTokenizer_128_30522_-1\n",
      "[NeMo I 2022-06-22 12:15:04 modelPT:544] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2022-06-22 12:15:04 lr_scheduler:621] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f4e2cff6370>\" \n",
      "    will be used during training (effective maximum steps = 360) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 360\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2022-06-22 12:15:06 modelPT:415] No optimizer config provided, therefore no optimizer was created\n",
      "\n",
      "    | Name                                                   | Type                 | Params\n",
      "--------------------------------------------------------------------------------------------------\n",
      "0   | bert_model                                             | BertEncoder          | 109 M \n",
      "1   | bert_model.embeddings                                  | BertEmbeddings       | 23.8 M\n",
      "2   | bert_model.embeddings.word_embeddings                  | Embedding            | 23.4 M\n",
      "3   | bert_model.embeddings.position_embeddings              | Embedding            | 393 K \n",
      "4   | bert_model.embeddings.token_type_embeddings            | Embedding            | 1.5 K \n",
      "5   | bert_model.embeddings.LayerNorm                        | LayerNorm            | 1.5 K \n",
      "6   | bert_model.embeddings.dropout                          | Dropout              | 0     \n",
      "7   | bert_model.encoder                                     | BertEncoder          | 85.1 M\n",
      "8   | bert_model.encoder.layer                               | ModuleList           | 85.1 M\n",
      "9   | bert_model.encoder.layer.0                             | BertLayer            | 7.1 M \n",
      "10  | bert_model.encoder.layer.0.attention                   | BertAttention        | 2.4 M \n",
      "11  | bert_model.encoder.layer.0.attention.self              | BertSelfAttention    | 1.8 M \n",
      "12  | bert_model.encoder.layer.0.attention.self.query        | Linear               | 590 K \n",
      "13  | bert_model.encoder.layer.0.attention.self.key          | Linear               | 590 K \n",
      "14  | bert_model.encoder.layer.0.attention.self.value        | Linear               | 590 K \n",
      "15  | bert_model.encoder.layer.0.attention.self.dropout      | Dropout              | 0     \n",
      "16  | bert_model.encoder.layer.0.attention.output            | BertSelfOutput       | 592 K \n",
      "17  | bert_model.encoder.layer.0.attention.output.dense      | Linear               | 590 K \n",
      "18  | bert_model.encoder.layer.0.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "19  | bert_model.encoder.layer.0.attention.output.dropout    | Dropout              | 0     \n",
      "20  | bert_model.encoder.layer.0.intermediate                | BertIntermediate     | 2.4 M \n",
      "21  | bert_model.encoder.layer.0.intermediate.dense          | Linear               | 2.4 M \n",
      "22  | bert_model.encoder.layer.0.output                      | BertOutput           | 2.4 M \n",
      "23  | bert_model.encoder.layer.0.output.dense                | Linear               | 2.4 M \n",
      "24  | bert_model.encoder.layer.0.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "25  | bert_model.encoder.layer.0.output.dropout              | Dropout              | 0     \n",
      "26  | bert_model.encoder.layer.1                             | BertLayer            | 7.1 M \n",
      "27  | bert_model.encoder.layer.1.attention                   | BertAttention        | 2.4 M \n",
      "28  | bert_model.encoder.layer.1.attention.self              | BertSelfAttention    | 1.8 M \n",
      "29  | bert_model.encoder.layer.1.attention.self.query        | Linear               | 590 K \n",
      "30  | bert_model.encoder.layer.1.attention.self.key          | Linear               | 590 K \n",
      "31  | bert_model.encoder.layer.1.attention.self.value        | Linear               | 590 K \n",
      "32  | bert_model.encoder.layer.1.attention.self.dropout      | Dropout              | 0     \n",
      "33  | bert_model.encoder.layer.1.attention.output            | BertSelfOutput       | 592 K \n",
      "34  | bert_model.encoder.layer.1.attention.output.dense      | Linear               | 590 K \n",
      "35  | bert_model.encoder.layer.1.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "36  | bert_model.encoder.layer.1.attention.output.dropout    | Dropout              | 0     \n",
      "37  | bert_model.encoder.layer.1.intermediate                | BertIntermediate     | 2.4 M \n",
      "38  | bert_model.encoder.layer.1.intermediate.dense          | Linear               | 2.4 M \n",
      "39  | bert_model.encoder.layer.1.output                      | BertOutput           | 2.4 M \n",
      "40  | bert_model.encoder.layer.1.output.dense                | Linear               | 2.4 M \n",
      "41  | bert_model.encoder.layer.1.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "42  | bert_model.encoder.layer.1.output.dropout              | Dropout              | 0     \n",
      "43  | bert_model.encoder.layer.2                             | BertLayer            | 7.1 M \n",
      "44  | bert_model.encoder.layer.2.attention                   | BertAttention        | 2.4 M \n",
      "45  | bert_model.encoder.layer.2.attention.self              | BertSelfAttention    | 1.8 M \n",
      "46  | bert_model.encoder.layer.2.attention.self.query        | Linear               | 590 K \n",
      "47  | bert_model.encoder.layer.2.attention.self.key          | Linear               | 590 K \n",
      "48  | bert_model.encoder.layer.2.attention.self.value        | Linear               | 590 K \n",
      "49  | bert_model.encoder.layer.2.attention.self.dropout      | Dropout              | 0     \n",
      "50  | bert_model.encoder.layer.2.attention.output            | BertSelfOutput       | 592 K \n",
      "51  | bert_model.encoder.layer.2.attention.output.dense      | Linear               | 590 K \n",
      "52  | bert_model.encoder.layer.2.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "53  | bert_model.encoder.layer.2.attention.output.dropout    | Dropout              | 0     \n",
      "54  | bert_model.encoder.layer.2.intermediate                | BertIntermediate     | 2.4 M \n",
      "55  | bert_model.encoder.layer.2.intermediate.dense          | Linear               | 2.4 M \n",
      "56  | bert_model.encoder.layer.2.output                      | BertOutput           | 2.4 M \n",
      "57  | bert_model.encoder.layer.2.output.dense                | Linear               | 2.4 M \n",
      "58  | bert_model.encoder.layer.2.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "59  | bert_model.encoder.layer.2.output.dropout              | Dropout              | 0     \n",
      "60  | bert_model.encoder.layer.3                             | BertLayer            | 7.1 M \n",
      "61  | bert_model.encoder.layer.3.attention                   | BertAttention        | 2.4 M \n",
      "62  | bert_model.encoder.layer.3.attention.self              | BertSelfAttention    | 1.8 M \n",
      "63  | bert_model.encoder.layer.3.attention.self.query        | Linear               | 590 K \n",
      "64  | bert_model.encoder.layer.3.attention.self.key          | Linear               | 590 K \n",
      "65  | bert_model.encoder.layer.3.attention.self.value        | Linear               | 590 K \n",
      "66  | bert_model.encoder.layer.3.attention.self.dropout      | Dropout              | 0     \n",
      "67  | bert_model.encoder.layer.3.attention.output            | BertSelfOutput       | 592 K \n",
      "68  | bert_model.encoder.layer.3.attention.output.dense      | Linear               | 590 K \n",
      "69  | bert_model.encoder.layer.3.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "70  | bert_model.encoder.layer.3.attention.output.dropout    | Dropout              | 0     \n",
      "71  | bert_model.encoder.layer.3.intermediate                | BertIntermediate     | 2.4 M \n",
      "72  | bert_model.encoder.layer.3.intermediate.dense          | Linear               | 2.4 M \n",
      "73  | bert_model.encoder.layer.3.output                      | BertOutput           | 2.4 M \n",
      "74  | bert_model.encoder.layer.3.output.dense                | Linear               | 2.4 M \n",
      "75  | bert_model.encoder.layer.3.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "76  | bert_model.encoder.layer.3.output.dropout              | Dropout              | 0     \n",
      "77  | bert_model.encoder.layer.4                             | BertLayer            | 7.1 M \n",
      "78  | bert_model.encoder.layer.4.attention                   | BertAttention        | 2.4 M \n",
      "79  | bert_model.encoder.layer.4.attention.self              | BertSelfAttention    | 1.8 M \n",
      "80  | bert_model.encoder.layer.4.attention.self.query        | Linear               | 590 K \n",
      "81  | bert_model.encoder.layer.4.attention.self.key          | Linear               | 590 K \n",
      "82  | bert_model.encoder.layer.4.attention.self.value        | Linear               | 590 K \n",
      "83  | bert_model.encoder.layer.4.attention.self.dropout      | Dropout              | 0     \n",
      "84  | bert_model.encoder.layer.4.attention.output            | BertSelfOutput       | 592 K \n",
      "85  | bert_model.encoder.layer.4.attention.output.dense      | Linear               | 590 K \n",
      "86  | bert_model.encoder.layer.4.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "87  | bert_model.encoder.layer.4.attention.output.dropout    | Dropout              | 0     \n",
      "88  | bert_model.encoder.layer.4.intermediate                | BertIntermediate     | 2.4 M \n",
      "89  | bert_model.encoder.layer.4.intermediate.dense          | Linear               | 2.4 M \n",
      "90  | bert_model.encoder.layer.4.output                      | BertOutput           | 2.4 M \n",
      "91  | bert_model.encoder.layer.4.output.dense                | Linear               | 2.4 M \n",
      "92  | bert_model.encoder.layer.4.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "93  | bert_model.encoder.layer.4.output.dropout              | Dropout              | 0     \n",
      "94  | bert_model.encoder.layer.5                             | BertLayer            | 7.1 M \n",
      "95  | bert_model.encoder.layer.5.attention                   | BertAttention        | 2.4 M \n",
      "96  | bert_model.encoder.layer.5.attention.self              | BertSelfAttention    | 1.8 M \n",
      "97  | bert_model.encoder.layer.5.attention.self.query        | Linear               | 590 K \n",
      "98  | bert_model.encoder.layer.5.attention.self.key          | Linear               | 590 K \n",
      "99  | bert_model.encoder.layer.5.attention.self.value        | Linear               | 590 K \n",
      "100 | bert_model.encoder.layer.5.attention.self.dropout      | Dropout              | 0     \n",
      "101 | bert_model.encoder.layer.5.attention.output            | BertSelfOutput       | 592 K \n",
      "102 | bert_model.encoder.layer.5.attention.output.dense      | Linear               | 590 K \n",
      "103 | bert_model.encoder.layer.5.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "104 | bert_model.encoder.layer.5.attention.output.dropout    | Dropout              | 0     \n",
      "105 | bert_model.encoder.layer.5.intermediate                | BertIntermediate     | 2.4 M \n",
      "106 | bert_model.encoder.layer.5.intermediate.dense          | Linear               | 2.4 M \n",
      "107 | bert_model.encoder.layer.5.output                      | BertOutput           | 2.4 M \n",
      "108 | bert_model.encoder.layer.5.output.dense                | Linear               | 2.4 M \n",
      "109 | bert_model.encoder.layer.5.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "110 | bert_model.encoder.layer.5.output.dropout              | Dropout              | 0     \n",
      "111 | bert_model.encoder.layer.6                             | BertLayer            | 7.1 M \n",
      "112 | bert_model.encoder.layer.6.attention                   | BertAttention        | 2.4 M \n",
      "113 | bert_model.encoder.layer.6.attention.self              | BertSelfAttention    | 1.8 M \n",
      "114 | bert_model.encoder.layer.6.attention.self.query        | Linear               | 590 K \n",
      "115 | bert_model.encoder.layer.6.attention.self.key          | Linear               | 590 K \n",
      "116 | bert_model.encoder.layer.6.attention.self.value        | Linear               | 590 K \n",
      "117 | bert_model.encoder.layer.6.attention.self.dropout      | Dropout              | 0     \n",
      "118 | bert_model.encoder.layer.6.attention.output            | BertSelfOutput       | 592 K \n",
      "119 | bert_model.encoder.layer.6.attention.output.dense      | Linear               | 590 K \n",
      "120 | bert_model.encoder.layer.6.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "121 | bert_model.encoder.layer.6.attention.output.dropout    | Dropout              | 0     \n",
      "122 | bert_model.encoder.layer.6.intermediate                | BertIntermediate     | 2.4 M \n",
      "123 | bert_model.encoder.layer.6.intermediate.dense          | Linear               | 2.4 M \n",
      "124 | bert_model.encoder.layer.6.output                      | BertOutput           | 2.4 M \n",
      "125 | bert_model.encoder.layer.6.output.dense                | Linear               | 2.4 M \n",
      "126 | bert_model.encoder.layer.6.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "127 | bert_model.encoder.layer.6.output.dropout              | Dropout              | 0     \n",
      "128 | bert_model.encoder.layer.7                             | BertLayer            | 7.1 M \n",
      "129 | bert_model.encoder.layer.7.attention                   | BertAttention        | 2.4 M \n",
      "130 | bert_model.encoder.layer.7.attention.self              | BertSelfAttention    | 1.8 M \n",
      "131 | bert_model.encoder.layer.7.attention.self.query        | Linear               | 590 K \n",
      "132 | bert_model.encoder.layer.7.attention.self.key          | Linear               | 590 K \n",
      "133 | bert_model.encoder.layer.7.attention.self.value        | Linear               | 590 K \n",
      "134 | bert_model.encoder.layer.7.attention.self.dropout      | Dropout              | 0     \n",
      "135 | bert_model.encoder.layer.7.attention.output            | BertSelfOutput       | 592 K \n",
      "136 | bert_model.encoder.layer.7.attention.output.dense      | Linear               | 590 K \n",
      "137 | bert_model.encoder.layer.7.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "138 | bert_model.encoder.layer.7.attention.output.dropout    | Dropout              | 0     \n",
      "139 | bert_model.encoder.layer.7.intermediate                | BertIntermediate     | 2.4 M \n",
      "140 | bert_model.encoder.layer.7.intermediate.dense          | Linear               | 2.4 M \n",
      "141 | bert_model.encoder.layer.7.output                      | BertOutput           | 2.4 M \n",
      "142 | bert_model.encoder.layer.7.output.dense                | Linear               | 2.4 M \n",
      "143 | bert_model.encoder.layer.7.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "144 | bert_model.encoder.layer.7.output.dropout              | Dropout              | 0     \n",
      "145 | bert_model.encoder.layer.8                             | BertLayer            | 7.1 M \n",
      "146 | bert_model.encoder.layer.8.attention                   | BertAttention        | 2.4 M \n",
      "147 | bert_model.encoder.layer.8.attention.self              | BertSelfAttention    | 1.8 M \n",
      "148 | bert_model.encoder.layer.8.attention.self.query        | Linear               | 590 K \n",
      "149 | bert_model.encoder.layer.8.attention.self.key          | Linear               | 590 K \n",
      "150 | bert_model.encoder.layer.8.attention.self.value        | Linear               | 590 K \n",
      "151 | bert_model.encoder.layer.8.attention.self.dropout      | Dropout              | 0     \n",
      "152 | bert_model.encoder.layer.8.attention.output            | BertSelfOutput       | 592 K \n",
      "153 | bert_model.encoder.layer.8.attention.output.dense      | Linear               | 590 K \n",
      "154 | bert_model.encoder.layer.8.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "155 | bert_model.encoder.layer.8.attention.output.dropout    | Dropout              | 0     \n",
      "156 | bert_model.encoder.layer.8.intermediate                | BertIntermediate     | 2.4 M \n",
      "157 | bert_model.encoder.layer.8.intermediate.dense          | Linear               | 2.4 M \n",
      "158 | bert_model.encoder.layer.8.output                      | BertOutput           | 2.4 M \n",
      "159 | bert_model.encoder.layer.8.output.dense                | Linear               | 2.4 M \n",
      "160 | bert_model.encoder.layer.8.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "161 | bert_model.encoder.layer.8.output.dropout              | Dropout              | 0     \n",
      "162 | bert_model.encoder.layer.9                             | BertLayer            | 7.1 M \n",
      "163 | bert_model.encoder.layer.9.attention                   | BertAttention        | 2.4 M \n",
      "164 | bert_model.encoder.layer.9.attention.self              | BertSelfAttention    | 1.8 M \n",
      "165 | bert_model.encoder.layer.9.attention.self.query        | Linear               | 590 K \n",
      "166 | bert_model.encoder.layer.9.attention.self.key          | Linear               | 590 K \n",
      "167 | bert_model.encoder.layer.9.attention.self.value        | Linear               | 590 K \n",
      "168 | bert_model.encoder.layer.9.attention.self.dropout      | Dropout              | 0     \n",
      "169 | bert_model.encoder.layer.9.attention.output            | BertSelfOutput       | 592 K \n",
      "170 | bert_model.encoder.layer.9.attention.output.dense      | Linear               | 590 K \n",
      "171 | bert_model.encoder.layer.9.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "172 | bert_model.encoder.layer.9.attention.output.dropout    | Dropout              | 0     \n",
      "173 | bert_model.encoder.layer.9.intermediate                | BertIntermediate     | 2.4 M \n",
      "174 | bert_model.encoder.layer.9.intermediate.dense          | Linear               | 2.4 M \n",
      "175 | bert_model.encoder.layer.9.output                      | BertOutput           | 2.4 M \n",
      "176 | bert_model.encoder.layer.9.output.dense                | Linear               | 2.4 M \n",
      "177 | bert_model.encoder.layer.9.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "178 | bert_model.encoder.layer.9.output.dropout              | Dropout              | 0     \n",
      "179 | bert_model.encoder.layer.10                            | BertLayer            | 7.1 M \n",
      "180 | bert_model.encoder.layer.10.attention                  | BertAttention        | 2.4 M \n",
      "181 | bert_model.encoder.layer.10.attention.self             | BertSelfAttention    | 1.8 M \n",
      "182 | bert_model.encoder.layer.10.attention.self.query       | Linear               | 590 K \n",
      "183 | bert_model.encoder.layer.10.attention.self.key         | Linear               | 590 K \n",
      "184 | bert_model.encoder.layer.10.attention.self.value       | Linear               | 590 K \n",
      "185 | bert_model.encoder.layer.10.attention.self.dropout     | Dropout              | 0     \n",
      "186 | bert_model.encoder.layer.10.attention.output           | BertSelfOutput       | 592 K \n",
      "187 | bert_model.encoder.layer.10.attention.output.dense     | Linear               | 590 K \n",
      "188 | bert_model.encoder.layer.10.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "189 | bert_model.encoder.layer.10.attention.output.dropout   | Dropout              | 0     \n",
      "190 | bert_model.encoder.layer.10.intermediate               | BertIntermediate     | 2.4 M \n",
      "191 | bert_model.encoder.layer.10.intermediate.dense         | Linear               | 2.4 M \n",
      "192 | bert_model.encoder.layer.10.output                     | BertOutput           | 2.4 M \n",
      "193 | bert_model.encoder.layer.10.output.dense               | Linear               | 2.4 M \n",
      "194 | bert_model.encoder.layer.10.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "195 | bert_model.encoder.layer.10.output.dropout             | Dropout              | 0     \n",
      "196 | bert_model.encoder.layer.11                            | BertLayer            | 7.1 M \n",
      "197 | bert_model.encoder.layer.11.attention                  | BertAttention        | 2.4 M \n",
      "198 | bert_model.encoder.layer.11.attention.self             | BertSelfAttention    | 1.8 M \n",
      "199 | bert_model.encoder.layer.11.attention.self.query       | Linear               | 590 K \n",
      "200 | bert_model.encoder.layer.11.attention.self.key         | Linear               | 590 K \n",
      "201 | bert_model.encoder.layer.11.attention.self.value       | Linear               | 590 K \n",
      "202 | bert_model.encoder.layer.11.attention.self.dropout     | Dropout              | 0     \n",
      "203 | bert_model.encoder.layer.11.attention.output           | BertSelfOutput       | 592 K \n",
      "204 | bert_model.encoder.layer.11.attention.output.dense     | Linear               | 590 K \n",
      "205 | bert_model.encoder.layer.11.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "206 | bert_model.encoder.layer.11.attention.output.dropout   | Dropout              | 0     \n",
      "207 | bert_model.encoder.layer.11.intermediate               | BertIntermediate     | 2.4 M \n",
      "208 | bert_model.encoder.layer.11.intermediate.dense         | Linear               | 2.4 M \n",
      "209 | bert_model.encoder.layer.11.output                     | BertOutput           | 2.4 M \n",
      "210 | bert_model.encoder.layer.11.output.dense               | Linear               | 2.4 M \n",
      "211 | bert_model.encoder.layer.11.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "212 | bert_model.encoder.layer.11.output.dropout             | Dropout              | 0     \n",
      "213 | bert_model.pooler                                      | BertPooler           | 590 K \n",
      "214 | bert_model.pooler.dense                                | Linear               | 590 K \n",
      "215 | bert_model.pooler.activation                           | Tanh                 | 0     \n",
      "216 | classifier                                             | TokenClassifier      | 603 K \n",
      "217 | classifier.dropout                                     | Dropout              | 0     \n",
      "218 | classifier.mlp                                         | MultiLayerPerceptron | 603 K \n",
      "219 | classifier.mlp.layer0                                  | Linear               | 590 K \n",
      "220 | classifier.mlp.layer2                                  | Linear               | 13.1 K\n",
      "221 | loss                                                   | CrossEntropyLoss     | 0     \n",
      "222 | classification_report                                  | ClassificationReport | 0     \n",
      "--------------------------------------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.344   Total estimated model params size (MB)\n",
      "Validation sanity check: 100%|████████████████████| 2/2 [00:01<00:00,  1.33it/s][NeMo I 2022-06-22 12:15:07 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                          0.00       0.00       0.00        667\n",
      "    B-Amenity (label_id: 1)                                  0.00       0.00       0.00         49\n",
      "    B-Cuisine (label_id: 2)                                  5.56      29.63       9.36         54\n",
      "    B-Dish (label_id: 3)                                     1.24      34.78       2.39         23\n",
      "    B-Hours (label_id: 4)                                    0.64       5.26       1.14         19\n",
      "    B-Location (label_id: 5)                                 0.00       0.00       0.00         92\n",
      "    B-Price (label_id: 6)                                    0.00       0.00       0.00         12\n",
      "    B-Rating (label_id: 7)                                   0.00       0.00       0.00         15\n",
      "    B-Restaurant_Name (label_id: 8)                          0.00       0.00       0.00         14\n",
      "    I-Amenity (label_id: 9)                                  0.00       0.00       0.00         56\n",
      "    I-Cuisine (label_id: 10)                                 0.00       0.00       0.00         15\n",
      "    I-Dish (label_id: 11)                                    0.00       0.00       0.00          9\n",
      "    I-Hours (label_id: 12)                                   0.00       0.00       0.00         39\n",
      "    I-Location (label_id: 13)                               11.11       1.94       3.31        103\n",
      "    I-Price (label_id: 14)                                   0.00       0.00       0.00          6\n",
      "    I-Rating (label_id: 15)                                  0.00       0.00       0.00          6\n",
      "    I-Restaurant_Name (label_id: 16)                         2.86       5.26       3.70         19\n",
      "    -------------------\n",
      "    micro avg                                                2.34       2.34       2.34       1198\n",
      "    macro avg                                                1.26       4.52       1.17       1198\n",
      "    weighted avg                                             1.28       2.34       0.83       1198\n",
      "    \n",
      "Epoch 0:  83%|███████▌ | 120/144 [00:46<00:09,  2.62it/s, loss=0.57, lr=3.72e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  85%|███████▋ | 122/144 [00:46<00:08,  2.65it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:   8%|██▋                             | 2/24 [00:00<00:04,  5.21it/s]\u001b[A\n",
      "Epoch 0:  86%|███████▊ | 124/144 [00:46<00:07,  2.68it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  17%|█████▎                          | 4/24 [00:00<00:02,  6.80it/s]\u001b[A\n",
      "Epoch 0:  88%|███████▉ | 126/144 [00:46<00:06,  2.71it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  25%|████████                        | 6/24 [00:00<00:02,  8.06it/s]\u001b[A\n",
      "Epoch 0:  89%|████████ | 128/144 [00:47<00:05,  2.74it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  33%|██████████▋                     | 8/24 [00:00<00:01,  8.80it/s]\u001b[A\n",
      "Epoch 0:  90%|████████▏| 130/144 [00:47<00:05,  2.77it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  42%|████████████▉                  | 10/24 [00:01<00:01,  9.07it/s]\u001b[A\n",
      "Epoch 0:  92%|████████▎| 132/144 [00:47<00:04,  2.80it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  50%|███████████████▌               | 12/24 [00:01<00:01,  9.33it/s]\u001b[A\n",
      "Epoch 0:  93%|████████▍| 134/144 [00:47<00:03,  2.83it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  58%|██████████████████             | 14/24 [00:01<00:01,  9.44it/s]\u001b[A\n",
      "Epoch 0:  94%|████████▌| 136/144 [00:47<00:02,  2.86it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  67%|████████████████████▋          | 16/24 [00:01<00:00,  9.47it/s]\u001b[A\n",
      "Epoch 0:  96%|████████▋| 138/144 [00:48<00:02,  2.89it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  75%|███████████████████████▎       | 18/24 [00:02<00:00,  9.54it/s]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 140/144 [00:48<00:01,  2.92it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  83%|█████████████████████████▊     | 20/24 [00:02<00:00,  9.59it/s]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 142/144 [00:48<00:00,  2.95it/s, loss=0.57, lr=3.72e-5]\u001b[A\n",
      "Validating:  92%|████████████████████████████▍  | 22/24 [00:02<00:00,  9.56it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████| 144/144 [00:48<00:00,  2.98it/s, loss=0.57, lr=3.72e-5]\u001b[A[NeMo I 2022-06-22 12:15:56 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         93.82      96.47      95.13       8659\n",
      "    B-Amenity (label_id: 1)                                 67.27      62.48      64.79        533\n",
      "    B-Cuisine (label_id: 2)                                 64.59      89.47      75.02        532\n",
      "    B-Dish (label_id: 3)                                    56.20      77.08      65.01        288\n",
      "    B-Hours (label_id: 4)                                    0.00       0.00       0.00        212\n",
      "    B-Location (label_id: 5)                                87.61      86.21      86.90        812\n",
      "    B-Price (label_id: 6)                                    0.00       0.00       0.00        171\n",
      "    B-Rating (label_id: 7)                                  37.89      91.04      53.51        201\n",
      "    B-Restaurant_Name (label_id: 8)                         55.78      82.84      66.67        402\n",
      "    I-Amenity (label_id: 9)                                 63.56      59.92      61.69        524\n",
      "    I-Cuisine (label_id: 10)                                 0.00       0.00       0.00        135\n",
      "    I-Dish (label_id: 11)                                    0.00       0.00       0.00        121\n",
      "    I-Hours (label_id: 12)                                  55.23      96.61      70.28        295\n",
      "    I-Location (label_id: 13)                               85.05      88.07      86.53        788\n",
      "    I-Price (label_id: 14)                                   0.00       0.00       0.00         66\n",
      "    I-Rating (label_id: 15)                                100.00       0.80       1.59        125\n",
      "    I-Restaurant_Name (label_id: 16)                        95.00       4.85       9.22        392\n",
      "    -------------------\n",
      "    micro avg                                               83.56      83.56      83.56      14256\n",
      "    macro avg                                               50.71      49.17      43.31      14256\n",
      "    weighted avg                                            81.81      83.56      80.67      14256\n",
      "    \n",
      "Epoch 0: 100%|█| 144/144 [00:48<00:00,  2.97it/s, loss=0.57, lr=3.72e-5, val_los\n",
      "                                                                                \u001b[AEpoch 0, global step 119: val_loss reached 0.49739 (best 0.49739), saving model to \"/workspace/mount/results/bert-base_ner/checkpoints/trained-model--val_loss=0.50-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  83%|▊| 120/144 [00:47<00:09,  2.55it/s, loss=0.316, lr=1.87e-5, val_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  85%|▊| 122/144 [00:47<00:08,  2.58it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:   8%|██▋                             | 2/24 [00:00<00:04,  5.20it/s]\u001b[A\n",
      "Epoch 1:  86%|▊| 124/144 [00:47<00:07,  2.61it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  17%|█████▎                          | 4/24 [00:00<00:02,  6.70it/s]\u001b[A\n",
      "Epoch 1:  88%|▉| 126/144 [00:48<00:06,  2.64it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  25%|████████                        | 6/24 [00:00<00:02,  7.84it/s]\u001b[A\n",
      "Epoch 1:  89%|▉| 128/144 [00:48<00:05,  2.67it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  33%|██████████▋                     | 8/24 [00:00<00:01,  8.61it/s]\u001b[A\n",
      "Epoch 1:  90%|▉| 130/144 [00:48<00:05,  2.70it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  42%|████████████▉                  | 10/24 [00:01<00:01,  8.93it/s]\u001b[A\n",
      "Epoch 1:  92%|▉| 132/144 [00:48<00:04,  2.73it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  50%|███████████████▌               | 12/24 [00:01<00:01,  9.15it/s]\u001b[A\n",
      "Epoch 1:  93%|▉| 134/144 [00:48<00:03,  2.76it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  58%|██████████████████             | 14/24 [00:01<00:01,  9.24it/s]\u001b[A\n",
      "Epoch 1:  94%|▉| 136/144 [00:49<00:02,  2.79it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  67%|████████████████████▋          | 16/24 [00:01<00:00,  9.32it/s]\u001b[A\n",
      "Epoch 1:  96%|▉| 138/144 [00:49<00:02,  2.82it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  75%|███████████████████████▎       | 18/24 [00:02<00:00,  9.30it/s]\u001b[A\n",
      "Epoch 1:  97%|▉| 140/144 [00:49<00:01,  2.84it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  83%|█████████████████████████▊     | 20/24 [00:02<00:00,  9.32it/s]\u001b[A\n",
      "Epoch 1:  99%|▉| 142/144 [00:49<00:00,  2.87it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  92%|████████████████████████████▍  | 22/24 [00:02<00:00,  9.36it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 144/144 [00:49<00:00,  2.90it/s, loss=0.316, lr=1.87e-5, val_lo\u001b[A[NeMo I 2022-06-22 12:17:10 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         96.27      95.78      96.03       8659\n",
      "    B-Amenity (label_id: 1)                                 73.17      76.74      74.91        533\n",
      "    B-Cuisine (label_id: 2)                                 89.51      86.65      88.06        532\n",
      "    B-Dish (label_id: 3)                                    82.55      85.42      83.96        288\n",
      "    B-Hours (label_id: 4)                                   70.71      66.04      68.29        212\n",
      "    B-Location (label_id: 5)                                89.19      90.39      89.79        812\n",
      "    B-Price (label_id: 6)                                   75.27      80.12      77.62        171\n",
      "    B-Rating (label_id: 7)                                  71.48      91.04      80.09        201\n",
      "    B-Restaurant_Name (label_id: 8)                         93.98      93.28      93.63        402\n",
      "    I-Amenity (label_id: 9)                                 77.40      78.44      77.91        524\n",
      "    I-Cuisine (label_id: 10)                                87.23      60.74      71.62        135\n",
      "    I-Dish (label_id: 11)                                   76.15      81.82      78.88        121\n",
      "    I-Hours (label_id: 12)                                  79.08      93.56      85.71        295\n",
      "    I-Location (label_id: 13)                               89.86      87.69      88.76        788\n",
      "    I-Price (label_id: 14)                                 100.00       6.06      11.43         66\n",
      "    I-Rating (label_id: 15)                                 79.31      73.60      76.35        125\n",
      "    I-Restaurant_Name (label_id: 16)                        87.32      93.11      90.12        392\n",
      "    -------------------\n",
      "    micro avg                                               91.18      91.18      91.18      14256\n",
      "    macro avg                                               83.44      78.85      78.42      14256\n",
      "    weighted avg                                            91.39      91.18      91.03      14256\n",
      "    \n",
      "Epoch 1: 100%|█| 144/144 [00:50<00:00,  2.89it/s, loss=0.316, lr=1.87e-5, val_lo\n",
      "                                                                                \u001b[AEpoch 1, global step 239: val_loss reached 0.28575 (best 0.28575), saving model to \"/workspace/mount/results/bert-base_ner/checkpoints/trained-model--val_loss=0.29-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  83%|▊| 120/144 [00:48<00:09,  2.51it/s, loss=0.232, lr=1.54e-7, val_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  85%|▊| 122/144 [00:48<00:08,  2.54it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:   8%|██▋                             | 2/24 [00:00<00:04,  5.07it/s]\u001b[A\n",
      "Epoch 2:  86%|▊| 124/144 [00:48<00:07,  2.57it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  17%|█████▎                          | 4/24 [00:00<00:03,  6.59it/s]\u001b[A\n",
      "Epoch 2:  88%|▉| 126/144 [00:48<00:06,  2.60it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  25%|████████                        | 6/24 [00:00<00:02,  7.76it/s]\u001b[A\n",
      "Epoch 2:  89%|▉| 128/144 [00:49<00:06,  2.63it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  33%|██████████▋                     | 8/24 [00:00<00:01,  8.49it/s]\u001b[A\n",
      "Epoch 2:  90%|▉| 130/144 [00:49<00:05,  2.66it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  42%|████████████▉                  | 10/24 [00:01<00:01,  8.80it/s]\u001b[A\n",
      "Epoch 2:  92%|▉| 132/144 [00:49<00:04,  2.69it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  50%|███████████████▌               | 12/24 [00:01<00:01,  9.03it/s]\u001b[A\n",
      "Epoch 2:  93%|▉| 134/144 [00:49<00:03,  2.72it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  58%|██████████████████             | 14/24 [00:01<00:01,  9.15it/s]\u001b[A\n",
      "Epoch 2:  94%|▉| 136/144 [00:49<00:02,  2.74it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  67%|████████████████████▋          | 16/24 [00:01<00:00,  9.22it/s]\u001b[A\n",
      "Epoch 2:  96%|▉| 138/144 [00:50<00:02,  2.77it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  75%|███████████████████████▎       | 18/24 [00:02<00:00,  9.25it/s]\u001b[A\n",
      "Epoch 2:  97%|▉| 140/144 [00:50<00:01,  2.80it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  83%|█████████████████████████▊     | 20/24 [00:02<00:00,  9.28it/s]\u001b[A\n",
      "Epoch 2:  99%|▉| 142/144 [00:50<00:00,  2.83it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  92%|████████████████████████████▍  | 22/24 [00:02<00:00,  9.28it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 144/144 [00:50<00:00,  2.86it/s, loss=0.232, lr=1.54e-7, val_lo\u001b[A[NeMo I 2022-06-22 12:18:24 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         96.97      95.29      96.12       8659\n",
      "    B-Amenity (label_id: 1)                                 74.18      76.55      75.35        533\n",
      "    B-Cuisine (label_id: 2)                                 86.48      87.78      87.13        532\n",
      "    B-Dish (label_id: 3)                                    83.86      82.99      83.42        288\n",
      "    B-Hours (label_id: 4)                                   69.64      73.58      71.56        212\n",
      "    B-Location (label_id: 5)                                90.10      90.76      90.43        812\n",
      "    B-Price (label_id: 6)                                   77.84      88.30      82.74        171\n",
      "    B-Rating (label_id: 7)                                  77.92      89.55      83.33        201\n",
      "    B-Restaurant_Name (label_id: 8)                         94.64      92.29      93.45        402\n",
      "    I-Amenity (label_id: 9)                                 75.31      80.34      77.75        524\n",
      "    I-Cuisine (label_id: 10)                                76.42      69.63      72.87        135\n",
      "    I-Dish (label_id: 11)                                   72.34      84.30      77.86        121\n",
      "    I-Hours (label_id: 12)                                  83.89      93.56      88.46        295\n",
      "    I-Location (label_id: 13)                               87.50      91.50      89.45        788\n",
      "    I-Price (label_id: 14)                                 100.00      28.79      44.71         66\n",
      "    I-Rating (label_id: 15)                                 76.52      80.80      78.60        125\n",
      "    I-Restaurant_Name (label_id: 16)                        90.67      89.29      89.97        392\n",
      "    -------------------\n",
      "    micro avg                                               91.50      91.50      91.50      14256\n",
      "    macro avg                                               83.19      82.08      81.36      14256\n",
      "    weighted avg                                            91.77      91.50      91.51      14256\n",
      "    \n",
      "Epoch 2: 100%|█| 144/144 [00:50<00:00,  2.85it/s, loss=0.232, lr=1.54e-7, val_lo\n",
      "                                                                                \u001b[AEpoch 2, global step 359: val_loss reached 0.27343 (best 0.27343), saving model to \"/workspace/mount/results/bert-base_ner/checkpoints/trained-model--val_loss=0.27-epoch=2.ckpt\" as top 3\n",
      "Epoch 2: 100%|█| 144/144 [01:14<00:00,  1.95it/s, loss=0.232, lr=1.54e-7, val_lo\n",
      "[NeMo I 2022-06-22 12:19:07 train:130] Experiment logs saved to '/workspace/mount/results/bert-base_ner'\n",
      "[NeMo I 2022-06-22 12:19:07 train:131] Trained model saved to '/workspace/mount/results/bert-base_ner/checkpoints/trained-model.tlt'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:19:09,892 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n",
      "CPU times: user 3.3 s, sys: 865 ms, total: 4.17 s\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# TAO train NER model - this takes few minutes\n",
    "!tao token_classification train \\\n",
    "    -e $SPECS_DIR/token_classification/train.yaml \\\n",
    "    -g 1  \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/bert-base_ner \\\n",
    "    data_dir={destination_mount}/data/restaurant \\\n",
    "    model.label_ids={destination_mount}/data/restaurant/label_ids.csv \\\n",
    "    trainer.max_epochs=3 \\\n",
    "    training_ds.num_samples=-1 \\\n",
    "    validation_ds.num_samples=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces `trained-model.tlt` saved at `$RESULTS_DIR/bert-base_ner/checkpoints/trained-model.tlt`. \n",
    "This file can be fed directly into the fine-tuning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.3 Faster Training with AMP\n",
    "There are a number of parameters you can change for training.  For example, the batch size (`training_ds.batch_size`) may influence the validation accuracy. Larger batch sizes are faster to train with, however, you may get slighly better results with smaller batches.\n",
    "\n",
    "An important consideration is the [Automatic Mixed Precision (AMP)](https://developer.nvidia.com/automatic-mixed-precision) setting.  To accelerate the training without loss of quality, it is possible to train with these parameters:  `trainer.amp_level=\"O1\"` and `trainer.precision=16` for reduced precision.\n",
    "\n",
    "Experiment by training again using mixed precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:20:22,578 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:20:22,710 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:20:22,747 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:20:27 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:27 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:27 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:27 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:20:28 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:20:29 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:20:29 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:20:29 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:29 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:33 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:33 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:33 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:33 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:20:34 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:20:34 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:20:34 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:20:34 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:34 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:20:35 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/train.py:139: UserWarning: \n",
      "    'train.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:20:35 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: ???\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/mount/results/bert-base_ner_fp16\n",
      "      exp_dir: null\n",
      "      name: trained-model\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: true\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: min\n",
      "        period: null\n",
      "        every_n_val_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .tlt\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "      files_to_copy: null\n",
      "    model:\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      label_ids: /workspace/mount/data/restaurant/label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: null\n",
      "        class_balancing: null\n",
      "        max_seq_length: 128\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 3\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 16\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O1\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices: null\n",
      "    training_ds:\n",
      "      batch_size: 64\n",
      "      text_file: text_train.txt\n",
      "      labels_file: labels_train.txt\n",
      "      shuffle: true\n",
      "      num_samples: -1\n",
      "    validation_ds:\n",
      "      batch_size: 64\n",
      "      text_file: text_dev.txt\n",
      "      labels_file: labels_dev.txt\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "    data_dir: /workspace/mount/data/restaurant\n",
      "    optim:\n",
      "      name: adam\n",
      "      lr: 5.0e-05\n",
      "      weight_decay: 0.0\n",
      "      sched:\n",
      "        name: WarmupAnnealing\n",
      "        warmup_steps: null\n",
      "        warmup_ratio: 0.1\n",
      "        last_epoch: -1\n",
      "        monitor: val_loss\n",
      "        reduce_on_plateau: false\n",
      "    encryption_key: '*****'\n",
      "    \n",
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-06-22 12:20:35 exp_manager:414] Exp_manager is logging to /workspace/mount/results/bert-base_ner_fp16, but it already exists.\n",
      "[NeMo I 2022-06-22 12:20:35 exp_manager:362] Resuming from /workspace/mount/results/bert-base_ner_fp16/checkpoints/trained-model--val_loss=0.28-epoch=2-last.ckpt\n",
      "[NeMo I 2022-06-22 12:20:35 exp_manager:220] Experiments will be logged at /workspace/mount/results/bert-base_ner_fp16\n",
      "[NeMo W 2022-06-22 12:20:35 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:240: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:20:35 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /workspace/mount/results/bert-base_ner_fp16/checkpoints exists and is not empty.\n",
      "      rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "    \n",
      "[NeMo I 2022-06-22 12:20:35 token_classification_model:58] Reusing label_ids file found at /workspace/mount/data/restaurant/label_ids.csv.\n",
      "[NeMo I 2022-06-22 12:20:35 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: None, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 139945533031664 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 31.0kB/s]\n",
      "Lock 139945533031664 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 139945533073488 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 757kB/s]\n",
      "Lock 139945533073488 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 139945533074112 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 61.4MB/s]\n",
      "Lock 139945533074112 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 139945533447712 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 67.1MB/s]\n",
      "Lock 139945533447712 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:20:36 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 139945532966704 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:04<00:00, 89.7MB/s]\n",
      "Lock 139945532966704 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:20:43 token_classification_model:103] Setting model.dataset.data_dir to /workspace/mount/data/restaurant.\n",
      "[NeMo I 2022-06-22 12:20:43 token_classification_utils:54] Processing /workspace/mount/data/restaurant/labels_train.txt\n",
      "[NeMo I 2022-06-22 12:20:43 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:20:43 token_classification_utils:90] Labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16} saved to : /workspace/mount/data/restaurant/label_ids.csv\n",
      "[NeMo I 2022-06-22 12:20:44 token_classification_utils:99] Three most popular labels in /workspace/mount/data/restaurant/labels_train.txt:\n",
      "[NeMo I 2022-06-22 12:20:44 data_preprocessing:194] label: 0, 43670 out of 70525 (61.92%).\n",
      "[NeMo I 2022-06-22 12:20:44 data_preprocessing:194] label: 5, 3817 out of 70525 (5.41%).\n",
      "[NeMo I 2022-06-22 12:20:44 data_preprocessing:194] label: 13, 3658 out of 70525 (5.19%).\n",
      "[NeMo I 2022-06-22 12:20:44 token_classification_utils:101] Total labels: 70525. Label frequencies - {0: 43670, 5: 3817, 13: 3658, 2: 2839, 9: 2676, 1: 2541, 8: 1901, 16: 1668, 3: 1475, 12: 1283, 7: 1070, 4: 990, 11: 767, 6: 730, 10: 630, 15: 527, 14: 283}\n",
      "[NeMo I 2022-06-22 12:20:44 token_classification_utils:107] Class weights restored from /workspace/mount/data/restaurant/labels_train_weights.p\n",
      "[NeMo I 2022-06-22 12:20:44 token_classification_dataset:277] features restored from /workspace/mount/data/restaurant/cached_text_train.txt_BertTokenizer_128_30522_-1\n",
      "[NeMo I 2022-06-22 12:20:44 token_classification_utils:54] Processing /workspace/mount/data/restaurant/labels_dev.txt\n",
      "[NeMo I 2022-06-22 12:20:44 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:20:44 token_classification_utils:96] /workspace/mount/data/restaurant/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-06-22 12:20:44 token_classification_dataset:277] features restored from /workspace/mount/data/restaurant/cached_text_dev.txt_BertTokenizer_128_30522_-1\n",
      "[NeMo I 2022-06-22 12:20:44 modelPT:544] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2022-06-22 12:20:44 lr_scheduler:621] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f479472cbe0>\" \n",
      "    will be used during training (effective maximum steps = 360) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 360\n",
      "    )\n",
      "Restoring states from the checkpoint file at /workspace/mount/results/bert-base_ner_fp16/checkpoints/trained-model--val_loss=0.28-epoch=2-last.ckpt\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2022-06-22 12:20:58 modelPT:415] No optimizer config provided, therefore no optimizer was created\n",
      "Restored all states from the checkpoint file at /workspace/mount/results/bert-base_ner_fp16/checkpoints/trained-model--val_loss=0.28-epoch=2-last.ckpt\n",
      "\n",
      "    | Name                                                   | Type                 | Params\n",
      "--------------------------------------------------------------------------------------------------\n",
      "0   | bert_model                                             | BertEncoder          | 109 M \n",
      "1   | bert_model.embeddings                                  | BertEmbeddings       | 23.8 M\n",
      "2   | bert_model.embeddings.word_embeddings                  | Embedding            | 23.4 M\n",
      "3   | bert_model.embeddings.position_embeddings              | Embedding            | 393 K \n",
      "4   | bert_model.embeddings.token_type_embeddings            | Embedding            | 1.5 K \n",
      "5   | bert_model.embeddings.LayerNorm                        | LayerNorm            | 1.5 K \n",
      "6   | bert_model.embeddings.dropout                          | Dropout              | 0     \n",
      "7   | bert_model.encoder                                     | BertEncoder          | 85.1 M\n",
      "8   | bert_model.encoder.layer                               | ModuleList           | 85.1 M\n",
      "9   | bert_model.encoder.layer.0                             | BertLayer            | 7.1 M \n",
      "10  | bert_model.encoder.layer.0.attention                   | BertAttention        | 2.4 M \n",
      "11  | bert_model.encoder.layer.0.attention.self              | BertSelfAttention    | 1.8 M \n",
      "12  | bert_model.encoder.layer.0.attention.self.query        | Linear               | 590 K \n",
      "13  | bert_model.encoder.layer.0.attention.self.key          | Linear               | 590 K \n",
      "14  | bert_model.encoder.layer.0.attention.self.value        | Linear               | 590 K \n",
      "15  | bert_model.encoder.layer.0.attention.self.dropout      | Dropout              | 0     \n",
      "16  | bert_model.encoder.layer.0.attention.output            | BertSelfOutput       | 592 K \n",
      "17  | bert_model.encoder.layer.0.attention.output.dense      | Linear               | 590 K \n",
      "18  | bert_model.encoder.layer.0.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "19  | bert_model.encoder.layer.0.attention.output.dropout    | Dropout              | 0     \n",
      "20  | bert_model.encoder.layer.0.intermediate                | BertIntermediate     | 2.4 M \n",
      "21  | bert_model.encoder.layer.0.intermediate.dense          | Linear               | 2.4 M \n",
      "22  | bert_model.encoder.layer.0.output                      | BertOutput           | 2.4 M \n",
      "23  | bert_model.encoder.layer.0.output.dense                | Linear               | 2.4 M \n",
      "24  | bert_model.encoder.layer.0.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "25  | bert_model.encoder.layer.0.output.dropout              | Dropout              | 0     \n",
      "26  | bert_model.encoder.layer.1                             | BertLayer            | 7.1 M \n",
      "27  | bert_model.encoder.layer.1.attention                   | BertAttention        | 2.4 M \n",
      "28  | bert_model.encoder.layer.1.attention.self              | BertSelfAttention    | 1.8 M \n",
      "29  | bert_model.encoder.layer.1.attention.self.query        | Linear               | 590 K \n",
      "30  | bert_model.encoder.layer.1.attention.self.key          | Linear               | 590 K \n",
      "31  | bert_model.encoder.layer.1.attention.self.value        | Linear               | 590 K \n",
      "32  | bert_model.encoder.layer.1.attention.self.dropout      | Dropout              | 0     \n",
      "33  | bert_model.encoder.layer.1.attention.output            | BertSelfOutput       | 592 K \n",
      "34  | bert_model.encoder.layer.1.attention.output.dense      | Linear               | 590 K \n",
      "35  | bert_model.encoder.layer.1.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "36  | bert_model.encoder.layer.1.attention.output.dropout    | Dropout              | 0     \n",
      "37  | bert_model.encoder.layer.1.intermediate                | BertIntermediate     | 2.4 M \n",
      "38  | bert_model.encoder.layer.1.intermediate.dense          | Linear               | 2.4 M \n",
      "39  | bert_model.encoder.layer.1.output                      | BertOutput           | 2.4 M \n",
      "40  | bert_model.encoder.layer.1.output.dense                | Linear               | 2.4 M \n",
      "41  | bert_model.encoder.layer.1.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "42  | bert_model.encoder.layer.1.output.dropout              | Dropout              | 0     \n",
      "43  | bert_model.encoder.layer.2                             | BertLayer            | 7.1 M \n",
      "44  | bert_model.encoder.layer.2.attention                   | BertAttention        | 2.4 M \n",
      "45  | bert_model.encoder.layer.2.attention.self              | BertSelfAttention    | 1.8 M \n",
      "46  | bert_model.encoder.layer.2.attention.self.query        | Linear               | 590 K \n",
      "47  | bert_model.encoder.layer.2.attention.self.key          | Linear               | 590 K \n",
      "48  | bert_model.encoder.layer.2.attention.self.value        | Linear               | 590 K \n",
      "49  | bert_model.encoder.layer.2.attention.self.dropout      | Dropout              | 0     \n",
      "50  | bert_model.encoder.layer.2.attention.output            | BertSelfOutput       | 592 K \n",
      "51  | bert_model.encoder.layer.2.attention.output.dense      | Linear               | 590 K \n",
      "52  | bert_model.encoder.layer.2.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "53  | bert_model.encoder.layer.2.attention.output.dropout    | Dropout              | 0     \n",
      "54  | bert_model.encoder.layer.2.intermediate                | BertIntermediate     | 2.4 M \n",
      "55  | bert_model.encoder.layer.2.intermediate.dense          | Linear               | 2.4 M \n",
      "56  | bert_model.encoder.layer.2.output                      | BertOutput           | 2.4 M \n",
      "57  | bert_model.encoder.layer.2.output.dense                | Linear               | 2.4 M \n",
      "58  | bert_model.encoder.layer.2.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "59  | bert_model.encoder.layer.2.output.dropout              | Dropout              | 0     \n",
      "60  | bert_model.encoder.layer.3                             | BertLayer            | 7.1 M \n",
      "61  | bert_model.encoder.layer.3.attention                   | BertAttention        | 2.4 M \n",
      "62  | bert_model.encoder.layer.3.attention.self              | BertSelfAttention    | 1.8 M \n",
      "63  | bert_model.encoder.layer.3.attention.self.query        | Linear               | 590 K \n",
      "64  | bert_model.encoder.layer.3.attention.self.key          | Linear               | 590 K \n",
      "65  | bert_model.encoder.layer.3.attention.self.value        | Linear               | 590 K \n",
      "66  | bert_model.encoder.layer.3.attention.self.dropout      | Dropout              | 0     \n",
      "67  | bert_model.encoder.layer.3.attention.output            | BertSelfOutput       | 592 K \n",
      "68  | bert_model.encoder.layer.3.attention.output.dense      | Linear               | 590 K \n",
      "69  | bert_model.encoder.layer.3.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "70  | bert_model.encoder.layer.3.attention.output.dropout    | Dropout              | 0     \n",
      "71  | bert_model.encoder.layer.3.intermediate                | BertIntermediate     | 2.4 M \n",
      "72  | bert_model.encoder.layer.3.intermediate.dense          | Linear               | 2.4 M \n",
      "73  | bert_model.encoder.layer.3.output                      | BertOutput           | 2.4 M \n",
      "74  | bert_model.encoder.layer.3.output.dense                | Linear               | 2.4 M \n",
      "75  | bert_model.encoder.layer.3.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "76  | bert_model.encoder.layer.3.output.dropout              | Dropout              | 0     \n",
      "77  | bert_model.encoder.layer.4                             | BertLayer            | 7.1 M \n",
      "78  | bert_model.encoder.layer.4.attention                   | BertAttention        | 2.4 M \n",
      "79  | bert_model.encoder.layer.4.attention.self              | BertSelfAttention    | 1.8 M \n",
      "80  | bert_model.encoder.layer.4.attention.self.query        | Linear               | 590 K \n",
      "81  | bert_model.encoder.layer.4.attention.self.key          | Linear               | 590 K \n",
      "82  | bert_model.encoder.layer.4.attention.self.value        | Linear               | 590 K \n",
      "83  | bert_model.encoder.layer.4.attention.self.dropout      | Dropout              | 0     \n",
      "84  | bert_model.encoder.layer.4.attention.output            | BertSelfOutput       | 592 K \n",
      "85  | bert_model.encoder.layer.4.attention.output.dense      | Linear               | 590 K \n",
      "86  | bert_model.encoder.layer.4.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "87  | bert_model.encoder.layer.4.attention.output.dropout    | Dropout              | 0     \n",
      "88  | bert_model.encoder.layer.4.intermediate                | BertIntermediate     | 2.4 M \n",
      "89  | bert_model.encoder.layer.4.intermediate.dense          | Linear               | 2.4 M \n",
      "90  | bert_model.encoder.layer.4.output                      | BertOutput           | 2.4 M \n",
      "91  | bert_model.encoder.layer.4.output.dense                | Linear               | 2.4 M \n",
      "92  | bert_model.encoder.layer.4.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "93  | bert_model.encoder.layer.4.output.dropout              | Dropout              | 0     \n",
      "94  | bert_model.encoder.layer.5                             | BertLayer            | 7.1 M \n",
      "95  | bert_model.encoder.layer.5.attention                   | BertAttention        | 2.4 M \n",
      "96  | bert_model.encoder.layer.5.attention.self              | BertSelfAttention    | 1.8 M \n",
      "97  | bert_model.encoder.layer.5.attention.self.query        | Linear               | 590 K \n",
      "98  | bert_model.encoder.layer.5.attention.self.key          | Linear               | 590 K \n",
      "99  | bert_model.encoder.layer.5.attention.self.value        | Linear               | 590 K \n",
      "100 | bert_model.encoder.layer.5.attention.self.dropout      | Dropout              | 0     \n",
      "101 | bert_model.encoder.layer.5.attention.output            | BertSelfOutput       | 592 K \n",
      "102 | bert_model.encoder.layer.5.attention.output.dense      | Linear               | 590 K \n",
      "103 | bert_model.encoder.layer.5.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "104 | bert_model.encoder.layer.5.attention.output.dropout    | Dropout              | 0     \n",
      "105 | bert_model.encoder.layer.5.intermediate                | BertIntermediate     | 2.4 M \n",
      "106 | bert_model.encoder.layer.5.intermediate.dense          | Linear               | 2.4 M \n",
      "107 | bert_model.encoder.layer.5.output                      | BertOutput           | 2.4 M \n",
      "108 | bert_model.encoder.layer.5.output.dense                | Linear               | 2.4 M \n",
      "109 | bert_model.encoder.layer.5.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "110 | bert_model.encoder.layer.5.output.dropout              | Dropout              | 0     \n",
      "111 | bert_model.encoder.layer.6                             | BertLayer            | 7.1 M \n",
      "112 | bert_model.encoder.layer.6.attention                   | BertAttention        | 2.4 M \n",
      "113 | bert_model.encoder.layer.6.attention.self              | BertSelfAttention    | 1.8 M \n",
      "114 | bert_model.encoder.layer.6.attention.self.query        | Linear               | 590 K \n",
      "115 | bert_model.encoder.layer.6.attention.self.key          | Linear               | 590 K \n",
      "116 | bert_model.encoder.layer.6.attention.self.value        | Linear               | 590 K \n",
      "117 | bert_model.encoder.layer.6.attention.self.dropout      | Dropout              | 0     \n",
      "118 | bert_model.encoder.layer.6.attention.output            | BertSelfOutput       | 592 K \n",
      "119 | bert_model.encoder.layer.6.attention.output.dense      | Linear               | 590 K \n",
      "120 | bert_model.encoder.layer.6.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "121 | bert_model.encoder.layer.6.attention.output.dropout    | Dropout              | 0     \n",
      "122 | bert_model.encoder.layer.6.intermediate                | BertIntermediate     | 2.4 M \n",
      "123 | bert_model.encoder.layer.6.intermediate.dense          | Linear               | 2.4 M \n",
      "124 | bert_model.encoder.layer.6.output                      | BertOutput           | 2.4 M \n",
      "125 | bert_model.encoder.layer.6.output.dense                | Linear               | 2.4 M \n",
      "126 | bert_model.encoder.layer.6.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "127 | bert_model.encoder.layer.6.output.dropout              | Dropout              | 0     \n",
      "128 | bert_model.encoder.layer.7                             | BertLayer            | 7.1 M \n",
      "129 | bert_model.encoder.layer.7.attention                   | BertAttention        | 2.4 M \n",
      "130 | bert_model.encoder.layer.7.attention.self              | BertSelfAttention    | 1.8 M \n",
      "131 | bert_model.encoder.layer.7.attention.self.query        | Linear               | 590 K \n",
      "132 | bert_model.encoder.layer.7.attention.self.key          | Linear               | 590 K \n",
      "133 | bert_model.encoder.layer.7.attention.self.value        | Linear               | 590 K \n",
      "134 | bert_model.encoder.layer.7.attention.self.dropout      | Dropout              | 0     \n",
      "135 | bert_model.encoder.layer.7.attention.output            | BertSelfOutput       | 592 K \n",
      "136 | bert_model.encoder.layer.7.attention.output.dense      | Linear               | 590 K \n",
      "137 | bert_model.encoder.layer.7.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "138 | bert_model.encoder.layer.7.attention.output.dropout    | Dropout              | 0     \n",
      "139 | bert_model.encoder.layer.7.intermediate                | BertIntermediate     | 2.4 M \n",
      "140 | bert_model.encoder.layer.7.intermediate.dense          | Linear               | 2.4 M \n",
      "141 | bert_model.encoder.layer.7.output                      | BertOutput           | 2.4 M \n",
      "142 | bert_model.encoder.layer.7.output.dense                | Linear               | 2.4 M \n",
      "143 | bert_model.encoder.layer.7.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "144 | bert_model.encoder.layer.7.output.dropout              | Dropout              | 0     \n",
      "145 | bert_model.encoder.layer.8                             | BertLayer            | 7.1 M \n",
      "146 | bert_model.encoder.layer.8.attention                   | BertAttention        | 2.4 M \n",
      "147 | bert_model.encoder.layer.8.attention.self              | BertSelfAttention    | 1.8 M \n",
      "148 | bert_model.encoder.layer.8.attention.self.query        | Linear               | 590 K \n",
      "149 | bert_model.encoder.layer.8.attention.self.key          | Linear               | 590 K \n",
      "150 | bert_model.encoder.layer.8.attention.self.value        | Linear               | 590 K \n",
      "151 | bert_model.encoder.layer.8.attention.self.dropout      | Dropout              | 0     \n",
      "152 | bert_model.encoder.layer.8.attention.output            | BertSelfOutput       | 592 K \n",
      "153 | bert_model.encoder.layer.8.attention.output.dense      | Linear               | 590 K \n",
      "154 | bert_model.encoder.layer.8.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "155 | bert_model.encoder.layer.8.attention.output.dropout    | Dropout              | 0     \n",
      "156 | bert_model.encoder.layer.8.intermediate                | BertIntermediate     | 2.4 M \n",
      "157 | bert_model.encoder.layer.8.intermediate.dense          | Linear               | 2.4 M \n",
      "158 | bert_model.encoder.layer.8.output                      | BertOutput           | 2.4 M \n",
      "159 | bert_model.encoder.layer.8.output.dense                | Linear               | 2.4 M \n",
      "160 | bert_model.encoder.layer.8.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "161 | bert_model.encoder.layer.8.output.dropout              | Dropout              | 0     \n",
      "162 | bert_model.encoder.layer.9                             | BertLayer            | 7.1 M \n",
      "163 | bert_model.encoder.layer.9.attention                   | BertAttention        | 2.4 M \n",
      "164 | bert_model.encoder.layer.9.attention.self              | BertSelfAttention    | 1.8 M \n",
      "165 | bert_model.encoder.layer.9.attention.self.query        | Linear               | 590 K \n",
      "166 | bert_model.encoder.layer.9.attention.self.key          | Linear               | 590 K \n",
      "167 | bert_model.encoder.layer.9.attention.self.value        | Linear               | 590 K \n",
      "168 | bert_model.encoder.layer.9.attention.self.dropout      | Dropout              | 0     \n",
      "169 | bert_model.encoder.layer.9.attention.output            | BertSelfOutput       | 592 K \n",
      "170 | bert_model.encoder.layer.9.attention.output.dense      | Linear               | 590 K \n",
      "171 | bert_model.encoder.layer.9.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "172 | bert_model.encoder.layer.9.attention.output.dropout    | Dropout              | 0     \n",
      "173 | bert_model.encoder.layer.9.intermediate                | BertIntermediate     | 2.4 M \n",
      "174 | bert_model.encoder.layer.9.intermediate.dense          | Linear               | 2.4 M \n",
      "175 | bert_model.encoder.layer.9.output                      | BertOutput           | 2.4 M \n",
      "176 | bert_model.encoder.layer.9.output.dense                | Linear               | 2.4 M \n",
      "177 | bert_model.encoder.layer.9.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "178 | bert_model.encoder.layer.9.output.dropout              | Dropout              | 0     \n",
      "179 | bert_model.encoder.layer.10                            | BertLayer            | 7.1 M \n",
      "180 | bert_model.encoder.layer.10.attention                  | BertAttention        | 2.4 M \n",
      "181 | bert_model.encoder.layer.10.attention.self             | BertSelfAttention    | 1.8 M \n",
      "182 | bert_model.encoder.layer.10.attention.self.query       | Linear               | 590 K \n",
      "183 | bert_model.encoder.layer.10.attention.self.key         | Linear               | 590 K \n",
      "184 | bert_model.encoder.layer.10.attention.self.value       | Linear               | 590 K \n",
      "185 | bert_model.encoder.layer.10.attention.self.dropout     | Dropout              | 0     \n",
      "186 | bert_model.encoder.layer.10.attention.output           | BertSelfOutput       | 592 K \n",
      "187 | bert_model.encoder.layer.10.attention.output.dense     | Linear               | 590 K \n",
      "188 | bert_model.encoder.layer.10.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "189 | bert_model.encoder.layer.10.attention.output.dropout   | Dropout              | 0     \n",
      "190 | bert_model.encoder.layer.10.intermediate               | BertIntermediate     | 2.4 M \n",
      "191 | bert_model.encoder.layer.10.intermediate.dense         | Linear               | 2.4 M \n",
      "192 | bert_model.encoder.layer.10.output                     | BertOutput           | 2.4 M \n",
      "193 | bert_model.encoder.layer.10.output.dense               | Linear               | 2.4 M \n",
      "194 | bert_model.encoder.layer.10.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "195 | bert_model.encoder.layer.10.output.dropout             | Dropout              | 0     \n",
      "196 | bert_model.encoder.layer.11                            | BertLayer            | 7.1 M \n",
      "197 | bert_model.encoder.layer.11.attention                  | BertAttention        | 2.4 M \n",
      "198 | bert_model.encoder.layer.11.attention.self             | BertSelfAttention    | 1.8 M \n",
      "199 | bert_model.encoder.layer.11.attention.self.query       | Linear               | 590 K \n",
      "200 | bert_model.encoder.layer.11.attention.self.key         | Linear               | 590 K \n",
      "201 | bert_model.encoder.layer.11.attention.self.value       | Linear               | 590 K \n",
      "202 | bert_model.encoder.layer.11.attention.self.dropout     | Dropout              | 0     \n",
      "203 | bert_model.encoder.layer.11.attention.output           | BertSelfOutput       | 592 K \n",
      "204 | bert_model.encoder.layer.11.attention.output.dense     | Linear               | 590 K \n",
      "205 | bert_model.encoder.layer.11.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "206 | bert_model.encoder.layer.11.attention.output.dropout   | Dropout              | 0     \n",
      "207 | bert_model.encoder.layer.11.intermediate               | BertIntermediate     | 2.4 M \n",
      "208 | bert_model.encoder.layer.11.intermediate.dense         | Linear               | 2.4 M \n",
      "209 | bert_model.encoder.layer.11.output                     | BertOutput           | 2.4 M \n",
      "210 | bert_model.encoder.layer.11.output.dense               | Linear               | 2.4 M \n",
      "211 | bert_model.encoder.layer.11.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "212 | bert_model.encoder.layer.11.output.dropout             | Dropout              | 0     \n",
      "213 | bert_model.pooler                                      | BertPooler           | 590 K \n",
      "214 | bert_model.pooler.dense                                | Linear               | 590 K \n",
      "215 | bert_model.pooler.activation                           | Tanh                 | 0     \n",
      "216 | classifier                                             | TokenClassifier      | 603 K \n",
      "217 | classifier.dropout                                     | Dropout              | 0     \n",
      "218 | classifier.mlp                                         | MultiLayerPerceptron | 603 K \n",
      "219 | classifier.mlp.layer0                                  | Linear               | 590 K \n",
      "220 | classifier.mlp.layer2                                  | Linear               | 13.1 K\n",
      "221 | loss                                                   | CrossEntropyLoss     | 0     \n",
      "222 | classification_report                                  | ClassificationReport | 0     \n",
      "--------------------------------------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.344   Total estimated model params size (MB)\n",
      "Validation sanity check:  50%|██████████          | 1/2 [00:00<00:00,  1.01it/s][NeMo I 2022-06-22 12:21:00 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         97.78      92.65      95.15        667\n",
      "    B-Amenity (label_id: 1)                                 69.64      79.59      74.29         49\n",
      "    B-Cuisine (label_id: 2)                                 87.72      92.59      90.09         54\n",
      "    B-Dish (label_id: 3)                                    81.82      78.26      80.00         23\n",
      "    B-Hours (label_id: 4)                                   60.87      73.68      66.67         19\n",
      "    B-Location (label_id: 5)                                92.31      91.30      91.80         92\n",
      "    B-Price (label_id: 6)                                   64.71      91.67      75.86         12\n",
      "    B-Rating (label_id: 7)                                  83.33     100.00      90.91         15\n",
      "    B-Restaurant_Name (label_id: 8)                         92.86      92.86      92.86         14\n",
      "    I-Amenity (label_id: 9)                                 71.64      85.71      78.05         56\n",
      "    I-Cuisine (label_id: 10)                                90.91      66.67      76.92         15\n",
      "    I-Dish (label_id: 11)                                   70.00      77.78      73.68          9\n",
      "    I-Hours (label_id: 12)                                  77.55      97.44      86.36         39\n",
      "    I-Location (label_id: 13)                               92.31      93.20      92.75        103\n",
      "    I-Price (label_id: 14)                                   0.00       0.00       0.00          6\n",
      "    I-Rating (label_id: 15)                                 85.71     100.00      92.31          6\n",
      "    I-Restaurant_Name (label_id: 16)                        95.00     100.00      97.44         19\n",
      "    -------------------\n",
      "    micro avg                                               90.65      90.65      90.65       1198\n",
      "    macro avg                                               77.30      83.14      79.71       1198\n",
      "    weighted avg                                            91.06      90.65      90.66       1198\n",
      "    \n",
      "[NeMo I 2022-06-22 12:21:01 train:130] Experiment logs saved to '/workspace/mount/results/bert-base_ner_fp16'\n",
      "[NeMo I 2022-06-22 12:21:01 train:131] Trained model saved to '/workspace/mount/results/bert-base_ner_fp16/checkpoints/trained-model.tlt'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:21:03,056 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n",
      "CPU times: user 500 ms, sys: 146 ms, total: 646 ms\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# TAO train NER model with mixed precision\n",
    "!tao token_classification train \\\n",
    "    -e $SPECS_DIR/token_classification/train.yaml \\\n",
    "    -g 1  \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/bert-base_ner_fp16 \\\n",
    "    data_dir={destination_mount}/data/restaurant \\\n",
    "    model.label_ids={destination_mount}/data/restaurant/label_ids.csv \\\n",
    "    trainer.max_epochs=3 \\\n",
    "    trainer.amp_level=\"O1\" \\\n",
    "    trainer.precision=16 \\\n",
    "    training_ds.num_samples=-1 \\\n",
    "    validation_ds.num_samples=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two trainings with and without AMP in terms of:\n",
    "- Training duration?\n",
    "- NER Model performance?\n",
    "\n",
    "Discuss your observations with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.4 Change the Language Model\n",
    "\n",
    "Before training the NER classifier, the input text is encoded using a language model. TAO Toolkit supports four BERT and Megatron language models: \n",
    "- `bert-base-cased`\n",
    "- `bert-base-uncased`\n",
    "- `megatron-bert-345m-cased`\n",
    "- `megatron-bert-345m-uncased`\n",
    "\n",
    "By default, TAO Toolkit uses `bert-base-uncased`. This is the encoder you've used so far for training. To specify a different language model, add the `pretrained_model_name` argument to the launch command:\n",
    "```python\n",
    "    model.language_model.pretrained_model_name=<language-model-name>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:21:32,208 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:21:32,339 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:21:32,375 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:21:36 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:36 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:36 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:36 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:21:37 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:21:37 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:21:38 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:21:38 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:38 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:42 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:42 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:42 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:42 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:21:43 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:21:43 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:21:43 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:21:43 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:43 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:21:44 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/train.py:139: UserWarning: \n",
      "    'train.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:21:44 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: ???\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/mount/results/megatron-base_ner5\n",
      "      exp_dir: null\n",
      "      name: trained-model\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: true\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: min\n",
      "        period: null\n",
      "        every_n_val_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .tlt\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "      files_to_copy: null\n",
      "    model:\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: megatron-bert-345m-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      label_ids: /workspace/mount/data/restaurant/label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: null\n",
      "        class_balancing: null\n",
      "        max_seq_length: 128\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 3\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 16\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O1\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices: null\n",
      "    training_ds:\n",
      "      batch_size: 64\n",
      "      text_file: text_train.txt\n",
      "      labels_file: labels_train.txt\n",
      "      shuffle: true\n",
      "      num_samples: -1\n",
      "    validation_ds:\n",
      "      batch_size: 64\n",
      "      text_file: text_dev.txt\n",
      "      labels_file: labels_dev.txt\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "    data_dir: /workspace/mount/data/restaurant\n",
      "    optim:\n",
      "      name: adam\n",
      "      lr: 5.0e-05\n",
      "      weight_decay: 0.0\n",
      "      sched:\n",
      "        name: WarmupAnnealing\n",
      "        warmup_steps: null\n",
      "        warmup_ratio: 0.1\n",
      "        last_epoch: -1\n",
      "        monitor: val_loss\n",
      "        reduce_on_plateau: false\n",
      "    encryption_key: '*****'\n",
      "    \n",
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-06-22 12:21:44 exp_manager:414] Exp_manager is logging to /workspace/mount/results/megatron-base_ner5, but it already exists.\n",
      "[NeMo W 2022-06-22 12:21:44 exp_manager:332] There was no checkpoint folder at checkpoint_dir :/workspace/mount/results/megatron-base_ner5/checkpoints. Training from scratch.\n",
      "[NeMo I 2022-06-22 12:21:44 exp_manager:220] Experiments will be logged at /workspace/mount/results/megatron-base_ner5\n",
      "[NeMo I 2022-06-22 12:21:44 token_classification_model:58] Reusing label_ids file found at /workspace/mount/data/restaurant/label_ids.csv.\n",
      "[NeMo I 2022-06-22 12:21:44 megatron_utils:274] Downloading from https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\n",
      "100% [........................................................] 231508 / 231508[NeMo I 2022-06-22 12:21:44 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-large-uncased, vocab_file: /root/.cache/torch/megatron/megatron-bert-345m-uncased_vocab, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 140321939902128 acquired on /root/.cache/huggingface/transformers/300ecd79785b4602752c0085f8a89c3f0232ef367eda291c79a5600f3778b677.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 37.7kB/s]\n",
      "Lock 140321939902128 released on /root/.cache/huggingface/transformers/300ecd79785b4602752c0085f8a89c3f0232ef367eda291c79a5600f3778b677.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 140321940621344 acquired on /root/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d.lock\n",
      "Downloading: 100%|██████████████████████████████| 571/571 [00:00<00:00, 772kB/s]\n",
      "Lock 140321940621344 released on /root/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d.lock\n",
      "Lock 140321940082112 acquired on /root/.cache/huggingface/transformers/e12f02d630da91a0982ce6db1ad595231d155a2b725ab106971898276d842ecc.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 74.4MB/s]\n",
      "Lock 140321940082112 released on /root/.cache/huggingface/transformers/e12f02d630da91a0982ce6db1ad595231d155a2b725ab106971898276d842ecc.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140321940080480 acquired on /root/.cache/huggingface/transformers/475d46024228961ca8770cead39e1079f135fd2441d14cf216727ffac8d41d78.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 49.7MB/s]\n",
      "Lock 140321940080480 released on /root/.cache/huggingface/transformers/475d46024228961ca8770cead39e1079f135fd2441d14cf216727ffac8d41d78.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:21:44 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2022-06-22 12:21:44 megatron_utils:274] Downloading from https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.0/files/release/mp_rank_00/model_optim_rng.pt\n",
      "100% [..................................................] 672662168 / 672662168using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 1\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_load ....................................... None\n",
      "  bias_dropout_fusion ............................. False\n",
      "  bias_gelu_fusion ................................ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... False\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... infer\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  DDP_impl ........................................ local\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  faiss_use_gpu ................................... False\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_allreduce .................................. False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 1\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... True\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_interval .................................... 100\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... False\n",
      "  no_load_rng ..................................... False\n",
      "  no_save_optim ................................... False\n",
      "  no_save_rng ..................................... False\n",
      "  num_attention_heads ............................. 16\n",
      "  num_layers ...................................... 24\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... True\n",
      "  openai_gelu ..................................... False\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  report_topk_accuracies .......................... []\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scaled_masked_softmax_fusion .................... False\n",
      "  scaled_upper_triang_masked_softmax_fusion ....... False\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... None\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 969, 30, 1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. BertWordPieceLowerCase\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_cpu_initialization .......................... False\n",
      "  use_one_sent_docs ............................... False\n",
      "  vocab_file ...................................... /root/.cache/huggingface/nemo_nlp_tmp/61306d8edc128c2e2f3207632faeaa45/tokenizer.vocab_file\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building BertWordPieceLowerCase tokenizer ...\n",
      " > padded vocab (size: 30522) with 70 dummy tokens (new size: 30592)\n",
      "[NeMo I 2022-06-22 12:22:14 megatron_bert:118] Megatron-lm argparse args: Namespace(DDP_impl='local', adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, adlr_autoresume=False, adlr_autoresume_interval=1000, apply_query_key_layer_scaling=True, apply_residual_connection_post_layernorm=False, attention_dropout=0.1, attention_softmax_in_fp32=False, bert_load=None, bias_dropout_fusion=False, bias_gelu_fusion=False, block_data_path=None, checkpoint_activations=False, checkpoint_num_layers=1, clip_grad=1.0, consumed_train_samples=0, consumed_valid_samples=0, data_impl='infer', data_parallel_size=1, data_path=None, distribute_checkpointed_activations=False, distributed_backend='nccl', eod_mask_loss=False, eval_interval=1000, eval_iters=100, exit_duration_in_mins=None, exit_interval=None, faiss_use_gpu=False, finetune=False, fp16=False, fp16_lm_cross_entropy=False, fp32_allreduce=False, fp32_residual_connection=False, global_batch_size=1, hidden_dropout=0.1, hidden_size=1024, hysteresis=2, ict_head_size=None, ict_load=None, indexer_batch_size=128, indexer_log_interval=1000, init_method_std=0.02, initial_loss_scale=4294967296, layernorm_epsilon=1e-05, lazy_mpu_init=True, load=None, local_rank=None, log_interval=100, loss_scale=None, loss_scale_window=1000, lr=None, lr_decay_iters=None, lr_decay_samples=None, lr_decay_style='linear', lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, make_vocab_size_divisible_by=128, mask_prob=0.15, max_position_embeddings=512, merge_file=None, micro_batch_size=1, min_loss_scale=1.0, min_lr=0.0, mmap_warmup=False, no_load_optim=False, no_load_rng=False, no_save_optim=False, no_save_rng=False, num_attention_heads=16, num_layers=24, num_workers=2, onnx_safe=True, openai_gelu=False, override_lr_scheduler=False, padded_vocab_size=30592, params_dtype=torch.float32, pipeline_model_parallel_size=1, query_in_block_prob=0.1, rampup_batch_size=None, rank=0, report_topk_accuracies=[], reset_attention_mask=False, reset_position_ids=False, save=None, save_interval=None, scaled_masked_softmax_fusion=False, scaled_upper_triang_masked_softmax_fusion=False, seed=1234, seq_length=None, short_seq_prob=0.1, split='969, 30, 1', tensor_model_parallel_size=1, tensorboard_dir=None, titles_data_path=None, tokenizer_type='BertWordPieceLowerCase', train_iters=None, train_samples=None, use_checkpoint_lr_scheduler=False, use_cpu_initialization=True, use_one_sent_docs=False, vocab_file='/root/.cache/huggingface/nemo_nlp_tmp/61306d8edc128c2e2f3207632faeaa45/tokenizer.vocab_file', weight_decay=0.01, world_size=1)\n",
      "[NeMo W 2022-06-22 12:22:17 megatron_bert:204] Megatron-lm checkpoint version not found. Setting checkpoint_version to 0.\n",
      "[NeMo I 2022-06-22 12:22:17 megatron_bert:212] Checkpoint loaded from from /root/.cache/torch/megatron/megatron-bert-345m-uncased\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_model:103] Setting model.dataset.data_dir to /workspace/mount/data/restaurant.\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_utils:54] Processing /workspace/mount/data/restaurant/labels_train.txt\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_utils:90] Labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16} saved to : /workspace/mount/data/restaurant/label_ids.csv\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_utils:99] Three most popular labels in /workspace/mount/data/restaurant/labels_train.txt:\n",
      "[NeMo I 2022-06-22 12:22:18 data_preprocessing:194] label: 0, 43670 out of 70525 (61.92%).\n",
      "[NeMo I 2022-06-22 12:22:18 data_preprocessing:194] label: 5, 3817 out of 70525 (5.41%).\n",
      "[NeMo I 2022-06-22 12:22:18 data_preprocessing:194] label: 13, 3658 out of 70525 (5.19%).\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_utils:101] Total labels: 70525. Label frequencies - {0: 43670, 5: 3817, 13: 3658, 2: 2839, 9: 2676, 1: 2541, 8: 1901, 16: 1668, 3: 1475, 12: 1283, 7: 1070, 4: 990, 11: 767, 6: 730, 10: 630, 15: 527, 14: 283}\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_utils:107] Class weights restored from /workspace/mount/data/restaurant/labels_train_weights.p\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_dataset:277] features restored from /workspace/mount/data/restaurant/cached_text_train.txt_BertTokenizer_128_30522_-1\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_utils:54] Processing /workspace/mount/data/restaurant/labels_dev.txt\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_utils:96] /workspace/mount/data/restaurant/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-06-22 12:22:18 token_classification_dataset:277] features restored from /workspace/mount/data/restaurant/cached_text_dev.txt_BertTokenizer_128_30522_-1\n",
      "[NeMo I 2022-06-22 12:22:18 modelPT:544] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2022-06-22 12:22:18 lr_scheduler:621] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f9f2bbc75b0>\" \n",
      "    will be used during training (effective maximum steps = 360) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 360\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2022-06-22 12:22:20 modelPT:415] No optimizer config provided, therefore no optimizer was created\n",
      "\n",
      "    | Name                                                                         | Type                     | Params\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "0   | bert_model                                                                   | MegatronBertEncoder      | 334 M \n",
      "1   | bert_model.language_model                                                    | TransformerLanguageModel | 334 M \n",
      "2   | bert_model.language_model.embedding                                          | Embedding                | 31.9 M\n",
      "3   | bert_model.language_model.embedding.word_embeddings                          | VocabParallelEmbedding   | 31.3 M\n",
      "4   | bert_model.language_model.embedding.position_embeddings                      | Embedding                | 524 K \n",
      "5   | bert_model.language_model.embedding.tokentype_embeddings                     | Embedding                | 2.0 K \n",
      "6   | bert_model.language_model.embedding.embedding_dropout                        | Dropout                  | 0     \n",
      "7   | bert_model.language_model.transformer                                        | ParallelTransformer      | 302 M \n",
      "8   | bert_model.language_model.transformer.layers                                 | ModuleList               | 302 M \n",
      "9   | bert_model.language_model.transformer.layers.0                               | ParallelTransformerLayer | 12.6 M\n",
      "10  | bert_model.language_model.transformer.layers.0.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "11  | bert_model.language_model.transformer.layers.0.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "12  | bert_model.language_model.transformer.layers.0.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "13  | bert_model.language_model.transformer.layers.0.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "14  | bert_model.language_model.transformer.layers.0.attention.attention_dropout   | Dropout                  | 0     \n",
      "15  | bert_model.language_model.transformer.layers.0.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "16  | bert_model.language_model.transformer.layers.0.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "17  | bert_model.language_model.transformer.layers.0.mlp                           | ParallelMLP              | 8.4 M \n",
      "18  | bert_model.language_model.transformer.layers.0.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "19  | bert_model.language_model.transformer.layers.0.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "20  | bert_model.language_model.transformer.layers.1                               | ParallelTransformerLayer | 12.6 M\n",
      "21  | bert_model.language_model.transformer.layers.1.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "22  | bert_model.language_model.transformer.layers.1.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "23  | bert_model.language_model.transformer.layers.1.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "24  | bert_model.language_model.transformer.layers.1.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "25  | bert_model.language_model.transformer.layers.1.attention.attention_dropout   | Dropout                  | 0     \n",
      "26  | bert_model.language_model.transformer.layers.1.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "27  | bert_model.language_model.transformer.layers.1.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "28  | bert_model.language_model.transformer.layers.1.mlp                           | ParallelMLP              | 8.4 M \n",
      "29  | bert_model.language_model.transformer.layers.1.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "30  | bert_model.language_model.transformer.layers.1.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "31  | bert_model.language_model.transformer.layers.2                               | ParallelTransformerLayer | 12.6 M\n",
      "32  | bert_model.language_model.transformer.layers.2.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "33  | bert_model.language_model.transformer.layers.2.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "34  | bert_model.language_model.transformer.layers.2.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "35  | bert_model.language_model.transformer.layers.2.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "36  | bert_model.language_model.transformer.layers.2.attention.attention_dropout   | Dropout                  | 0     \n",
      "37  | bert_model.language_model.transformer.layers.2.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "38  | bert_model.language_model.transformer.layers.2.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "39  | bert_model.language_model.transformer.layers.2.mlp                           | ParallelMLP              | 8.4 M \n",
      "40  | bert_model.language_model.transformer.layers.2.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "41  | bert_model.language_model.transformer.layers.2.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "42  | bert_model.language_model.transformer.layers.3                               | ParallelTransformerLayer | 12.6 M\n",
      "43  | bert_model.language_model.transformer.layers.3.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "44  | bert_model.language_model.transformer.layers.3.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "45  | bert_model.language_model.transformer.layers.3.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "46  | bert_model.language_model.transformer.layers.3.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "47  | bert_model.language_model.transformer.layers.3.attention.attention_dropout   | Dropout                  | 0     \n",
      "48  | bert_model.language_model.transformer.layers.3.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "49  | bert_model.language_model.transformer.layers.3.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "50  | bert_model.language_model.transformer.layers.3.mlp                           | ParallelMLP              | 8.4 M \n",
      "51  | bert_model.language_model.transformer.layers.3.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "52  | bert_model.language_model.transformer.layers.3.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "53  | bert_model.language_model.transformer.layers.4                               | ParallelTransformerLayer | 12.6 M\n",
      "54  | bert_model.language_model.transformer.layers.4.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "55  | bert_model.language_model.transformer.layers.4.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "56  | bert_model.language_model.transformer.layers.4.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "57  | bert_model.language_model.transformer.layers.4.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "58  | bert_model.language_model.transformer.layers.4.attention.attention_dropout   | Dropout                  | 0     \n",
      "59  | bert_model.language_model.transformer.layers.4.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "60  | bert_model.language_model.transformer.layers.4.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "61  | bert_model.language_model.transformer.layers.4.mlp                           | ParallelMLP              | 8.4 M \n",
      "62  | bert_model.language_model.transformer.layers.4.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "63  | bert_model.language_model.transformer.layers.4.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "64  | bert_model.language_model.transformer.layers.5                               | ParallelTransformerLayer | 12.6 M\n",
      "65  | bert_model.language_model.transformer.layers.5.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "66  | bert_model.language_model.transformer.layers.5.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "67  | bert_model.language_model.transformer.layers.5.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "68  | bert_model.language_model.transformer.layers.5.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "69  | bert_model.language_model.transformer.layers.5.attention.attention_dropout   | Dropout                  | 0     \n",
      "70  | bert_model.language_model.transformer.layers.5.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "71  | bert_model.language_model.transformer.layers.5.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "72  | bert_model.language_model.transformer.layers.5.mlp                           | ParallelMLP              | 8.4 M \n",
      "73  | bert_model.language_model.transformer.layers.5.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "74  | bert_model.language_model.transformer.layers.5.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "75  | bert_model.language_model.transformer.layers.6                               | ParallelTransformerLayer | 12.6 M\n",
      "76  | bert_model.language_model.transformer.layers.6.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "77  | bert_model.language_model.transformer.layers.6.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "78  | bert_model.language_model.transformer.layers.6.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "79  | bert_model.language_model.transformer.layers.6.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "80  | bert_model.language_model.transformer.layers.6.attention.attention_dropout   | Dropout                  | 0     \n",
      "81  | bert_model.language_model.transformer.layers.6.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "82  | bert_model.language_model.transformer.layers.6.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "83  | bert_model.language_model.transformer.layers.6.mlp                           | ParallelMLP              | 8.4 M \n",
      "84  | bert_model.language_model.transformer.layers.6.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "85  | bert_model.language_model.transformer.layers.6.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "86  | bert_model.language_model.transformer.layers.7                               | ParallelTransformerLayer | 12.6 M\n",
      "87  | bert_model.language_model.transformer.layers.7.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "88  | bert_model.language_model.transformer.layers.7.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "89  | bert_model.language_model.transformer.layers.7.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "90  | bert_model.language_model.transformer.layers.7.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "91  | bert_model.language_model.transformer.layers.7.attention.attention_dropout   | Dropout                  | 0     \n",
      "92  | bert_model.language_model.transformer.layers.7.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "93  | bert_model.language_model.transformer.layers.7.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "94  | bert_model.language_model.transformer.layers.7.mlp                           | ParallelMLP              | 8.4 M \n",
      "95  | bert_model.language_model.transformer.layers.7.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "96  | bert_model.language_model.transformer.layers.7.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "97  | bert_model.language_model.transformer.layers.8                               | ParallelTransformerLayer | 12.6 M\n",
      "98  | bert_model.language_model.transformer.layers.8.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "99  | bert_model.language_model.transformer.layers.8.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "100 | bert_model.language_model.transformer.layers.8.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "101 | bert_model.language_model.transformer.layers.8.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "102 | bert_model.language_model.transformer.layers.8.attention.attention_dropout   | Dropout                  | 0     \n",
      "103 | bert_model.language_model.transformer.layers.8.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "104 | bert_model.language_model.transformer.layers.8.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "105 | bert_model.language_model.transformer.layers.8.mlp                           | ParallelMLP              | 8.4 M \n",
      "106 | bert_model.language_model.transformer.layers.8.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "107 | bert_model.language_model.transformer.layers.8.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "108 | bert_model.language_model.transformer.layers.9                               | ParallelTransformerLayer | 12.6 M\n",
      "109 | bert_model.language_model.transformer.layers.9.input_layernorm               | FusedLayerNorm           | 2.0 K \n",
      "110 | bert_model.language_model.transformer.layers.9.attention                     | ParallelSelfAttention    | 4.2 M \n",
      "111 | bert_model.language_model.transformer.layers.9.attention.query_key_value     | ColumnParallelLinear     | 3.1 M \n",
      "112 | bert_model.language_model.transformer.layers.9.attention.scale_mask_softmax  | FusedScaleMaskSoftmax    | 0     \n",
      "113 | bert_model.language_model.transformer.layers.9.attention.attention_dropout   | Dropout                  | 0     \n",
      "114 | bert_model.language_model.transformer.layers.9.attention.dense               | RowParallelLinear        | 1.0 M \n",
      "115 | bert_model.language_model.transformer.layers.9.post_attention_layernorm      | FusedLayerNorm           | 2.0 K \n",
      "116 | bert_model.language_model.transformer.layers.9.mlp                           | ParallelMLP              | 8.4 M \n",
      "117 | bert_model.language_model.transformer.layers.9.mlp.dense_h_to_4h             | ColumnParallelLinear     | 4.2 M \n",
      "118 | bert_model.language_model.transformer.layers.9.mlp.dense_4h_to_h             | RowParallelLinear        | 4.2 M \n",
      "119 | bert_model.language_model.transformer.layers.10                              | ParallelTransformerLayer | 12.6 M\n",
      "120 | bert_model.language_model.transformer.layers.10.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "121 | bert_model.language_model.transformer.layers.10.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "122 | bert_model.language_model.transformer.layers.10.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "123 | bert_model.language_model.transformer.layers.10.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "124 | bert_model.language_model.transformer.layers.10.attention.attention_dropout  | Dropout                  | 0     \n",
      "125 | bert_model.language_model.transformer.layers.10.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "126 | bert_model.language_model.transformer.layers.10.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "127 | bert_model.language_model.transformer.layers.10.mlp                          | ParallelMLP              | 8.4 M \n",
      "128 | bert_model.language_model.transformer.layers.10.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "129 | bert_model.language_model.transformer.layers.10.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "130 | bert_model.language_model.transformer.layers.11                              | ParallelTransformerLayer | 12.6 M\n",
      "131 | bert_model.language_model.transformer.layers.11.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "132 | bert_model.language_model.transformer.layers.11.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "133 | bert_model.language_model.transformer.layers.11.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "134 | bert_model.language_model.transformer.layers.11.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "135 | bert_model.language_model.transformer.layers.11.attention.attention_dropout  | Dropout                  | 0     \n",
      "136 | bert_model.language_model.transformer.layers.11.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "137 | bert_model.language_model.transformer.layers.11.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "138 | bert_model.language_model.transformer.layers.11.mlp                          | ParallelMLP              | 8.4 M \n",
      "139 | bert_model.language_model.transformer.layers.11.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "140 | bert_model.language_model.transformer.layers.11.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "141 | bert_model.language_model.transformer.layers.12                              | ParallelTransformerLayer | 12.6 M\n",
      "142 | bert_model.language_model.transformer.layers.12.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "143 | bert_model.language_model.transformer.layers.12.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "144 | bert_model.language_model.transformer.layers.12.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "145 | bert_model.language_model.transformer.layers.12.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "146 | bert_model.language_model.transformer.layers.12.attention.attention_dropout  | Dropout                  | 0     \n",
      "147 | bert_model.language_model.transformer.layers.12.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "148 | bert_model.language_model.transformer.layers.12.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "149 | bert_model.language_model.transformer.layers.12.mlp                          | ParallelMLP              | 8.4 M \n",
      "150 | bert_model.language_model.transformer.layers.12.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "151 | bert_model.language_model.transformer.layers.12.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "152 | bert_model.language_model.transformer.layers.13                              | ParallelTransformerLayer | 12.6 M\n",
      "153 | bert_model.language_model.transformer.layers.13.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "154 | bert_model.language_model.transformer.layers.13.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "155 | bert_model.language_model.transformer.layers.13.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "156 | bert_model.language_model.transformer.layers.13.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "157 | bert_model.language_model.transformer.layers.13.attention.attention_dropout  | Dropout                  | 0     \n",
      "158 | bert_model.language_model.transformer.layers.13.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "159 | bert_model.language_model.transformer.layers.13.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "160 | bert_model.language_model.transformer.layers.13.mlp                          | ParallelMLP              | 8.4 M \n",
      "161 | bert_model.language_model.transformer.layers.13.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "162 | bert_model.language_model.transformer.layers.13.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "163 | bert_model.language_model.transformer.layers.14                              | ParallelTransformerLayer | 12.6 M\n",
      "164 | bert_model.language_model.transformer.layers.14.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "165 | bert_model.language_model.transformer.layers.14.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "166 | bert_model.language_model.transformer.layers.14.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "167 | bert_model.language_model.transformer.layers.14.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "168 | bert_model.language_model.transformer.layers.14.attention.attention_dropout  | Dropout                  | 0     \n",
      "169 | bert_model.language_model.transformer.layers.14.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "170 | bert_model.language_model.transformer.layers.14.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "171 | bert_model.language_model.transformer.layers.14.mlp                          | ParallelMLP              | 8.4 M \n",
      "172 | bert_model.language_model.transformer.layers.14.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "173 | bert_model.language_model.transformer.layers.14.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "174 | bert_model.language_model.transformer.layers.15                              | ParallelTransformerLayer | 12.6 M\n",
      "175 | bert_model.language_model.transformer.layers.15.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "176 | bert_model.language_model.transformer.layers.15.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "177 | bert_model.language_model.transformer.layers.15.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "178 | bert_model.language_model.transformer.layers.15.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "179 | bert_model.language_model.transformer.layers.15.attention.attention_dropout  | Dropout                  | 0     \n",
      "180 | bert_model.language_model.transformer.layers.15.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "181 | bert_model.language_model.transformer.layers.15.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "182 | bert_model.language_model.transformer.layers.15.mlp                          | ParallelMLP              | 8.4 M \n",
      "183 | bert_model.language_model.transformer.layers.15.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "184 | bert_model.language_model.transformer.layers.15.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "185 | bert_model.language_model.transformer.layers.16                              | ParallelTransformerLayer | 12.6 M\n",
      "186 | bert_model.language_model.transformer.layers.16.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "187 | bert_model.language_model.transformer.layers.16.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "188 | bert_model.language_model.transformer.layers.16.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "189 | bert_model.language_model.transformer.layers.16.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "190 | bert_model.language_model.transformer.layers.16.attention.attention_dropout  | Dropout                  | 0     \n",
      "191 | bert_model.language_model.transformer.layers.16.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "192 | bert_model.language_model.transformer.layers.16.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "193 | bert_model.language_model.transformer.layers.16.mlp                          | ParallelMLP              | 8.4 M \n",
      "194 | bert_model.language_model.transformer.layers.16.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "195 | bert_model.language_model.transformer.layers.16.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "196 | bert_model.language_model.transformer.layers.17                              | ParallelTransformerLayer | 12.6 M\n",
      "197 | bert_model.language_model.transformer.layers.17.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "198 | bert_model.language_model.transformer.layers.17.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "199 | bert_model.language_model.transformer.layers.17.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "200 | bert_model.language_model.transformer.layers.17.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "201 | bert_model.language_model.transformer.layers.17.attention.attention_dropout  | Dropout                  | 0     \n",
      "202 | bert_model.language_model.transformer.layers.17.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "203 | bert_model.language_model.transformer.layers.17.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "204 | bert_model.language_model.transformer.layers.17.mlp                          | ParallelMLP              | 8.4 M \n",
      "205 | bert_model.language_model.transformer.layers.17.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "206 | bert_model.language_model.transformer.layers.17.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "207 | bert_model.language_model.transformer.layers.18                              | ParallelTransformerLayer | 12.6 M\n",
      "208 | bert_model.language_model.transformer.layers.18.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "209 | bert_model.language_model.transformer.layers.18.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "210 | bert_model.language_model.transformer.layers.18.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "211 | bert_model.language_model.transformer.layers.18.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "212 | bert_model.language_model.transformer.layers.18.attention.attention_dropout  | Dropout                  | 0     \n",
      "213 | bert_model.language_model.transformer.layers.18.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "214 | bert_model.language_model.transformer.layers.18.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "215 | bert_model.language_model.transformer.layers.18.mlp                          | ParallelMLP              | 8.4 M \n",
      "216 | bert_model.language_model.transformer.layers.18.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "217 | bert_model.language_model.transformer.layers.18.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "218 | bert_model.language_model.transformer.layers.19                              | ParallelTransformerLayer | 12.6 M\n",
      "219 | bert_model.language_model.transformer.layers.19.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "220 | bert_model.language_model.transformer.layers.19.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "221 | bert_model.language_model.transformer.layers.19.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "222 | bert_model.language_model.transformer.layers.19.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "223 | bert_model.language_model.transformer.layers.19.attention.attention_dropout  | Dropout                  | 0     \n",
      "224 | bert_model.language_model.transformer.layers.19.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "225 | bert_model.language_model.transformer.layers.19.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "226 | bert_model.language_model.transformer.layers.19.mlp                          | ParallelMLP              | 8.4 M \n",
      "227 | bert_model.language_model.transformer.layers.19.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "228 | bert_model.language_model.transformer.layers.19.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "229 | bert_model.language_model.transformer.layers.20                              | ParallelTransformerLayer | 12.6 M\n",
      "230 | bert_model.language_model.transformer.layers.20.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "231 | bert_model.language_model.transformer.layers.20.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "232 | bert_model.language_model.transformer.layers.20.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "233 | bert_model.language_model.transformer.layers.20.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "234 | bert_model.language_model.transformer.layers.20.attention.attention_dropout  | Dropout                  | 0     \n",
      "235 | bert_model.language_model.transformer.layers.20.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "236 | bert_model.language_model.transformer.layers.20.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "237 | bert_model.language_model.transformer.layers.20.mlp                          | ParallelMLP              | 8.4 M \n",
      "238 | bert_model.language_model.transformer.layers.20.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "239 | bert_model.language_model.transformer.layers.20.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "240 | bert_model.language_model.transformer.layers.21                              | ParallelTransformerLayer | 12.6 M\n",
      "241 | bert_model.language_model.transformer.layers.21.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "242 | bert_model.language_model.transformer.layers.21.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "243 | bert_model.language_model.transformer.layers.21.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "244 | bert_model.language_model.transformer.layers.21.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "245 | bert_model.language_model.transformer.layers.21.attention.attention_dropout  | Dropout                  | 0     \n",
      "246 | bert_model.language_model.transformer.layers.21.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "247 | bert_model.language_model.transformer.layers.21.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "248 | bert_model.language_model.transformer.layers.21.mlp                          | ParallelMLP              | 8.4 M \n",
      "249 | bert_model.language_model.transformer.layers.21.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "250 | bert_model.language_model.transformer.layers.21.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "251 | bert_model.language_model.transformer.layers.22                              | ParallelTransformerLayer | 12.6 M\n",
      "252 | bert_model.language_model.transformer.layers.22.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "253 | bert_model.language_model.transformer.layers.22.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "254 | bert_model.language_model.transformer.layers.22.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "255 | bert_model.language_model.transformer.layers.22.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "256 | bert_model.language_model.transformer.layers.22.attention.attention_dropout  | Dropout                  | 0     \n",
      "257 | bert_model.language_model.transformer.layers.22.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "258 | bert_model.language_model.transformer.layers.22.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "259 | bert_model.language_model.transformer.layers.22.mlp                          | ParallelMLP              | 8.4 M \n",
      "260 | bert_model.language_model.transformer.layers.22.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "261 | bert_model.language_model.transformer.layers.22.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "262 | bert_model.language_model.transformer.layers.23                              | ParallelTransformerLayer | 12.6 M\n",
      "263 | bert_model.language_model.transformer.layers.23.input_layernorm              | FusedLayerNorm           | 2.0 K \n",
      "264 | bert_model.language_model.transformer.layers.23.attention                    | ParallelSelfAttention    | 4.2 M \n",
      "265 | bert_model.language_model.transformer.layers.23.attention.query_key_value    | ColumnParallelLinear     | 3.1 M \n",
      "266 | bert_model.language_model.transformer.layers.23.attention.scale_mask_softmax | FusedScaleMaskSoftmax    | 0     \n",
      "267 | bert_model.language_model.transformer.layers.23.attention.attention_dropout  | Dropout                  | 0     \n",
      "268 | bert_model.language_model.transformer.layers.23.attention.dense              | RowParallelLinear        | 1.0 M \n",
      "269 | bert_model.language_model.transformer.layers.23.post_attention_layernorm     | FusedLayerNorm           | 2.0 K \n",
      "270 | bert_model.language_model.transformer.layers.23.mlp                          | ParallelMLP              | 8.4 M \n",
      "271 | bert_model.language_model.transformer.layers.23.mlp.dense_h_to_4h            | ColumnParallelLinear     | 4.2 M \n",
      "272 | bert_model.language_model.transformer.layers.23.mlp.dense_4h_to_h            | RowParallelLinear        | 4.2 M \n",
      "273 | bert_model.language_model.transformer.final_layernorm                        | FusedLayerNorm           | 2.0 K \n",
      "274 | classifier                                                                   | TokenClassifier          | 1.1 M \n",
      "275 | classifier.dropout                                                           | Dropout                  | 0     \n",
      "276 | classifier.mlp                                                               | MultiLayerPerceptron     | 1.1 M \n",
      "277 | classifier.mlp.layer0                                                        | Linear                   | 1.0 M \n",
      "278 | classifier.mlp.layer2                                                        | Linear                   | 17.4 K\n",
      "279 | loss                                                                         | CrossEntropyLoss         | 0     \n",
      "280 | classification_report                                                        | ClassificationReport     | 0     \n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "335 M     Trainable params\n",
      "0         Non-trainable params\n",
      "335 M     Total params\n",
      "1,340.924 Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/2 [00:00<?, ?it/s]torch distributed is already initialized, skipping initialization ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "Validation sanity check: 100%|████████████████████| 2/2 [00:01<00:00,  1.07s/it][NeMo I 2022-06-22 12:22:23 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         25.00       0.75       1.46        667\n",
      "    B-Amenity (label_id: 1)                                  0.00       0.00       0.00         49\n",
      "    B-Cuisine (label_id: 2)                                  0.00       0.00       0.00         54\n",
      "    B-Dish (label_id: 3)                                     6.67       4.35       5.26         23\n",
      "    B-Hours (label_id: 4)                                    0.00       0.00       0.00         19\n",
      "    B-Location (label_id: 5)                                 0.00       0.00       0.00         92\n",
      "    B-Price (label_id: 6)                                    0.91      25.00       1.75         12\n",
      "    B-Rating (label_id: 7)                                   3.37      60.00       6.38         15\n",
      "    B-Restaurant_Name (label_id: 8)                          1.02       7.14       1.79         14\n",
      "    I-Amenity (label_id: 9)                                  0.72       1.79       1.03         56\n",
      "    I-Cuisine (label_id: 10)                                 2.70       6.67       3.85         15\n",
      "    I-Dish (label_id: 11)                                    0.00       0.00       0.00          9\n",
      "    I-Hours (label_id: 12)                                   0.00       0.00       0.00         39\n",
      "    I-Location (label_id: 13)                              100.00       3.88       7.48        103\n",
      "    I-Price (label_id: 14)                                   0.00       0.00       0.00          6\n",
      "    I-Rating (label_id: 15)                                  0.00       0.00       0.00          6\n",
      "    I-Restaurant_Name (label_id: 16)                         3.54      21.05       6.06         19\n",
      "    -------------------\n",
      "    micro avg                                                2.42       2.42       2.42       1198\n",
      "    macro avg                                                8.47       7.68       2.06       1198\n",
      "    weighted avg                                            22.83       2.42       1.86       1198\n",
      "    \n",
      "Epoch 0:   0%|                                | 0/144 [00:00<00:00, 4650.00it/s][W reducer.cpp:1151] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  83%|██████▋ | 120/144 [01:10<00:14,  1.70it/s, loss=0.299, lr=3.72e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  85%|██████▊ | 122/144 [01:11<00:12,  1.73it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:   8%|██▋                             | 2/24 [00:00<00:04,  4.64it/s]\u001b[A\n",
      "Epoch 0:  86%|██████▉ | 124/144 [01:11<00:11,  1.75it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  17%|█████▎                          | 4/24 [00:00<00:03,  6.07it/s]\u001b[A\n",
      "Epoch 0:  88%|███████ | 126/144 [01:11<00:10,  1.77it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  25%|████████                        | 6/24 [00:00<00:02,  7.16it/s]\u001b[A\n",
      "Epoch 0:  89%|███████ | 128/144 [01:11<00:08,  1.79it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  33%|██████████▋                     | 8/24 [00:01<00:02,  7.87it/s]\u001b[A\n",
      "Epoch 0:  90%|███████▏| 130/144 [01:12<00:07,  1.81it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  42%|████████████▉                  | 10/24 [00:01<00:01,  8.26it/s]\u001b[A\n",
      "Epoch 0:  92%|███████▎| 132/144 [01:12<00:06,  1.84it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  50%|███████████████▌               | 12/24 [00:01<00:01,  8.48it/s]\u001b[A\n",
      "Epoch 0:  93%|███████▍| 134/144 [01:12<00:05,  1.86it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  58%|██████████████████             | 14/24 [00:01<00:01,  8.54it/s]\u001b[A\n",
      "Epoch 0:  94%|███████▌| 136/144 [01:12<00:04,  1.88it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  67%|████████████████████▋          | 16/24 [00:01<00:00,  8.57it/s]\u001b[A\n",
      "Epoch 0:  96%|███████▋| 138/144 [01:13<00:03,  1.90it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  75%|███████████████████████▎       | 18/24 [00:02<00:00,  8.61it/s]\u001b[A\n",
      "Epoch 0:  97%|███████▊| 140/144 [01:13<00:02,  1.92it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  83%|█████████████████████████▊     | 20/24 [00:02<00:00,  8.67it/s]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 142/144 [01:13<00:01,  1.94it/s, loss=0.299, lr=3.72e-5]\u001b[A\n",
      "Validating:  92%|████████████████████████████▍  | 22/24 [00:02<00:00,  8.65it/s]\u001b[A\n",
      "Epoch 0: 100%|████████| 144/144 [01:13<00:00,  1.96it/s, loss=0.299, lr=3.72e-5]\u001b[A[NeMo I 2022-06-22 12:23:37 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         96.74      94.81      95.77       8659\n",
      "    B-Amenity (label_id: 1)                                 70.73      78.42      74.38        533\n",
      "    B-Cuisine (label_id: 2)                                 81.90      89.29      85.43        532\n",
      "    B-Dish (label_id: 3)                                    78.59      85.42      81.86        288\n",
      "    B-Hours (label_id: 4)                                   68.33      71.23      69.75        212\n",
      "    B-Location (label_id: 5)                                89.60      90.15      89.87        812\n",
      "    B-Price (label_id: 6)                                   76.88      89.47      82.70        171\n",
      "    B-Rating (label_id: 7)                                  78.48      92.54      84.93        201\n",
      "    B-Restaurant_Name (label_id: 8)                         95.80      90.80      93.23        402\n",
      "    I-Amenity (label_id: 9)                                 76.45      80.53      78.44        524\n",
      "    I-Cuisine (label_id: 10)                                72.95      65.93      69.26        135\n",
      "    I-Dish (label_id: 11)                                   77.69      77.69      77.69        121\n",
      "    I-Hours (label_id: 12)                                  83.70      90.51      86.97        295\n",
      "    I-Location (label_id: 13)                               87.98      90.99      89.46        788\n",
      "    I-Price (label_id: 14)                                  93.55      43.94      59.79         66\n",
      "    I-Rating (label_id: 15)                                 85.71      81.60      83.61        125\n",
      "    I-Restaurant_Name (label_id: 16)                        94.30      84.44      89.10        392\n",
      "    -------------------\n",
      "    micro avg                                               91.10      91.10      91.10      14256\n",
      "    macro avg                                               82.90      82.22      81.90      14256\n",
      "    weighted avg                                            91.43      91.10      91.17      14256\n",
      "    \n",
      "Epoch 0: 100%|█| 144/144 [01:13<00:00,  1.96it/s, loss=0.299, lr=3.72e-5, val_lo\n",
      "Epoch 1:  83%|▊| 120/144 [01:11<00:14,  1.69it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  85%|▊| 122/144 [01:12<00:12,  1.71it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:   8%|██▋                             | 2/24 [00:00<00:04,  4.59it/s]\u001b[A\n",
      "Epoch 1:  86%|▊| 124/144 [01:12<00:11,  1.73it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  17%|█████▎                          | 4/24 [00:00<00:03,  5.95it/s]\u001b[A\n",
      "Epoch 1:  88%|▉| 126/144 [01:12<00:10,  1.75it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  25%|████████                        | 6/24 [00:00<00:02,  6.98it/s]\u001b[A\n",
      "Epoch 1:  89%|▉| 128/144 [01:12<00:09,  1.77it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  33%|██████████▋                     | 8/24 [00:01<00:02,  7.60it/s]\u001b[A\n",
      "Epoch 1:  90%|▉| 130/144 [01:13<00:07,  1.79it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  42%|████████████▉                  | 10/24 [00:01<00:01,  7.98it/s]\u001b[A\n",
      "Epoch 1:  92%|▉| 132/144 [01:13<00:06,  1.82it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  50%|███████████████▌               | 12/24 [00:01<00:01,  8.18it/s]\u001b[A\n",
      "Epoch 1:  93%|▉| 134/144 [01:13<00:05,  1.84it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  58%|██████████████████             | 14/24 [00:01<00:01,  8.29it/s]\u001b[A\n",
      "Epoch 1:  94%|▉| 136/144 [01:13<00:04,  1.86it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  67%|████████████████████▋          | 16/24 [00:02<00:00,  8.37it/s]\u001b[A\n",
      "Epoch 1:  96%|▉| 138/144 [01:13<00:03,  1.88it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  75%|███████████████████████▎       | 18/24 [00:02<00:00,  8.38it/s]\u001b[A\n",
      "Epoch 1:  97%|▉| 140/144 [01:14<00:02,  1.90it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  83%|█████████████████████████▊     | 20/24 [00:02<00:00,  8.39it/s]\u001b[A\n",
      "Epoch 1:  99%|▉| 142/144 [01:14<00:01,  1.92it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A\n",
      "Validating:  92%|████████████████████████████▍  | 22/24 [00:02<00:00,  8.38it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 144/144 [01:14<00:00,  1.94it/s, loss=0.229, lr=1.87e-5, val_lo\u001b[A[NeMo I 2022-06-22 12:24:51 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         96.84      95.65      96.24       8659\n",
      "    B-Amenity (label_id: 1)                                 75.04      81.80      78.28        533\n",
      "    B-Cuisine (label_id: 2)                                 90.77      86.84      88.76        532\n",
      "    B-Dish (label_id: 3)                                    87.28      85.76      86.51        288\n",
      "    B-Hours (label_id: 4)                                   69.04      77.83      73.17        212\n",
      "    B-Location (label_id: 5)                                91.06      90.27      90.66        812\n",
      "    B-Price (label_id: 6)                                   83.06      88.89      85.88        171\n",
      "    B-Rating (label_id: 7)                                  84.76      88.56      86.62        201\n",
      "    B-Restaurant_Name (label_id: 8)                         92.44      94.28      93.35        402\n",
      "    I-Amenity (label_id: 9)                                 78.72      84.73      81.62        524\n",
      "    I-Cuisine (label_id: 10)                                81.25      77.04      79.09        135\n",
      "    I-Dish (label_id: 11)                                   78.46      84.30      81.27        121\n",
      "    I-Hours (label_id: 12)                                  84.91      91.53      88.09        295\n",
      "    I-Location (label_id: 13)                               90.23      90.23      90.23        788\n",
      "    I-Price (label_id: 14)                                  81.48      66.67      73.33         66\n",
      "    I-Rating (label_id: 15)                                 92.04      83.20      87.39        125\n",
      "    I-Restaurant_Name (label_id: 16)                        91.00      90.31      90.65        392\n",
      "    -------------------\n",
      "    micro avg                                               92.36      92.36      92.36      14256\n",
      "    macro avg                                               85.20      85.76      85.36      14256\n",
      "    weighted avg                                            92.55      92.36      92.43      14256\n",
      "    \n",
      "Epoch 1: 100%|█| 144/144 [01:14<00:00,  1.94it/s, loss=0.229, lr=1.87e-5, val_lo\n",
      "Epoch 2:  83%|▊| 120/144 [01:12<00:14,  1.66it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  85%|▊| 122/144 [01:12<00:13,  1.69it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:   8%|██▋                             | 2/24 [00:00<00:04,  4.62it/s]\u001b[A\n",
      "Epoch 2:  86%|▊| 124/144 [01:13<00:11,  1.71it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  17%|█████▎                          | 4/24 [00:00<00:03,  5.98it/s]\u001b[A\n",
      "Epoch 2:  88%|▉| 126/144 [01:13<00:10,  1.73it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  25%|████████                        | 6/24 [00:00<00:02,  7.01it/s]\u001b[A\n",
      "Epoch 2:  89%|▉| 128/144 [01:13<00:09,  1.75it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  33%|██████████▋                     | 8/24 [00:01<00:02,  7.62it/s]\u001b[A\n",
      "Epoch 2:  90%|▉| 130/144 [01:13<00:07,  1.77it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  42%|████████████▉                  | 10/24 [00:01<00:01,  7.97it/s]\u001b[A\n",
      "Epoch 2:  92%|▉| 132/144 [01:14<00:06,  1.79it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  50%|███████████████▌               | 12/24 [00:01<00:01,  8.14it/s]\u001b[A\n",
      "Epoch 2:  93%|▉| 134/144 [01:14<00:05,  1.81it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  58%|██████████████████             | 14/24 [00:01<00:01,  8.24it/s]\u001b[A\n",
      "Epoch 2:  94%|▉| 136/144 [01:14<00:04,  1.84it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  67%|████████████████████▋          | 16/24 [00:02<00:00,  8.27it/s]\u001b[A\n",
      "Epoch 2:  96%|▉| 138/144 [01:14<00:03,  1.86it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  75%|███████████████████████▎       | 18/24 [00:02<00:00,  8.30it/s]\u001b[A\n",
      "Epoch 2:  97%|▉| 140/144 [01:15<00:02,  1.88it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  83%|█████████████████████████▊     | 20/24 [00:02<00:00,  8.34it/s]\u001b[A\n",
      "Epoch 2:  99%|▉| 142/144 [01:15<00:01,  1.90it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "Validating:  92%|████████████████████████████▍  | 22/24 [00:02<00:00,  8.33it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 144/144 [01:15<00:00,  1.92it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A[NeMo I 2022-06-22 12:26:07 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         97.45      95.03      96.23       8659\n",
      "    B-Amenity (label_id: 1)                                 75.22      81.99      78.46        533\n",
      "    B-Cuisine (label_id: 2)                                 88.47      87.97      88.22        532\n",
      "    B-Dish (label_id: 3)                                    84.30      85.76      85.03        288\n",
      "    B-Hours (label_id: 4)                                   69.23      80.66      74.51        212\n",
      "    B-Location (label_id: 5)                                90.70      91.26      90.98        812\n",
      "    B-Price (label_id: 6)                                   82.51      88.30      85.31        171\n",
      "    B-Rating (label_id: 7)                                  83.33      89.55      86.33        201\n",
      "    B-Restaurant_Name (label_id: 8)                         96.13      92.79      94.43        402\n",
      "    I-Amenity (label_id: 9)                                 76.19      85.50      80.58        524\n",
      "    I-Cuisine (label_id: 10)                                74.13      78.52      76.26        135\n",
      "    I-Dish (label_id: 11)                                   75.74      85.12      80.16        121\n",
      "    I-Hours (label_id: 12)                                  84.00      92.54      88.06        295\n",
      "    I-Location (label_id: 13)                               89.31      92.26      90.76        788\n",
      "    I-Price (label_id: 14)                                  75.81      71.21      73.44         66\n",
      "    I-Rating (label_id: 15)                                 86.29      85.60      85.94        125\n",
      "    I-Restaurant_Name (label_id: 16)                        94.26      88.01      91.03        392\n",
      "    -------------------\n",
      "    micro avg                                               92.26      92.26      92.26      14256\n",
      "    macro avg                                               83.71      86.59      85.04      14256\n",
      "    weighted avg                                            92.60      92.26      92.38      14256\n",
      "    \n",
      "Epoch 2: 100%|█| 144/144 [01:15<00:00,  1.91it/s, loss=0.174, lr=1.54e-7, val_lo\n",
      "Epoch 2: 100%|█| 144/144 [01:15<00:00,  1.91it/s, loss=0.174, lr=1.54e-7, val_lo\u001b[A\n",
      "[NeMo I 2022-06-22 12:26:11 train:130] Experiment logs saved to '/workspace/mount/results/megatron-base_ner5'\n",
      "[NeMo I 2022-06-22 12:26:11 train:131] Trained model saved to '/workspace/mount/results/megatron-base_ner5/checkpoints/trained-model.tlt'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:26:13,432 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n",
      "CPU times: user 5.33 s, sys: 4.72 s, total: 10.1 s\n",
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# TAO train NER model with Megatron. This takes few minutes\n",
    "!tao token_classification train \\\n",
    "    -e $SPECS_DIR/token_classification/train.yaml \\\n",
    "    -g 1  \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/megatron-base_ner5 \\\n",
    "    data_dir={destination_mount}/data/restaurant \\\n",
    "    model.label_ids={destination_mount}/data/restaurant/label_ids.csv \\\n",
    "    exp_manager.create_checkpoint_callback=false\\\n",
    "    trainer.amp_level=\"O1\" \\\n",
    "    trainer.precision=16 \\\n",
    "    training_ds.num_samples=-1 \\\n",
    "    validation_ds.num_samples=-1 \\\n",
    "    trainer.max_epochs=3 \\\n",
    "    model.language_model.pretrained_model_name=megatron-bert-345m-uncased "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.5 Evaluate the Trained Model\n",
    "\n",
    "For the token classification task, several metrics are recorded for the evaluation:\n",
    "- Test loss\n",
    "- F1 score, precision and recall per class\n",
    "- F1 score, precision and recall aggregated with micro, macro and weighted average\n",
    "\n",
    "Check out [this article](https://towardsdatascience.com/performance-metrics-confusion-matrix-precision-recall-and-f1-score-a8fe076a2262) to larn more about the performance metrics. \n",
    "\n",
    "The evaluation spec YAML is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
      "# TLT Spec file for evaluation of a Token Classification model\n",
      "\n",
      "# Name of the .tlt from which the model will be loaded.\n",
      "restore_from: trained-model.tlt\n",
      "\n",
      "data_dir: ???\n",
      "\n",
      "# Test settings: dataset.\n",
      "test_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  batch_size: 1\n",
      "  shuffle: false\n",
      "  num_samples: -1 # number of samples to be considered, -1 means the whole the dataset\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation spec file \n",
    "!cat $source_mount/specs/token_classification/evaluate.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `data_dir` is not defined, which is an indication that we should override it in the command.  To evaluate the model, we use `tao text_classification evaluate` and override `data_dir`. Other arguments follow the same pattern as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:26:52,114 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:26:52,250 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:26:52,287 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:26:57 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:26:57 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:26:57 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:26:57 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:26:57 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:26:57 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:26:58 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:26:58 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:26:58 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:27:02 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:27:02 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:27:02 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:27:02 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:27:03 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:27:03 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:27:03 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:27:03 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:27:03 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:27:04 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/evaluate.py:96: UserWarning: \n",
      "    'evaluate.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:27:04 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /workspace/mount/results/bert-base_ner/checkpoints/trained-model.tlt\n",
      "    data_dir: /workspace/mount/data/restaurant\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/mount/results/bert-base_ner/evaluate\n",
      "      exp_dir: null\n",
      "      name: null\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: false\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: false\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: min\n",
      "        period: null\n",
      "        every_n_val_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .nemo\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "      files_to_copy: null\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 1000\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 32\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O2\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices: null\n",
      "    test_ds:\n",
      "      batch_size: 1\n",
      "      text_file: text_dev.txt\n",
      "      labels_file: labels_dev.txt\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "    encryption_key: '*****'\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-06-22 12:27:04 exp_manager:414] Exp_manager is logging to /workspace/mount/results/bert-base_ner/evaluate, but it already exists.\n",
      "[NeMo I 2022-06-22 12:27:04 exp_manager:220] Experiments will be logged at /workspace/mount/results/bert-base_ner/evaluate\n",
      "[NeMo I 2022-06-22 12:27:07 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmpbwlqdnyp/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 139913160632736 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 40.4kB/s]\n",
      "Lock 139913160632736 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 139913161732784 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|█████████████████████████████| 570/570 [00:00<00:00, 1.02MB/s]\n",
      "Lock 139913161732784 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 139913160632640 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 80.6MB/s]\n",
      "Lock 139913160632640 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 139913160854928 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 84.7MB/s]\n",
      "Lock 139913160854928 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:27:07 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2022-06-22 12:27:07 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 139913160475696 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|████████████████████████████| 440M/440M [00:04<00:00, 100MB/s]\n",
      "Lock 139913160475696 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:27:21 token_classification_model:103] Setting model.dataset.data_dir to /workspace/mount/data/restaurant.\n",
      "[NeMo I 2022-06-22 12:27:21 token_classification_utils:54] Processing /workspace/mount/data/restaurant/labels_dev.txt\n",
      "[NeMo I 2022-06-22 12:27:21 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:27:21 token_classification_utils:96] /workspace/mount/data/restaurant/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-06-22 12:27:21 token_classification_dataset:277] features restored from /workspace/mount/data/restaurant/cached_text_dev.txt_BertTokenizer_128_30522_-1\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Testing: 100%|██████████████████████████████| 1521/1521 [00:38<00:00, 40.30it/s][NeMo I 2022-06-22 12:28:00 token_classification_model:202] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         96.97      95.29      96.12       8659\n",
      "    B-Amenity (label_id: 1)                                 74.18      76.55      75.35        533\n",
      "    B-Cuisine (label_id: 2)                                 86.48      87.78      87.13        532\n",
      "    B-Dish (label_id: 3)                                    83.86      82.99      83.42        288\n",
      "    B-Hours (label_id: 4)                                   69.64      73.58      71.56        212\n",
      "    B-Location (label_id: 5)                                90.10      90.76      90.43        812\n",
      "    B-Price (label_id: 6)                                   77.84      88.30      82.74        171\n",
      "    B-Rating (label_id: 7)                                  77.92      89.55      83.33        201\n",
      "    B-Restaurant_Name (label_id: 8)                         94.64      92.29      93.45        402\n",
      "    I-Amenity (label_id: 9)                                 75.31      80.34      77.75        524\n",
      "    I-Cuisine (label_id: 10)                                76.42      69.63      72.87        135\n",
      "    I-Dish (label_id: 11)                                   72.34      84.30      77.86        121\n",
      "    I-Hours (label_id: 12)                                  83.89      93.56      88.46        295\n",
      "    I-Location (label_id: 13)                               87.50      91.50      89.45        788\n",
      "    I-Price (label_id: 14)                                 100.00      28.79      44.71         66\n",
      "    I-Rating (label_id: 15)                                 76.52      80.80      78.60        125\n",
      "    I-Restaurant_Name (label_id: 16)                        90.67      89.29      89.97        392\n",
      "    -------------------\n",
      "    micro avg                                               91.50      91.50      91.50      14256\n",
      "    macro avg                                               83.19      82.08      81.36      14256\n",
      "    weighted avg                                            91.77      91.50      91.51      14256\n",
      "    \n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'f1': 81.36471557617188,\n",
      " 'precision': 83.193359375,\n",
      " 'recall': 82.07637023925781,\n",
      " 'test_loss': 0.2558504641056061}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████████████████████████| 1521/1521 [00:38<00:00, 39.51it/s]\n",
      "[NeMo I 2022-06-22 12:28:00 evaluate:92] Experiment logs saved to '/workspace/mount/results/bert-base_ner/evaluate'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:28:02,248 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# TAO evaluate NER model\n",
    "!tao token_classification evaluate  \\\n",
    "   -e $SPECS_DIR/token_classification/evaluate.yaml \\\n",
    "   -r $RESULTS_DIR/bert-base_ner/evaluate \\\n",
    "   -g 1 \\\n",
    "   -m $RESULTS_DIR/bert-base_ner/checkpoints/trained-model.tlt \\\n",
    "   -k $KEY \\\n",
    "   data_dir={destination_mount}/data/restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe the F1 score, precision and recall metrics per class:\n",
    "\n",
    "```\n",
    "    label                                                precision    recall       f1           support   \n",
    "    O (label_id: 0)                                         96.67      95.61      96.14       8659\n",
    "    B-Amenity (label_id: 1)                                 74.37      77.30      75.80        533\n",
    "    B-Cuisine (label_id: 2)                                 89.57      85.53      87.50        532\n",
    "    B-Dish (label_id: 3)                                    82.83      85.42      84.10        288\n",
    "    B-Hours (label_id: 4)                                   71.74      77.83      74.66        212\n",
    "    B-Location (label_id: 5)                                90.48      90.15      90.31        812\n",
    "    B-Price (label_id: 6)                                   82.97      88.30      85.55        171\n",
    "    B-Rating (label_id: 7)                                  79.00      86.07      82.38        201\n",
    "    B-Restaurant_Name (label_id: 8)                         94.90      92.54      93.70        402\n",
    "    I-Amenity (label_id: 9)                                 75.99      80.92      78.37        524\n",
    "    I-Cuisine (label_id: 10)                                74.26      74.81      74.54        135\n",
    "    I-Dish (label_id: 11)                                   71.23      85.95      77.90        121\n",
    "    I-Hours (label_id: 12)                                  84.42      91.86      87.99        295\n",
    "    I-Location (label_id: 13)                               90.04      90.61      90.32        788\n",
    "    I-Price (label_id: 14)                                  89.80      66.67      76.52         66\n",
    "    I-Rating (label_id: 15)                                 84.87      80.80      82.79        125\n",
    "    I-Restaurant_Name (label_id: 16)                        93.40      90.31      91.83        392\n",
    "    -------------------\n",
    "    micro avg                                               91.88      91.88      91.88      14256\n",
    "    macro avg                                               83.91      84.75      84.14      14256\n",
    "    weighted avg                                            92.07      91.88      91.94      14256\n",
    "```\n",
    "\n",
    "you can also observe the test loss (`test_loss`) and the aggregated F1 score, precision, recall metrics on the entire test set:\n",
    "```\n",
    "DATALOADER:0 TEST RESULTS\n",
    "{'f1': tensor(84.1423, device='cuda:0'),\n",
    " 'precision': tensor(83.9138, device='cuda:0'),\n",
    " 'recall': tensor(84.7453, device='cuda:0'),\n",
    " 'test_loss': tensor(0.2424, device='cuda:0')}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 NER Fine-Tuning\n",
    "\n",
    "The TAO Toolkit command for fine-tuning is very similar to that of training. Instead of `tao text_classification train`, use `tao text_classification finetune`.  This command will generate a fine-tuned model `finetuned-model.tlt` at `$RESULTS_DIR/bert-base-finetuned_ner/checkpoints`. \n",
    "\n",
    "The fine-tuning process will start with the trained model weights instead of random weights for the token classification model.  \n",
    "Specify the model checkpoint from the previously trained model with the `-m` argument and specify the spec file corresponding to fine-tuning. \n",
    "\n",
    "The token classification fine-tuning of TAO allows users to:\n",
    "- Fine-tune the token classifier on additional data\n",
    "- Fine-tune on a subset of labels by removing or merging entities in the dataset\n",
    "\n",
    "For this demonstration, as \"Cuisine\" and \"Dish\" labels are very close semantically in our context, we will merge them and keep one entity, the \"Dish\" label.  The merged data is set up in the directory `tao/data/restaurant_finetune`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "772\n"
     ]
    }
   ],
   "source": [
    "# We should not find any \"Cuisine\" labels as they have been renamed \"Dish\"\n",
    "!grep Cuisine $source_mount/data/restaurant_finetune/labels_dev.txt |wc -l\n",
    "!grep Dish $source_mount/data/restaurant_finetune/labels_dev.txt |wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
      "# TLT Spec file for finetuning of the pre-trained TokenClassification model\n",
      "\n",
      "\n",
      "data_dir: ???\n",
      "\n",
      "# Fine-tuning settings: training dataset.\n",
      "finetuning_ds:\n",
      "  num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
      "\n",
      "# Fine-tuning settings: validation dataset.\n",
      "validation_ds:\n",
      "  num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
      "\n",
      "# Fine-tuning settings: different optimizer.\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 2e-5\n",
      "\n",
      "trainer:\n",
      "  max_epochs: 3"
     ]
    }
   ],
   "source": [
    "# Print the fine-tuning spec file \n",
    "!cat $source_mount/specs/token_classification/finetune.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:28:25,984 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:28:26,127 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:28:26,166 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:28:30 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:30 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:30 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:30 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:28:31 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:28:31 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:28:31 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:28:31 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:31 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:36 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:36 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:36 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:36 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:28:37 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:28:37 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:28:37 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:28:37 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:37 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:28:38 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/finetune.py:136: UserWarning: \n",
      "    'finetune.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:28:38 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /workspace/mount/results/bert-base_ner/checkpoints/trained-model.tlt\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/mount/results/bert-base-finetuned_ner/\n",
      "      exp_dir: null\n",
      "      name: finetuned-model\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: true\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: min\n",
      "        period: null\n",
      "        every_n_val_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .tlt\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "      files_to_copy: null\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 2\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 16\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O1\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices: null\n",
      "    data_dir: /workspace/mount/data/restaurant_finetune\n",
      "    finetuning_ds:\n",
      "      batch_size: 32\n",
      "      text_file: text_train.txt\n",
      "      labels_file: labels_train.txt\n",
      "      shuffle: true\n",
      "      num_samples: -1\n",
      "    validation_ds:\n",
      "      batch_size: 32\n",
      "      text_file: text_dev.txt\n",
      "      labels_file: labels_dev.txt\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "    optim:\n",
      "      name: adam\n",
      "      lr: 2.0e-05\n",
      "    encryption_key: '****'\n",
      "    \n",
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-06-22 12:28:38 exp_manager:414] Exp_manager is logging to /workspace/mount/results/bert-base-finetuned_ner/, but it already exists.\n",
      "[NeMo W 2022-06-22 12:28:38 exp_manager:332] There was no checkpoint folder at checkpoint_dir :/workspace/mount/results/bert-base-finetuned_ner/checkpoints. Training from scratch.\n",
      "[NeMo I 2022-06-22 12:28:38 exp_manager:220] Experiments will be logged at /workspace/mount/results/bert-base-finetuned_ner\n",
      "[NeMo W 2022-06-22 12:28:38 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:240: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2022-06-22 12:28:40 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmpoewrv20v/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 140384397204256 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 22.8kB/s]\n",
      "Lock 140384397204256 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 140384396812928 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 555kB/s]\n",
      "Lock 140384396812928 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 140384396858128 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 39.2MB/s]\n",
      "Lock 140384396858128 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140384396861152 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 72.5MB/s]\n",
      "Lock 140384396861152 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:28:41 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2022-06-22 12:28:41 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 140384396772880 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|████████████████████████████| 440M/440M [00:04<00:00, 105MB/s]\n",
      "Lock 140384396772880 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:28:55 finetune:110] Model restored from '/workspace/mount/results/bert-base_ner/checkpoints/trained-model.tlt'\n",
      "[NeMo I 2022-06-22 12:28:55 token_classification_model:103] Setting model.dataset.data_dir to /workspace/mount/data/restaurant_finetune.\n",
      "[NeMo I 2022-06-22 12:28:55 token_classification_utils:54] Processing /workspace/mount/data/restaurant_finetune/labels_train.txt\n",
      "[NeMo I 2022-06-22 12:28:55 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:28:55 token_classification_utils:90] Labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16} saved to : /workspace/mount/data/restaurant/label_ids.csv\n",
      "[NeMo I 2022-06-22 12:28:55 token_classification_utils:99] Three most popular labels in /workspace/mount/data/restaurant_finetune/labels_train.txt:\n",
      "[NeMo I 2022-06-22 12:28:55 data_preprocessing:194] label: 0, 43670 out of 70525 (61.92%).\n",
      "[NeMo I 2022-06-22 12:28:55 data_preprocessing:194] label: 3, 4314 out of 70525 (6.12%).\n",
      "[NeMo I 2022-06-22 12:28:55 data_preprocessing:194] label: 5, 3817 out of 70525 (5.41%).\n",
      "[NeMo I 2022-06-22 12:28:55 token_classification_utils:101] Total labels: 70525. Label frequencies - {0: 43670, 3: 4314, 5: 3817, 13: 3658, 9: 2676, 1: 2541, 8: 1901, 16: 1668, 11: 1397, 12: 1283, 7: 1070, 4: 990, 6: 730, 15: 527, 14: 283}\n",
      "[NeMo I 2022-06-22 12:28:55 token_classification_utils:110] Class Weights: {0: 0.10766353713456989, 3: 1.0898624632977902, 5: 1.2317701510785084, 13: 1.2853107344632768, 9: 1.7569755854509217, 1: 1.8503213957759412, 8: 2.47325968788357, 16: 2.8187450039968027, 11: 3.3655452159389165, 12: 3.664588204728501, 7: 4.394080996884735, 4: 4.749158249158249, 6: 6.440639269406392, 15: 8.92156862745098, 14: 16.613663133097763}\n",
      "[NeMo I 2022-06-22 12:28:55 token_classification_utils:114] Class weights saved to /workspace/mount/data/restaurant_finetune/labels_train_weights.p\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_dataset:121] Setting Max Seq length to: 39\n",
      "[NeMo I 2022-06-22 12:29:00 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-06-22 12:29:00 data_preprocessing:360] Min: 3 |                  Max: 39 |                  Mean: 11.860182767624021 |                  Median: 11.0\n",
      "[NeMo I 2022-06-22 12:29:00 data_preprocessing:366] 75 percentile: 14.00\n",
      "[NeMo I 2022-06-22 12:29:00 data_preprocessing:367] 99 percentile: 24.00\n",
      "[NeMo W 2022-06-22 12:29:00 token_classification_dataset:150] 0 are longer than 39\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_dataset:153] *** Example ***\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_dataset:154] i: 0\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_dataset:155] subtokens: [CLS] 2 start restaurants with inside dining [SEP]\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_dataset:156] loss_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_dataset:157] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_dataset:158] subtokens_mask: 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_dataset:160] labels: 0 7 15 0 0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_dataset:269] features saved to /workspace/mount/data/restaurant_finetune/cached_text_train.txt_BertTokenizer_128_30522_-1\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_utils:54] Processing /workspace/mount/data/restaurant_finetune/labels_dev.txt\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_utils:99] Three most popular labels in /workspace/mount/data/restaurant_finetune/labels_dev.txt:\n",
      "[NeMo I 2022-06-22 12:29:00 data_preprocessing:194] label: 0, 8659 out of 14256 (60.74%).\n",
      "[NeMo I 2022-06-22 12:29:00 data_preprocessing:194] label: 3, 820 out of 14256 (5.75%).\n",
      "[NeMo I 2022-06-22 12:29:00 data_preprocessing:194] label: 5, 812 out of 14256 (5.70%).\n",
      "[NeMo I 2022-06-22 12:29:00 token_classification_utils:101] Total labels: 14256. Label frequencies - {0: 8659, 3: 820, 5: 812, 13: 788, 1: 533, 9: 524, 8: 402, 16: 392, 12: 295, 11: 256, 4: 212, 7: 201, 6: 171, 15: 125, 14: 66}\n",
      "[NeMo I 2022-06-22 12:29:01 token_classification_dataset:121] Setting Max Seq length to: 28\n",
      "[NeMo I 2022-06-22 12:29:01 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-06-22 12:29:01 data_preprocessing:360] Min: 3 |                  Max: 28 |                  Mean: 12.021696252465484 |                  Median: 12.0\n",
      "[NeMo I 2022-06-22 12:29:01 data_preprocessing:366] 75 percentile: 14.00\n",
      "[NeMo I 2022-06-22 12:29:01 data_preprocessing:367] 99 percentile: 23.00\n",
      "[NeMo W 2022-06-22 12:29:01 token_classification_dataset:150] 0 are longer than 28\n",
      "[NeMo I 2022-06-22 12:29:01 token_classification_dataset:153] *** Example ***\n",
      "[NeMo I 2022-06-22 12:29:01 token_classification_dataset:154] i: 0\n",
      "[NeMo I 2022-06-22 12:29:01 token_classification_dataset:155] subtokens: [CLS] a four star restaurant with a bar [SEP]\n",
      "[NeMo I 2022-06-22 12:29:01 token_classification_dataset:156] loss_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-06-22 12:29:01 token_classification_dataset:157] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-06-22 12:29:01 token_classification_dataset:158] subtokens_mask: 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-06-22 12:29:01 token_classification_dataset:160] labels: 0 0 7 15 0 5 13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-06-22 12:29:01 token_classification_dataset:269] features saved to /workspace/mount/data/restaurant_finetune/cached_text_dev.txt_BertTokenizer_128_30522_-1\n",
      "[NeMo W 2022-06-22 12:29:01 modelPT:436] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n",
      "[NeMo I 2022-06-22 12:29:01 modelPT:544] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 2e-05\n",
      "        weight_decay: 0\n",
      "    )\n",
      "[NeMo I 2022-06-22 12:29:01 lr_scheduler:496] Scheduler not initialized as no `sched` config supplied to setup_optimizer()\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2022-06-22 12:29:02 modelPT:197] You tried to register an artifact under config key=language_model.config_file but an artifact for it has already been registered.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2022-06-22 12:29:02 modelPT:415] No optimizer config provided, therefore no optimizer was created\n",
      "\n",
      "    | Name                                                   | Type                 | Params\n",
      "--------------------------------------------------------------------------------------------------\n",
      "0   | bert_model                                             | BertEncoder          | 109 M \n",
      "1   | bert_model.embeddings                                  | BertEmbeddings       | 23.8 M\n",
      "2   | bert_model.embeddings.word_embeddings                  | Embedding            | 23.4 M\n",
      "3   | bert_model.embeddings.position_embeddings              | Embedding            | 393 K \n",
      "4   | bert_model.embeddings.token_type_embeddings            | Embedding            | 1.5 K \n",
      "5   | bert_model.embeddings.LayerNorm                        | LayerNorm            | 1.5 K \n",
      "6   | bert_model.embeddings.dropout                          | Dropout              | 0     \n",
      "7   | bert_model.encoder                                     | BertEncoder          | 85.1 M\n",
      "8   | bert_model.encoder.layer                               | ModuleList           | 85.1 M\n",
      "9   | bert_model.encoder.layer.0                             | BertLayer            | 7.1 M \n",
      "10  | bert_model.encoder.layer.0.attention                   | BertAttention        | 2.4 M \n",
      "11  | bert_model.encoder.layer.0.attention.self              | BertSelfAttention    | 1.8 M \n",
      "12  | bert_model.encoder.layer.0.attention.self.query        | Linear               | 590 K \n",
      "13  | bert_model.encoder.layer.0.attention.self.key          | Linear               | 590 K \n",
      "14  | bert_model.encoder.layer.0.attention.self.value        | Linear               | 590 K \n",
      "15  | bert_model.encoder.layer.0.attention.self.dropout      | Dropout              | 0     \n",
      "16  | bert_model.encoder.layer.0.attention.output            | BertSelfOutput       | 592 K \n",
      "17  | bert_model.encoder.layer.0.attention.output.dense      | Linear               | 590 K \n",
      "18  | bert_model.encoder.layer.0.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "19  | bert_model.encoder.layer.0.attention.output.dropout    | Dropout              | 0     \n",
      "20  | bert_model.encoder.layer.0.intermediate                | BertIntermediate     | 2.4 M \n",
      "21  | bert_model.encoder.layer.0.intermediate.dense          | Linear               | 2.4 M \n",
      "22  | bert_model.encoder.layer.0.output                      | BertOutput           | 2.4 M \n",
      "23  | bert_model.encoder.layer.0.output.dense                | Linear               | 2.4 M \n",
      "24  | bert_model.encoder.layer.0.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "25  | bert_model.encoder.layer.0.output.dropout              | Dropout              | 0     \n",
      "26  | bert_model.encoder.layer.1                             | BertLayer            | 7.1 M \n",
      "27  | bert_model.encoder.layer.1.attention                   | BertAttention        | 2.4 M \n",
      "28  | bert_model.encoder.layer.1.attention.self              | BertSelfAttention    | 1.8 M \n",
      "29  | bert_model.encoder.layer.1.attention.self.query        | Linear               | 590 K \n",
      "30  | bert_model.encoder.layer.1.attention.self.key          | Linear               | 590 K \n",
      "31  | bert_model.encoder.layer.1.attention.self.value        | Linear               | 590 K \n",
      "32  | bert_model.encoder.layer.1.attention.self.dropout      | Dropout              | 0     \n",
      "33  | bert_model.encoder.layer.1.attention.output            | BertSelfOutput       | 592 K \n",
      "34  | bert_model.encoder.layer.1.attention.output.dense      | Linear               | 590 K \n",
      "35  | bert_model.encoder.layer.1.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "36  | bert_model.encoder.layer.1.attention.output.dropout    | Dropout              | 0     \n",
      "37  | bert_model.encoder.layer.1.intermediate                | BertIntermediate     | 2.4 M \n",
      "38  | bert_model.encoder.layer.1.intermediate.dense          | Linear               | 2.4 M \n",
      "39  | bert_model.encoder.layer.1.output                      | BertOutput           | 2.4 M \n",
      "40  | bert_model.encoder.layer.1.output.dense                | Linear               | 2.4 M \n",
      "41  | bert_model.encoder.layer.1.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "42  | bert_model.encoder.layer.1.output.dropout              | Dropout              | 0     \n",
      "43  | bert_model.encoder.layer.2                             | BertLayer            | 7.1 M \n",
      "44  | bert_model.encoder.layer.2.attention                   | BertAttention        | 2.4 M \n",
      "45  | bert_model.encoder.layer.2.attention.self              | BertSelfAttention    | 1.8 M \n",
      "46  | bert_model.encoder.layer.2.attention.self.query        | Linear               | 590 K \n",
      "47  | bert_model.encoder.layer.2.attention.self.key          | Linear               | 590 K \n",
      "48  | bert_model.encoder.layer.2.attention.self.value        | Linear               | 590 K \n",
      "49  | bert_model.encoder.layer.2.attention.self.dropout      | Dropout              | 0     \n",
      "50  | bert_model.encoder.layer.2.attention.output            | BertSelfOutput       | 592 K \n",
      "51  | bert_model.encoder.layer.2.attention.output.dense      | Linear               | 590 K \n",
      "52  | bert_model.encoder.layer.2.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "53  | bert_model.encoder.layer.2.attention.output.dropout    | Dropout              | 0     \n",
      "54  | bert_model.encoder.layer.2.intermediate                | BertIntermediate     | 2.4 M \n",
      "55  | bert_model.encoder.layer.2.intermediate.dense          | Linear               | 2.4 M \n",
      "56  | bert_model.encoder.layer.2.output                      | BertOutput           | 2.4 M \n",
      "57  | bert_model.encoder.layer.2.output.dense                | Linear               | 2.4 M \n",
      "58  | bert_model.encoder.layer.2.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "59  | bert_model.encoder.layer.2.output.dropout              | Dropout              | 0     \n",
      "60  | bert_model.encoder.layer.3                             | BertLayer            | 7.1 M \n",
      "61  | bert_model.encoder.layer.3.attention                   | BertAttention        | 2.4 M \n",
      "62  | bert_model.encoder.layer.3.attention.self              | BertSelfAttention    | 1.8 M \n",
      "63  | bert_model.encoder.layer.3.attention.self.query        | Linear               | 590 K \n",
      "64  | bert_model.encoder.layer.3.attention.self.key          | Linear               | 590 K \n",
      "65  | bert_model.encoder.layer.3.attention.self.value        | Linear               | 590 K \n",
      "66  | bert_model.encoder.layer.3.attention.self.dropout      | Dropout              | 0     \n",
      "67  | bert_model.encoder.layer.3.attention.output            | BertSelfOutput       | 592 K \n",
      "68  | bert_model.encoder.layer.3.attention.output.dense      | Linear               | 590 K \n",
      "69  | bert_model.encoder.layer.3.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "70  | bert_model.encoder.layer.3.attention.output.dropout    | Dropout              | 0     \n",
      "71  | bert_model.encoder.layer.3.intermediate                | BertIntermediate     | 2.4 M \n",
      "72  | bert_model.encoder.layer.3.intermediate.dense          | Linear               | 2.4 M \n",
      "73  | bert_model.encoder.layer.3.output                      | BertOutput           | 2.4 M \n",
      "74  | bert_model.encoder.layer.3.output.dense                | Linear               | 2.4 M \n",
      "75  | bert_model.encoder.layer.3.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "76  | bert_model.encoder.layer.3.output.dropout              | Dropout              | 0     \n",
      "77  | bert_model.encoder.layer.4                             | BertLayer            | 7.1 M \n",
      "78  | bert_model.encoder.layer.4.attention                   | BertAttention        | 2.4 M \n",
      "79  | bert_model.encoder.layer.4.attention.self              | BertSelfAttention    | 1.8 M \n",
      "80  | bert_model.encoder.layer.4.attention.self.query        | Linear               | 590 K \n",
      "81  | bert_model.encoder.layer.4.attention.self.key          | Linear               | 590 K \n",
      "82  | bert_model.encoder.layer.4.attention.self.value        | Linear               | 590 K \n",
      "83  | bert_model.encoder.layer.4.attention.self.dropout      | Dropout              | 0     \n",
      "84  | bert_model.encoder.layer.4.attention.output            | BertSelfOutput       | 592 K \n",
      "85  | bert_model.encoder.layer.4.attention.output.dense      | Linear               | 590 K \n",
      "86  | bert_model.encoder.layer.4.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "87  | bert_model.encoder.layer.4.attention.output.dropout    | Dropout              | 0     \n",
      "88  | bert_model.encoder.layer.4.intermediate                | BertIntermediate     | 2.4 M \n",
      "89  | bert_model.encoder.layer.4.intermediate.dense          | Linear               | 2.4 M \n",
      "90  | bert_model.encoder.layer.4.output                      | BertOutput           | 2.4 M \n",
      "91  | bert_model.encoder.layer.4.output.dense                | Linear               | 2.4 M \n",
      "92  | bert_model.encoder.layer.4.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "93  | bert_model.encoder.layer.4.output.dropout              | Dropout              | 0     \n",
      "94  | bert_model.encoder.layer.5                             | BertLayer            | 7.1 M \n",
      "95  | bert_model.encoder.layer.5.attention                   | BertAttention        | 2.4 M \n",
      "96  | bert_model.encoder.layer.5.attention.self              | BertSelfAttention    | 1.8 M \n",
      "97  | bert_model.encoder.layer.5.attention.self.query        | Linear               | 590 K \n",
      "98  | bert_model.encoder.layer.5.attention.self.key          | Linear               | 590 K \n",
      "99  | bert_model.encoder.layer.5.attention.self.value        | Linear               | 590 K \n",
      "100 | bert_model.encoder.layer.5.attention.self.dropout      | Dropout              | 0     \n",
      "101 | bert_model.encoder.layer.5.attention.output            | BertSelfOutput       | 592 K \n",
      "102 | bert_model.encoder.layer.5.attention.output.dense      | Linear               | 590 K \n",
      "103 | bert_model.encoder.layer.5.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "104 | bert_model.encoder.layer.5.attention.output.dropout    | Dropout              | 0     \n",
      "105 | bert_model.encoder.layer.5.intermediate                | BertIntermediate     | 2.4 M \n",
      "106 | bert_model.encoder.layer.5.intermediate.dense          | Linear               | 2.4 M \n",
      "107 | bert_model.encoder.layer.5.output                      | BertOutput           | 2.4 M \n",
      "108 | bert_model.encoder.layer.5.output.dense                | Linear               | 2.4 M \n",
      "109 | bert_model.encoder.layer.5.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "110 | bert_model.encoder.layer.5.output.dropout              | Dropout              | 0     \n",
      "111 | bert_model.encoder.layer.6                             | BertLayer            | 7.1 M \n",
      "112 | bert_model.encoder.layer.6.attention                   | BertAttention        | 2.4 M \n",
      "113 | bert_model.encoder.layer.6.attention.self              | BertSelfAttention    | 1.8 M \n",
      "114 | bert_model.encoder.layer.6.attention.self.query        | Linear               | 590 K \n",
      "115 | bert_model.encoder.layer.6.attention.self.key          | Linear               | 590 K \n",
      "116 | bert_model.encoder.layer.6.attention.self.value        | Linear               | 590 K \n",
      "117 | bert_model.encoder.layer.6.attention.self.dropout      | Dropout              | 0     \n",
      "118 | bert_model.encoder.layer.6.attention.output            | BertSelfOutput       | 592 K \n",
      "119 | bert_model.encoder.layer.6.attention.output.dense      | Linear               | 590 K \n",
      "120 | bert_model.encoder.layer.6.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "121 | bert_model.encoder.layer.6.attention.output.dropout    | Dropout              | 0     \n",
      "122 | bert_model.encoder.layer.6.intermediate                | BertIntermediate     | 2.4 M \n",
      "123 | bert_model.encoder.layer.6.intermediate.dense          | Linear               | 2.4 M \n",
      "124 | bert_model.encoder.layer.6.output                      | BertOutput           | 2.4 M \n",
      "125 | bert_model.encoder.layer.6.output.dense                | Linear               | 2.4 M \n",
      "126 | bert_model.encoder.layer.6.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "127 | bert_model.encoder.layer.6.output.dropout              | Dropout              | 0     \n",
      "128 | bert_model.encoder.layer.7                             | BertLayer            | 7.1 M \n",
      "129 | bert_model.encoder.layer.7.attention                   | BertAttention        | 2.4 M \n",
      "130 | bert_model.encoder.layer.7.attention.self              | BertSelfAttention    | 1.8 M \n",
      "131 | bert_model.encoder.layer.7.attention.self.query        | Linear               | 590 K \n",
      "132 | bert_model.encoder.layer.7.attention.self.key          | Linear               | 590 K \n",
      "133 | bert_model.encoder.layer.7.attention.self.value        | Linear               | 590 K \n",
      "134 | bert_model.encoder.layer.7.attention.self.dropout      | Dropout              | 0     \n",
      "135 | bert_model.encoder.layer.7.attention.output            | BertSelfOutput       | 592 K \n",
      "136 | bert_model.encoder.layer.7.attention.output.dense      | Linear               | 590 K \n",
      "137 | bert_model.encoder.layer.7.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "138 | bert_model.encoder.layer.7.attention.output.dropout    | Dropout              | 0     \n",
      "139 | bert_model.encoder.layer.7.intermediate                | BertIntermediate     | 2.4 M \n",
      "140 | bert_model.encoder.layer.7.intermediate.dense          | Linear               | 2.4 M \n",
      "141 | bert_model.encoder.layer.7.output                      | BertOutput           | 2.4 M \n",
      "142 | bert_model.encoder.layer.7.output.dense                | Linear               | 2.4 M \n",
      "143 | bert_model.encoder.layer.7.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "144 | bert_model.encoder.layer.7.output.dropout              | Dropout              | 0     \n",
      "145 | bert_model.encoder.layer.8                             | BertLayer            | 7.1 M \n",
      "146 | bert_model.encoder.layer.8.attention                   | BertAttention        | 2.4 M \n",
      "147 | bert_model.encoder.layer.8.attention.self              | BertSelfAttention    | 1.8 M \n",
      "148 | bert_model.encoder.layer.8.attention.self.query        | Linear               | 590 K \n",
      "149 | bert_model.encoder.layer.8.attention.self.key          | Linear               | 590 K \n",
      "150 | bert_model.encoder.layer.8.attention.self.value        | Linear               | 590 K \n",
      "151 | bert_model.encoder.layer.8.attention.self.dropout      | Dropout              | 0     \n",
      "152 | bert_model.encoder.layer.8.attention.output            | BertSelfOutput       | 592 K \n",
      "153 | bert_model.encoder.layer.8.attention.output.dense      | Linear               | 590 K \n",
      "154 | bert_model.encoder.layer.8.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "155 | bert_model.encoder.layer.8.attention.output.dropout    | Dropout              | 0     \n",
      "156 | bert_model.encoder.layer.8.intermediate                | BertIntermediate     | 2.4 M \n",
      "157 | bert_model.encoder.layer.8.intermediate.dense          | Linear               | 2.4 M \n",
      "158 | bert_model.encoder.layer.8.output                      | BertOutput           | 2.4 M \n",
      "159 | bert_model.encoder.layer.8.output.dense                | Linear               | 2.4 M \n",
      "160 | bert_model.encoder.layer.8.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "161 | bert_model.encoder.layer.8.output.dropout              | Dropout              | 0     \n",
      "162 | bert_model.encoder.layer.9                             | BertLayer            | 7.1 M \n",
      "163 | bert_model.encoder.layer.9.attention                   | BertAttention        | 2.4 M \n",
      "164 | bert_model.encoder.layer.9.attention.self              | BertSelfAttention    | 1.8 M \n",
      "165 | bert_model.encoder.layer.9.attention.self.query        | Linear               | 590 K \n",
      "166 | bert_model.encoder.layer.9.attention.self.key          | Linear               | 590 K \n",
      "167 | bert_model.encoder.layer.9.attention.self.value        | Linear               | 590 K \n",
      "168 | bert_model.encoder.layer.9.attention.self.dropout      | Dropout              | 0     \n",
      "169 | bert_model.encoder.layer.9.attention.output            | BertSelfOutput       | 592 K \n",
      "170 | bert_model.encoder.layer.9.attention.output.dense      | Linear               | 590 K \n",
      "171 | bert_model.encoder.layer.9.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "172 | bert_model.encoder.layer.9.attention.output.dropout    | Dropout              | 0     \n",
      "173 | bert_model.encoder.layer.9.intermediate                | BertIntermediate     | 2.4 M \n",
      "174 | bert_model.encoder.layer.9.intermediate.dense          | Linear               | 2.4 M \n",
      "175 | bert_model.encoder.layer.9.output                      | BertOutput           | 2.4 M \n",
      "176 | bert_model.encoder.layer.9.output.dense                | Linear               | 2.4 M \n",
      "177 | bert_model.encoder.layer.9.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "178 | bert_model.encoder.layer.9.output.dropout              | Dropout              | 0     \n",
      "179 | bert_model.encoder.layer.10                            | BertLayer            | 7.1 M \n",
      "180 | bert_model.encoder.layer.10.attention                  | BertAttention        | 2.4 M \n",
      "181 | bert_model.encoder.layer.10.attention.self             | BertSelfAttention    | 1.8 M \n",
      "182 | bert_model.encoder.layer.10.attention.self.query       | Linear               | 590 K \n",
      "183 | bert_model.encoder.layer.10.attention.self.key         | Linear               | 590 K \n",
      "184 | bert_model.encoder.layer.10.attention.self.value       | Linear               | 590 K \n",
      "185 | bert_model.encoder.layer.10.attention.self.dropout     | Dropout              | 0     \n",
      "186 | bert_model.encoder.layer.10.attention.output           | BertSelfOutput       | 592 K \n",
      "187 | bert_model.encoder.layer.10.attention.output.dense     | Linear               | 590 K \n",
      "188 | bert_model.encoder.layer.10.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "189 | bert_model.encoder.layer.10.attention.output.dropout   | Dropout              | 0     \n",
      "190 | bert_model.encoder.layer.10.intermediate               | BertIntermediate     | 2.4 M \n",
      "191 | bert_model.encoder.layer.10.intermediate.dense         | Linear               | 2.4 M \n",
      "192 | bert_model.encoder.layer.10.output                     | BertOutput           | 2.4 M \n",
      "193 | bert_model.encoder.layer.10.output.dense               | Linear               | 2.4 M \n",
      "194 | bert_model.encoder.layer.10.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "195 | bert_model.encoder.layer.10.output.dropout             | Dropout              | 0     \n",
      "196 | bert_model.encoder.layer.11                            | BertLayer            | 7.1 M \n",
      "197 | bert_model.encoder.layer.11.attention                  | BertAttention        | 2.4 M \n",
      "198 | bert_model.encoder.layer.11.attention.self             | BertSelfAttention    | 1.8 M \n",
      "199 | bert_model.encoder.layer.11.attention.self.query       | Linear               | 590 K \n",
      "200 | bert_model.encoder.layer.11.attention.self.key         | Linear               | 590 K \n",
      "201 | bert_model.encoder.layer.11.attention.self.value       | Linear               | 590 K \n",
      "202 | bert_model.encoder.layer.11.attention.self.dropout     | Dropout              | 0     \n",
      "203 | bert_model.encoder.layer.11.attention.output           | BertSelfOutput       | 592 K \n",
      "204 | bert_model.encoder.layer.11.attention.output.dense     | Linear               | 590 K \n",
      "205 | bert_model.encoder.layer.11.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "206 | bert_model.encoder.layer.11.attention.output.dropout   | Dropout              | 0     \n",
      "207 | bert_model.encoder.layer.11.intermediate               | BertIntermediate     | 2.4 M \n",
      "208 | bert_model.encoder.layer.11.intermediate.dense         | Linear               | 2.4 M \n",
      "209 | bert_model.encoder.layer.11.output                     | BertOutput           | 2.4 M \n",
      "210 | bert_model.encoder.layer.11.output.dense               | Linear               | 2.4 M \n",
      "211 | bert_model.encoder.layer.11.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "212 | bert_model.encoder.layer.11.output.dropout             | Dropout              | 0     \n",
      "213 | bert_model.pooler                                      | BertPooler           | 590 K \n",
      "214 | bert_model.pooler.dense                                | Linear               | 590 K \n",
      "215 | bert_model.pooler.activation                           | Tanh                 | 0     \n",
      "216 | classifier                                             | TokenClassifier      | 603 K \n",
      "217 | classifier.dropout                                     | Dropout              | 0     \n",
      "218 | classifier.mlp                                         | MultiLayerPerceptron | 603 K \n",
      "219 | classifier.mlp.layer0                                  | Linear               | 590 K \n",
      "220 | classifier.mlp.layer2                                  | Linear               | 13.1 K\n",
      "221 | loss                                                   | CrossEntropyLoss     | 0     \n",
      "222 | classification_report                                  | ClassificationReport | 0     \n",
      "--------------------------------------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.344   Total estimated model params size (MB)\n",
      "Validation sanity check:  50%|██████████          | 1/2 [00:00<00:00,  1.08it/s][NeMo I 2022-06-22 12:29:03 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         96.13      90.58      93.27        329\n",
      "    B-Amenity (label_id: 1)                                 60.00      57.69      58.82         26\n",
      "    B-Cuisine (label_id: 2)                                  0.00       0.00       0.00          0\n",
      "    B-Dish (label_id: 3)                                    83.33      13.89      23.81         36\n",
      "    B-Hours (label_id: 4)                                   60.00      75.00      66.67         12\n",
      "    B-Location (label_id: 5)                                89.58      86.00      87.76         50\n",
      "    B-Price (label_id: 6)                                   80.00     100.00      88.89          8\n",
      "    B-Rating (label_id: 7)                                  69.23     100.00      81.82          9\n",
      "    B-Restaurant_Name (label_id: 8)                         87.50     100.00      93.33          7\n",
      "    I-Amenity (label_id: 9)                                 72.00      75.00      73.47         24\n",
      "    I-Cuisine (label_id: 10)                                 0.00       0.00       0.00          0\n",
      "    I-Dish (label_id: 11)                                    0.00       0.00       0.00          7\n",
      "    I-Hours (label_id: 12)                                  74.07     100.00      85.11         20\n",
      "    I-Location (label_id: 13)                               87.50      89.36      88.42         47\n",
      "    I-Price (label_id: 14)                                 100.00      50.00      66.67          4\n",
      "    I-Rating (label_id: 15)                                100.00     100.00     100.00          4\n",
      "    I-Restaurant_Name (label_id: 16)                       100.00     100.00     100.00         10\n",
      "    -------------------\n",
      "    micro avg                                               82.63      82.63      82.63        593\n",
      "    macro avg                                               77.29      75.83      73.87        593\n",
      "    weighted avg                                            88.34      82.63      83.73        593\n",
      "    \n",
      "Epoch 0:  83%|█████████▏ | 240/288 [00:30<00:05,  8.01it/s, loss=0.206, lr=2e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  84%|█████████▏ | 242/288 [00:30<00:05,  8.04it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  85%|█████████▍ | 246/288 [00:30<00:05,  8.13it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  87%|█████████▌ | 250/288 [00:30<00:04,  8.23it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  88%|█████████▋ | 254/288 [00:30<00:04,  8.32it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  90%|█████████▊ | 258/288 [00:30<00:03,  8.42it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Validating:  38%|███████████▋                   | 18/48 [00:00<00:01, 18.27it/s]\u001b[A\n",
      "Epoch 0:  91%|██████████ | 262/288 [00:30<00:03,  8.51it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  92%|██████████▏| 266/288 [00:31<00:02,  8.60it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  94%|██████████▎| 270/288 [00:31<00:02,  8.69it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  95%|██████████▍| 274/288 [00:31<00:01,  8.79it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  97%|██████████▌| 278/288 [00:31<00:01,  8.88it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  98%|██████████▊| 282/288 [00:31<00:00,  8.97it/s, loss=0.206, lr=2e-5]\u001b[A\n",
      "Epoch 0:  99%|██████████▉| 286/288 [00:31<00:00,  9.05it/s, loss=0.206, lr=2e-5]\u001b[A[NeMo I 2022-06-22 12:29:35 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         96.87      95.09      95.97       8659\n",
      "    B-Amenity (label_id: 1)                                 74.86      74.30      74.58        533\n",
      "    B-Cuisine (label_id: 2)                                  0.00       0.00       0.00          0\n",
      "    B-Dish (label_id: 3)                                    85.98      95.73      90.59        820\n",
      "    B-Hours (label_id: 4)                                   70.13      76.42      73.14        212\n",
      "    B-Location (label_id: 5)                                90.01      89.90      89.96        812\n",
      "    B-Price (label_id: 6)                                   82.87      87.72      85.23        171\n",
      "    B-Rating (label_id: 7)                                  75.21      90.55      82.17        201\n",
      "    B-Restaurant_Name (label_id: 8)                         95.91      87.56      91.55        402\n",
      "    I-Amenity (label_id: 9)                                 74.60      80.73      77.54        524\n",
      "    I-Cuisine (label_id: 10)                                 0.00       0.00       0.00          0\n",
      "    I-Dish (label_id: 11)                                   70.42      78.12      74.07        256\n",
      "    I-Hours (label_id: 12)                                  85.03      90.51      87.68        295\n",
      "    I-Location (label_id: 13)                               88.69      89.59      89.14        788\n",
      "    I-Price (label_id: 14)                                  83.33      60.61      70.18         66\n",
      "    I-Rating (label_id: 15)                                 76.30      82.40      79.23        125\n",
      "    I-Restaurant_Name (label_id: 16)                        93.49      80.61      86.58        392\n",
      "    -------------------\n",
      "    micro avg                                               91.51      91.51      91.51      14256\n",
      "    macro avg                                               82.91      83.99      83.17      14256\n",
      "    weighted avg                                            91.81      91.51      91.59      14256\n",
      "    \n",
      "Epoch 0: 100%|█| 288/288 [00:31<00:00,  9.07it/s, loss=0.206, lr=2e-5, val_loss=\n",
      "                                                                                \u001b[AEpoch 0, global step 239: val_loss reached 0.26511 (best 0.26511), saving model to \"/workspace/mount/results/bert-base-finetuned_ner/checkpoints/finetuned-model--val_loss=0.27-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  83%|▊| 240/288 [00:30<00:05,  8.01it/s, loss=0.186, lr=2e-5, val_loss=\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  84%|▊| 242/288 [00:30<00:05,  8.03it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  85%|▊| 245/288 [00:30<00:05,  8.10it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  86%|▊| 249/288 [00:30<00:04,  8.20it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  88%|▉| 253/288 [00:30<00:04,  8.29it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  89%|▉| 257/288 [00:30<00:03,  8.39it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  91%|▉| 261/288 [00:30<00:03,  8.48it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  92%|▉| 265/288 [00:31<00:02,  8.58it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  93%|▉| 269/288 [00:31<00:02,  8.67it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  95%|▉| 273/288 [00:31<00:01,  8.76it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Validating:  69%|█████████████████████▎         | 33/48 [00:01<00:00, 26.08it/s]\u001b[A\n",
      "Epoch 1:  96%|▉| 277/288 [00:31<00:01,  8.85it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  98%|▉| 281/288 [00:31<00:00,  8.94it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Epoch 1:  99%|▉| 285/288 [00:31<00:00,  9.03it/s, loss=0.186, lr=2e-5, val_loss=\u001b[A\n",
      "Validating:  98%|██████████████████████████████▎| 47/48 [00:01<00:00, 28.98it/s]\u001b[A[NeMo I 2022-06-22 12:30:30 token_classification_model:176] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         97.28      94.84      96.04       8659\n",
      "    B-Amenity (label_id: 1)                                 72.02      81.61      76.52        533\n",
      "    B-Cuisine (label_id: 2)                                  0.00       0.00       0.00          0\n",
      "    B-Dish (label_id: 3)                                    90.93      92.93      91.92        820\n",
      "    B-Hours (label_id: 4)                                   67.45      81.13      73.66        212\n",
      "    B-Location (label_id: 5)                                86.75      91.13      88.89        812\n",
      "    B-Price (label_id: 6)                                   79.80      92.40      85.64        171\n",
      "    B-Rating (label_id: 7)                                  75.42      88.56      81.46        201\n",
      "    B-Restaurant_Name (label_id: 8)                         94.81      90.80      92.76        402\n",
      "    I-Amenity (label_id: 9)                                 78.12      77.67      77.89        524\n",
      "    I-Cuisine (label_id: 10)                                 0.00       0.00       0.00          0\n",
      "    I-Dish (label_id: 11)                                   74.65      83.98      79.04        256\n",
      "    I-Hours (label_id: 12)                                  82.44      93.90      87.80        295\n",
      "    I-Location (label_id: 13)                               90.93      85.28      88.02        788\n",
      "    I-Price (label_id: 14)                                  71.21      71.21      71.21         66\n",
      "    I-Rating (label_id: 15)                                 85.45      75.20      80.00        125\n",
      "    I-Restaurant_Name (label_id: 16)                        89.61      88.01      88.80        392\n",
      "    -------------------\n",
      "    micro avg                                               91.74      91.74      91.74      14256\n",
      "    macro avg                                               82.46      85.91      83.98      14256\n",
      "    weighted avg                                            92.13      91.74      91.87      14256\n",
      "    \n",
      "Epoch 1: 100%|█| 288/288 [00:31<00:00,  9.06it/s, loss=0.186, lr=2e-5, val_loss=\n",
      "                                                                                \u001b[AEpoch 1, global step 479: val_loss reached 0.25678 (best 0.25678), saving model to \"/workspace/mount/results/bert-base-finetuned_ner/checkpoints/finetuned-model--val_loss=0.26-epoch=1.ckpt\" as top 3\n",
      "Epoch 1: 100%|█| 288/288 [00:55<00:00,  5.22it/s, loss=0.186, lr=2e-5, val_loss=\n",
      "[NeMo I 2022-06-22 12:31:14 finetune:127] Experiment logs saved to '/workspace/mount/results/bert-base-finetuned_ner'\n",
      "[NeMo I 2022-06-22 12:31:14 finetune:128] Fine-tuned model saved to '/workspace/mount/results/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:31:16,392 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# TAO NER model finetuning\n",
    "!tao token_classification finetune \\\n",
    "   -e $SPECS_DIR/token_classification/finetune.yaml \\\n",
    "   -r $RESULTS_DIR/bert-base-finetuned_ner/ \\\n",
    "   -m $RESULTS_DIR/bert-base_ner/checkpoints/trained-model.tlt \\\n",
    "   -g 1 \\\n",
    "   data_dir={destination_mount}/data/restaurant_finetune \\\n",
    "   trainer.max_epochs=2 \\\n",
    "   trainer.amp_level=\"O1\" \\\n",
    "   trainer.precision=16 \\\n",
    "   -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5.1 Exercise: Evaluate the Fine-Tuned Model\n",
    "\n",
    "Based on what you've learned, evaluate the performance of the fine-tuned NER model. If you get stuck, you can look at the [solution](solutions/ex7.5.1.ipynb).  If you are unsure of the location of the fine-tuned model, check the outputs from the fine-tuning or the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:43:18,419 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:43:18,550 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:43:18,583 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:43:23 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:23 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:23 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:23 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:43:24 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:43:24 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:43:24 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:43:24 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:24 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:29 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:29 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:29 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:29 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:43:29 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:43:30 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:43:30 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:43:30 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:30 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:43:31 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/evaluate.py:96: UserWarning: \n",
      "    'evaluate.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:43:31 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /workspace/mount/results/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt\n",
      "    data_dir: /workspace/mount/data/restaurant_finetune\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/mount/results/bert-base-finetuned_ner/evaluate\n",
      "      exp_dir: null\n",
      "      name: null\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: false\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: false\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: min\n",
      "        period: null\n",
      "        every_n_val_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .nemo\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "      files_to_copy: null\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 1000\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 32\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O2\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices: null\n",
      "    test_ds:\n",
      "      batch_size: 1\n",
      "      text_file: text_dev.txt\n",
      "      labels_file: labels_dev.txt\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "    encryption_key: '********'\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-06-22 12:43:31 exp_manager:414] Exp_manager is logging to /workspace/mount/results/bert-base-finetuned_ner/evaluate, but it already exists.\n",
      "[NeMo I 2022-06-22 12:43:31 exp_manager:220] Experiments will be logged at /workspace/mount/results/bert-base-finetuned_ner/evaluate\n",
      "[NeMo I 2022-06-22 12:43:33 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmpw8f1s6vq/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 140063845129952 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 27.2kB/s]\n",
      "Lock 140063845129952 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 140063845077488 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 806kB/s]\n",
      "Lock 140063845077488 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 140063844368736 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 72.2MB/s]\n",
      "Lock 140063844368736 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140063844248640 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 85.8MB/s]\n",
      "Lock 140063844248640 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:43:34 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2022-06-22 12:43:34 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 140063843865120 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:04<00:00, 99.7MB/s]\n",
      "Lock 140063843865120 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:43:47 token_classification_model:103] Setting model.dataset.data_dir to /workspace/mount/data/restaurant_finetune.\n",
      "[NeMo I 2022-06-22 12:43:47 token_classification_utils:54] Processing /workspace/mount/data/restaurant_finetune/labels_dev.txt\n",
      "[NeMo I 2022-06-22 12:43:47 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Amenity': 1, 'B-Cuisine': 2, 'B-Dish': 3, 'B-Hours': 4, 'B-Location': 5, 'B-Price': 6, 'B-Rating': 7, 'B-Restaurant_Name': 8, 'I-Amenity': 9, 'I-Cuisine': 10, 'I-Dish': 11, 'I-Hours': 12, 'I-Location': 13, 'I-Price': 14, 'I-Rating': 15, 'I-Restaurant_Name': 16}\n",
      "[NeMo I 2022-06-22 12:43:47 token_classification_utils:96] /workspace/mount/data/restaurant_finetune/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-06-22 12:43:47 token_classification_dataset:277] features restored from /workspace/mount/data/restaurant_finetune/cached_text_dev.txt_BertTokenizer_128_30522_-1\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Testing: 100%|█████████████████████████████▉| 1517/1521 [00:38<00:00, 39.08it/s][NeMo I 2022-06-22 12:44:26 token_classification_model:202] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         97.28      94.84      96.04       8659\n",
      "    B-Amenity (label_id: 1)                                 72.02      81.61      76.52        533\n",
      "    B-Cuisine (label_id: 2)                                  0.00       0.00       0.00          0\n",
      "    B-Dish (label_id: 3)                                    90.93      92.93      91.92        820\n",
      "    B-Hours (label_id: 4)                                   67.45      81.13      73.66        212\n",
      "    B-Location (label_id: 5)                                86.75      91.13      88.89        812\n",
      "    B-Price (label_id: 6)                                   79.80      92.40      85.64        171\n",
      "    B-Rating (label_id: 7)                                  75.42      88.56      81.46        201\n",
      "    B-Restaurant_Name (label_id: 8)                         94.81      90.80      92.76        402\n",
      "    I-Amenity (label_id: 9)                                 78.12      77.67      77.89        524\n",
      "    I-Cuisine (label_id: 10)                                 0.00       0.00       0.00          0\n",
      "    I-Dish (label_id: 11)                                   74.65      83.98      79.04        256\n",
      "    I-Hours (label_id: 12)                                  82.44      93.90      87.80        295\n",
      "    I-Location (label_id: 13)                               90.93      85.28      88.02        788\n",
      "    I-Price (label_id: 14)                                  71.21      71.21      71.21         66\n",
      "    I-Rating (label_id: 15)                                 85.45      75.20      80.00        125\n",
      "    I-Restaurant_Name (label_id: 16)                        89.61      88.01      88.80        392\n",
      "    -------------------\n",
      "    micro avg                                               91.74      91.74      91.74      14256\n",
      "    macro avg                                               82.46      85.91      83.98      14256\n",
      "    weighted avg                                            92.13      91.74      91.87      14256\n",
      "    \n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'f1': 83.97683715820312,\n",
      " 'precision': 82.45864868164062,\n",
      " 'recall': 85.90999603271484,\n",
      " 'test_loss': 0.23768267035484314}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████████████████████████| 1521/1521 [00:38<00:00, 39.64it/s]\n",
      "[NeMo I 2022-06-22 12:44:26 evaluate:92] Experiment logs saved to '/workspace/mount/results/bert-base-finetuned_ner/evaluate'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:44:28,520 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# TODO evaluate the fine-tuned model\n",
    "# TAO evaluate fine-tuned NER model\n",
    "!tao token_classification evaluate  \\\n",
    "   -e $SPECS_DIR/token_classification/evaluate.yaml \\\n",
    "   -r $RESULTS_DIR/bert-base-finetuned_ner/evaluate \\\n",
    "   -g 1 \\\n",
    "   -m $RESULTS_DIR/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt \\\n",
    "   -k $KEY \\\n",
    "   data_dir={destination_mount}/data/restaurant_finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the evaluation results, you can either continue fine-tuning the model for more epochs, or move on to inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5.2 Inference on the Fine-Tuned Model\n",
    "\n",
    "Try inference on the NER fine-tuned model using a few sentences within the restaurant context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.\n",
      "# TAO Spec file for inference using a previously pretrained BERT model for a text classification task.\n",
      "\n",
      "# \"Simulate\" user input: batch with four samples.\n",
      "input_batch:\n",
      "  - \"I would like to order a pizza for 6pm\"\n",
      "  - \"what sauce in your burger .\"\n",
      "  - \"mhh nice food.\"\n",
      "  - \"any good cheap german restaurants nearby\"\n",
      "  - \"any good ice cream parlors around\"\n",
      "  - \"any good place to get a pie at an affordable price\"\n"
     ]
    }
   ],
   "source": [
    "!cat /dli/task/tao/specs/token_classification/infer_restaurant.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference can be done using the command `tao token_classification infer` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:44:48,461 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:44:48,605 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:44:48,643 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:44:53 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:44:53 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:44:53 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:44:53 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:44:53 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:44:54 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:44:54 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:44:54 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:44:54 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:44:58 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:44:58 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:44:58 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:44:58 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:44:59 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:44:59 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:44:59 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:44:59 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:44:59 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:45:00 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/infer.py:84: UserWarning: \n",
      "    'infer_restaurant.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:45:00 tlt_logging:20] Experiment configuration:\n",
      "    exp_manager:\n",
      "      task_name: infer\n",
      "      explicit_log_dir: /workspace/mount/results/bert-base-finetuned_ner/\n",
      "    restore_from: /workspace/mount/results/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt\n",
      "    input_batch:\n",
      "    - I would like to order a pizza for 6pm\n",
      "    - what sauce in your burger .\n",
      "    - mhh nice food.\n",
      "    - any good cheap german restaurants nearby\n",
      "    - any good ice cream parlors around\n",
      "    - any good place to get a pie at an affordable price\n",
      "    encryption_key: '****'\n",
      "    \n",
      "[NeMo W 2022-06-22 12:45:00 exp_manager:26] Exp_manager is logging to `/workspace/mount/results/bert-base-finetuned_ner/``, but it already exists.\n",
      "[NeMo I 2022-06-22 12:45:03 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmpgq6l4c54/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 140295245858752 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 21.2kB/s]\n",
      "Lock 140295245858752 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 140295246132464 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 624kB/s]\n",
      "Lock 140295246132464 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 140295246134672 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 71.0MB/s]\n",
      "Lock 140295246134672 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140295246134288 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 81.7MB/s]\n",
      "Lock 140295246134288 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:45:03 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2022-06-22 12:45:03 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 140295254137536 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|████████████████████████████| 440M/440M [00:04<00:00, 105MB/s]\n",
      "Lock 140295254137536 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:45:17 token_classification_dataset:121] Setting Max Seq length to: 13\n",
      "[NeMo I 2022-06-22 12:45:17 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-06-22 12:45:17 data_preprocessing:360] Min: 7 |                  Max: 13 |                  Mean: 9.5 |                  Median: 8.5\n",
      "[NeMo I 2022-06-22 12:45:17 data_preprocessing:366] 75 percentile: 11.25\n",
      "[NeMo I 2022-06-22 12:45:17 data_preprocessing:367] 99 percentile: 12.95\n",
      "[NeMo W 2022-06-22 12:45:17 token_classification_dataset:150] 0 are longer than 13\n",
      "[NeMo I 2022-06-22 12:45:17 token_classification_dataset:153] *** Example ***\n",
      "[NeMo I 2022-06-22 12:45:17 token_classification_dataset:154] i: 0\n",
      "[NeMo I 2022-06-22 12:45:17 token_classification_dataset:155] subtokens: [CLS] i would like to order a pizza for 6 ##pm [SEP]\n",
      "[NeMo I 2022-06-22 12:45:17 token_classification_dataset:156] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "[NeMo I 2022-06-22 12:45:17 token_classification_dataset:157] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "[NeMo I 2022-06-22 12:45:17 token_classification_dataset:158] subtokens_mask: 0 1 1 1 1 1 1 1 1 1 0 0 0\n",
      "[NeMo I 2022-06-22 12:45:18 infer:76] Query  : I would like to order a pizza for 6pm\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: I would like to order a pizza[B-Dish] for 6pm[B-Hours]\n",
      "[NeMo I 2022-06-22 12:45:18 infer:76] Query  : what sauce in your burger .\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: what sauce[B-Dish] in your burger[B-Dish] .\n",
      "[NeMo I 2022-06-22 12:45:18 infer:76] Query  : mhh nice food.\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: mhh nice[B-Rating] food.\n",
      "[NeMo I 2022-06-22 12:45:18 infer:76] Query  : any good cheap german restaurants nearby\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: any good[B-Rating] cheap[B-Price] german[B-Dish] restaurants nearby[B-Location]\n",
      "[NeMo I 2022-06-22 12:45:18 infer:76] Query  : any good ice cream parlors around\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: any good[B-Rating] ice[B-Dish] cream[I-Dish] parlors[I-Dish] around[B-Location]\n",
      "[NeMo I 2022-06-22 12:45:18 infer:76] Query  : any good place to get a pie at an affordable price\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: any good[B-Rating] place to get a pie[B-Dish] at an affordable[B-Price] price\n",
      "[NeMo I 2022-06-22 12:45:18 infer:80] Experiment logs saved to '/workspace/mount/results/bert-base-finetuned_ner'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:45:19,841 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# TAO fine-tuned NER inference \n",
    "!tao token_classification infer \\\n",
    "    -e $SPECS_DIR/token_classification/infer_restaurant.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/bert-base-finetuned_ner/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: I would like to order a pizza[B-Dish] for 6pm[B-Hours]\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: what sauce[B-Dish] in your burger[B-Dish] .\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: mhh nice[B-Rating] food.\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: any good[B-Rating] cheap[B-Price] german[B-Dish] restaurants nearby[B-Location]\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: any good[B-Rating] ice[B-Dish] cream[I-Dish] parlors[I-Dish] around[B-Location]\n",
      "[NeMo I 2022-06-22 12:45:18 infer:77] Results: any good[B-Rating] place to get a pie[B-Dish] at an affordable[B-Price] price\n"
     ]
    }
   ],
   "source": [
    "!grep Results $source_mount/results/bert-base-finetuned_ner/infer.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model should be able to recognize several useful enties with the restaurant context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7.6 Export for Deployment\n",
    "With TAO, we can export the fine-tuned model in a format that can be deployed using NVIDIA Riva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:48:22,170 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:48:22,317 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:48:22,350 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:48:26 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:26 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:26 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:26 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:48:27 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:48:27 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:48:28 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:48:28 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:28 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:32 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:32 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:32 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:32 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:48:33 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:48:33 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:48:33 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:48:33 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:33 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:48:34 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/export.py:85: UserWarning: \n",
      "    'export.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:48:34 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /workspace/mount/results/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt\n",
      "    export_to: exported-model-NER.riva\n",
      "    export_format: RIVA\n",
      "    exp_manager:\n",
      "      task_name: export\n",
      "      explicit_log_dir: /workspace/mount/results/export/\n",
      "    encryption_key: '****'\n",
      "    \n",
      "[NeMo W 2022-06-22 12:48:34 exp_manager:26] Exp_manager is logging to `/workspace/mount/results/export/``, but it already exists.\n",
      "[NeMo I 2022-06-22 12:48:36 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmp8u7fcefd/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 140123812228592 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 43.9kB/s]\n",
      "Lock 140123812228592 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 140123812228976 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|█████████████████████████████| 570/570 [00:00<00:00, 1.03MB/s]\n",
      "Lock 140123812228976 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 140123812228352 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 84.8MB/s]\n",
      "Lock 140123812228352 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140123812228976 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 78.6MB/s]\n",
      "Lock 140123812228976 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:48:37 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2022-06-22 12:48:37 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 140123821354384 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|████████████████████████████| 440M/440M [00:04<00:00, 107MB/s]\n",
      "Lock 140123821354384 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:48:50 export:56] Model restored from '/workspace/mount/results/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt'\n",
      "Could not retrieve the artifact /workspace/tlt/samples/tokenizer.vocab_file used in tokenizer.vocab_file\n",
      "Could not retrieve the artifact /root/.cache/huggingface/nemo_nlp_tmp/bert-base-uncased_encoder_config.json used in language_model.config_file\n",
      "[NeMo I 2022-06-22 12:49:11 export:71] Experiment logs saved to '/workspace/mount/results/export'\n",
      "[NeMo I 2022-06-22 12:49:11 export:72] Exported model to '/workspace/mount/results/export/exported-model-NER.riva'\n",
      "[NeMo I 2022-06-22 12:49:12 export:80] Exported model is compliant with Riva\n",
      "\u001b[0m\u001b[0m2022-06-22 12:49:14,480 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# TAO export to Riva\n",
    "!tao token_classification export \\\n",
    "     -e $SPECS_DIR/token_classification/export.yaml \\\n",
    "     -r $RESULTS_DIR/export/ \\\n",
    "     -m $RESULTS_DIR/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt \\\n",
    "     -k $KEY \\\n",
    "     export_to=exported-model-NER.riva \\\n",
    "     export_format=RIVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your model was exported as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/tao/results/export/exported-model-NER.riva\n"
     ]
    }
   ],
   "source": [
    "!ls /dli/task/tao/results/export/exported-model-NER.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6.1 Exercise: NER Model Export to ONNX\n",
    "\n",
    "Using what you've learned, export the fine-tuned NER model to ONNX format.  Name the final model `exported-model-NER.eonnx`.  If you get stuck, you can look at the [solution](solutions/ex7.6.1.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 12:49:52,050 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-22 12:49:52,189 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-22 12:49:52,224 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-06-22 12:49:56 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:49:56 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:49:56 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:49:56 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:49:57 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:49:57 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2022-06-22 12:49:57 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:49:57 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:49:57 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:50:02 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:50:02 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:50:02 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:50:02 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2022-06-22 12:50:03 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-06-22 12:50:03 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:50:03 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2022-06-22 12:50:03 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:50:03 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-22 12:50:04 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/token_classification/scripts/export.py:85: UserWarning: \n",
      "    'export.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2022-06-22 12:50:04 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /workspace/mount/results/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt\n",
      "    export_to: exported-model-NER.eonnx\n",
      "    export_format: ONNX\n",
      "    exp_manager:\n",
      "      task_name: export\n",
      "      explicit_log_dir: /workspace/mount/results/export/\n",
      "    encryption_key: '*******'\n",
      "    \n",
      "[NeMo W 2022-06-22 12:50:04 exp_manager:26] Exp_manager is logging to `/workspace/mount/results/export/``, but it already exists.\n",
      "[NeMo I 2022-06-22 12:50:06 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmp7bg9ibmi/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 139803419471584 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 23.8kB/s]\n",
      "Lock 139803419471584 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 139803419471488 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 702kB/s]\n",
      "Lock 139803419471488 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 139803419471248 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 74.1MB/s]\n",
      "Lock 139803419471248 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 139803419489136 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 80.9MB/s]\n",
      "Lock 139803419489136 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-06-22 12:50:07 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2022-06-22 12:50:07 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 139803428584992 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|████████████████████████████| 440M/440M [00:04<00:00, 108MB/s]\n",
      "Lock 139803428584992 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-06-22 12:50:20 export:56] Model restored from '/workspace/mount/results/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt'\n",
      "Could not retrieve the artifact /workspace/tlt/samples/tokenizer.vocab_file used in tokenizer.vocab_file\n",
      "Could not retrieve the artifact /root/.cache/huggingface/nemo_nlp_tmp/bert-base-uncased_encoder_config.json used in language_model.config_file\n",
      "[NeMo W 2022-06-22 12:50:21 export_utils:198] Swapped 0 modules\n",
      "[NeMo I 2022-06-22 12:50:49 export:71] Experiment logs saved to '/workspace/mount/results/export'\n",
      "[NeMo I 2022-06-22 12:50:49 export:72] Exported model to '/workspace/mount/results/export/exported-model-NER.eonnx'\n",
      "\u001b[0m\u001b[0m2022-06-22 12:50:50,792 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# TODO export the fine-tuned model to \"exported-model-NER.eonnx\"\n",
    "# TAO export fine-tuned model to ONNX\n",
    "!tao token_classification export \\\n",
    "     -e $SPECS_DIR/token_classification/export.yaml \\\n",
    "     -r $RESULTS_DIR/export/ \\\n",
    "     -m $RESULTS_DIR/bert-base-finetuned_ner/checkpoints/finetuned-model.tlt \\\n",
    "     -k $KEY \\\n",
    "     export_to=exported-model-NER.eonnx \\\n",
    "     export_format=ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You did it!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if(os.path.exists('/dli/task/tao/results/export/exported-model-NER.eonnx')):\n",
    "   print(\"You did it!\")\n",
    "else: \n",
    "   print(\"Sorry, the model isn't there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Gained an understanding IOB formatting for NER datasets\n",
    "- Trained and fine-tuned an NER model with TAO Toolkit\n",
    "- Launched TAO with an implicit docker container to run NER inference on text samples\n",
    "- Exported the model to both ONNX and RIVA formats\n",
    "\n",
    "Next, you'll deploy the model on NVIDIA Riva. Move on to [NLP Deployment with Riva](008_NLP_Deploy_NER.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
