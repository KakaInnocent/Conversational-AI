{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 NER Model Deployment with Riva\n",
    "## (part of Lab 2)\n",
    "\n",
    "In this notebook, you'll begin with a custom `.riva` restaurant NER model.  You'll then convert it to an optimized model with the Riva ServiceMaker framework, which aggregates the necessary artifacts for deployment to a target environment. Finally, you'll connect the deployed model to a demo web app and see it in action!\n",
    "\n",
    "**[8.1 Deployment Recap](#8.1-Deployment-Recap)<br>**\n",
    "**[8.2 Riva ServiceMaker](#8.2-Riva-ServiceMaker)<br>**\n",
    "**[8.3 Riva Server](#8.3-Riva-Server)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.3.1 Exercise: Riva Configuration Custom NER](#8.3.1-Exercise:-Riva-Configuration-Custom-NER)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.3.2 Start Riva Services](#8.3.2-Start-Riva-Services)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.3.3 Riva Available Services Check](#8.3.3-Riva-Available-Services-Check)<br>\n",
    "**[8.4 Riva NLP Service Request](#8.4-Riva-NLP-Service-Request)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.4.1 Python Client Demo](#8.4.1-Python-Client-Demo)<br>\n",
    "**[8.5 Restaurant NER with the Riva Contact App](#8.5-Restaurant-NER-with-the-Riva-Contact-App)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.5.1 Stop, Reconfigure, and Restart Riva](#8.5.1-Stop,-Reconfigure,-and-Restart-Riva)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[8.5.1.1 Exercise: Enable ASR in config.sh](#8.5.1.1-Exercise:-Enable-ASR-in-config.sh)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[8.5.1.2 Restart Riva Services](#8.5.1.2-Restart-Riva-Services)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.5.2 Update and Start Riva Contact App](#8.5.2-Update-and-Start-Riva-Contact-App)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[8.5.2.1 Exercise: Update `env.txt`](#8.5.2.1-Exercise:-Update-env.txt)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[8.5.2.2 Start the Contact Web Server](#8.5.2.2-Start-the-Contact-Web-Server)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.5.3 Stop Riva Services](#8.5.3-Stop-Riva-Services)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "To run this app on your system, you will need:\n",
    "1. **Microphone**<br>\n",
    "For best ASR results, a headset is recommended.  \n",
    "1. **Chrome browser**<br>\n",
    "In order to use the app over HTTP in our class setup, you will need to override the browser block to your camera and microphone.  Instructions are included later in the notebook.\n",
    "1. **NGC Credentials**<br>Be sure you have added your NGC credential as described in the [NGC Setup notebook](003_Intro_NGC_Setup.ipynb)\n",
    "2. **exported-model-NER.riva**<br>\n",
    "Execute the next cell to load a backup copy of the exported Riva NER model into the correct location if it was not  exported in the previous notebook with `tao token_classification export`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /dli/task/tao/results/export\n",
    "cp /dli/task/tao/backup_riva/exported-model-NER.riva /dli/task/tao/results/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# Check running docker containers. This should be empty.\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not empty,\n",
    "# Clear Docker containers to start fresh...\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8.1 Deployment Recap\n",
    "Deployment with Riva ServiceMaker `riva-build` and `riva-deploy` commands work the same way for our new NER model as for the ASR model exported in an earlier notebook.  To recap, the components of deployment are:\n",
    "1. **Riva ServiceMaker**\n",
    "   - Set up path variables for a clean workflow\n",
    "   - Run the ServiceMaker container with `riva-build` for the token_classification to combine `.riva` files and create a RMIR file\n",
    "   - Run the ServiceMaker container with `riva-deploy` to deploy the RMIR file to a specified target model repository\n",
    "1. **Riva Server**\n",
    "    - Configure the `config.sh` file for your application\n",
    "    - Initialize the server with `javis_init.sh`\n",
    "    - Start the Riva server with `riva_start.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8.2 Riva ServiceMaker\n",
    "Set up the path variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the WORKSPACE path to \"/path/to/your/workspace\"\n",
    "WORKSPACE = \"/dli/task\"\n",
    "\n",
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-servicemaker\"\n",
    "\n",
    "# Directory where the .riva model is stored $MODEL_LOC/*.riva\n",
    "MODEL_LOC = WORKSPACE + '/tao/results/export'\n",
    "\n",
    "# Directory where the .rmir model is stored $RMIR_LOC/*.rmir\n",
    "RIVA_MODEL_LOC = WORKSPACE + '/riva/riva_quickstart/models_repo_NER'\n",
    "RMIR_LOC = RIVA_MODEL_LOC + \"/rmir\"\n",
    "\n",
    "# Name of the .erjvs file\n",
    "EXPORT_MODEL_NAME = \"exported-model-NER.riva\"\n",
    "RMIR_MODEL_NAME = \"token-classification.rmir\"\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TAO\n",
    "KEY='tlt_encode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "2022-06-22 12:58:24,438 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-06-22 12:58:24,509 [INFO] Packing binaries for label_tokens\n",
      "2022-06-22 12:58:25,696 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-06-22 12:58:25,757 [INFO] Trying to extract from model exported-model-NER.riva\n",
      "2022-06-22 12:58:26,982 [INFO] Packing binaries for language_model\n",
      "2022-06-22 12:58:28,151 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-06-22 12:58:28,213 [INFO] Trying to extract from model exported-model-NER.riva\n",
      "2022-06-22 12:58:34,316 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-06-22 12:58:34,379 [INFO] Trying to extract from model exported-model-NER.riva\n",
      "2022-06-22 12:58:35,625 [INFO] Packing binaries for tokenizer\n",
      "2022-06-22 12:58:36,805 [ERROR] Condition for key 'runtime' (PyTorch  <built-in function eq> ONNX) is not fulfilled\n",
      "2022-06-22 12:58:36,869 [INFO] Trying to extract from model exported-model-NER.riva\n"
     ]
    }
   ],
   "source": [
    "# Syntax: riva-build <task-name> output-dir-for-rmir/model.rmir:key dir-for-riva/model.riva:key\n",
    "!docker run --rm --gpus 1 \\\n",
    "    -v $MODEL_LOC:/tao \\\n",
    "    -v $RMIR_LOC:/riva \\\n",
    "    $RIVA_SM_CONTAINER -- \\\n",
    "    riva-build token_classification /riva/$RMIR_MODEL_NAME:$KEY /tao/$EXPORT_MODEL_NAME:$KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token-classification.rmir\n"
     ]
    }
   ],
   "source": [
    "# check the token-classification RMIR file\n",
    "!ls $RMIR_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "2022-06-22 13:14:46,587 [INFO] Writing Riva model repository to '/data/models/'...\n",
      "2022-06-22 13:14:46,587 [INFO] The riva model repo target directory is /data/models/\n",
      "2022-06-22 13:14:47,752 [INFO] Extract_binaries for tokenizer -> /data/models/riva_tokenizer/1\n",
      "2022-06-22 13:14:47,754 [INFO] Extract_binaries for language_model -> /data/models/riva-trt-riva_ner-nn-bert-base-uncased/1\n",
      "2022-06-22 13:14:51,865 [INFO] Printing copied artifacts:\n",
      "2022-06-22 13:14:51,865 [INFO] {'ckpt': '/data/models/riva-trt-riva_ner-nn-bert-base-uncased/1/model_weights.ckpt', 'bert_config_file': '/data/models/riva-trt-riva_ner-nn-bert-base-uncased/1/bert-base-uncased_encoder_config.json'}\n",
      "2022-06-22 13:14:51,865 [INFO] Building TRT engine from PyTorch Checkpoint\n",
      "2022-06-22 13:16:19,963 [INFO] NER classes: 17\n",
      "2022-06-22 13:16:19,964 [INFO] Extract_binaries for label_tokens -> /data/models/riva_ner_label_tokens/1\n",
      "2022-06-22 13:16:19,966 [INFO] Extract_binaries for detokenizer -> /data/models/riva_detokenize/1\n",
      "2022-06-22 13:16:19,967 [INFO] Extract_binaries for self -> /data/models/riva_ner/1\n"
     ]
    }
   ],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "!docker run --rm --gpus 1 \\\n",
    "     -v $RIVA_MODEL_LOC:/data \\\n",
    "     $RIVA_SM_CONTAINER -- \\\n",
    "     riva-deploy -f  /data/rmir/$RMIR_MODEL_NAME:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riva-trt-riva_ner-nn-bert-base-uncased\triva_ner\t       riva_tokenizer\n",
      "riva_detokenize\t\t\t\triva_ner_label_tokens\n"
     ]
    }
   ],
   "source": [
    "# Check optimized models \n",
    "!ls $RIVA_MODEL_LOC/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`riva-ner/config.pbtxt`](riva/riva_quickstart/models_repo_NER/models/riva-ner/config.pbtxt) describes the model input/output format and the ensemble scheduling for the NER task, which includes the following models:\n",
    "\n",
    "- Tokenizer `riva_tokenizer`\n",
    "- NER `riva-trt-riva_ner-nn-bert-base-uncased`\n",
    "- Mapper for labels to predictions `riva_ner_label_tokens`\n",
    "- Detokenizer `riva_detokenize`\n",
    "\n",
    "<img src=\"images/ner/riva_ner.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8.3 Riva Server\n",
    "\n",
    "Once the model repository is generated, we are ready to start the Riva server. As before with ASR, we rely on the scripts from the [Riva Quick Start](https://ngc.nvidia.com/catalog/resources/nvidia:riva:riva_quickstart) scripts from NGC (preloaded for this course).  \n",
    "\n",
    "Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva Quick Start directory\n",
    "RIVA_QS = WORKSPACE + \"/riva/riva_quickstart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.1 Exercise: Riva Configuration Custom NER\n",
    "\n",
    "Once again, we must modify [config.sh](riva/riva_quickstart/config.sh) for this particular deployment.  We need to:\n",
    "* Enable NLP services for token classification.\n",
    "* Provide the encryption key (should already be correct)\n",
    "* Provide the path to the model repository where we generated the RMIR model in the previous step, which is now located at `/dli/task/riva/riva_quickstart/models_repo_NER`\n",
    "\n",
    "Edit [config.sh](riva/riva_quickstart/config.sh) and make changes where necessary. \n",
    "\n",
    "Check your work against the [solution](solutions/ex8.3.1.sh) before moving on to the next section.  You can verify it with `diff` in the next cell. You should get no \"difference\" (an empty output) if your config file matches the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modify config.sh so that this cell verifies changes are correct\n",
    "# There should be no output if the files match\n",
    "!diff $RIVA_QS/config.sh solutions/ex8.3.1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.2 Start Riva Services\n",
    "Initialize and start the Riva server with NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging into NGC docker registry if necessary...\n",
      "Pulling required docker images if necessary...\n",
      "Note: This may take some time, depending on the speed of your Internet connection.\n",
      "> Pulling Riva Speech Server images.\n",
      "  > Image nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-server exists. Skipping.\n",
      "  > Image nvcr.io/nvidia/riva/riva-speech-client:1.4.0-beta exists. Skipping.\n",
      "  > Image nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-servicemaker exists. Skipping.\n",
      "\n",
      "Downloading models (RMIRs) from NGC...\n",
      "Note: this may take some time, depending on the speed of your Internet connection.\n",
      "To skip this process and use existing RMIRs set the location and corresponding flag in config.sh.\n",
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "/data/artifacts /opt/riva\n",
      "/opt/riva\n",
      "\n",
      "Converting RMIRs at /dli/task/riva/riva_quickstart/models_repo_NER/rmir to Riva Model repository.\n",
      "+ docker run --init -it --rm --gpus '\"device=0\"' -v /dli/task/riva/riva_quickstart/models_repo_NER:/data -e MODEL_DEPLOY_KEY=tlt_encode --name riva-service-maker nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-servicemaker deploy_all_models /data/rmir /data/models\n",
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "2022-06-22 13:21:58,736 [INFO] Writing Riva model repository to '/data/models'...\n",
      "2022-06-22 13:21:58,737 [INFO] The riva model repo target directory is /data/models\n",
      "2022-06-22 13:21:59,923 [WARNING] /data/models/riva_tokenizer already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva_tokenizer\n",
      "2022-06-22 13:21:59,923 [WARNING] /data/models/riva-trt-riva_ner-nn-bert-base-uncased already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva-trt-riva_ner-nn-bert-base-uncased\n",
      "2022-06-22 13:21:59,923 [WARNING] /data/models/riva_ner_label_tokens already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva_ner_label_tokens\n",
      "2022-06-22 13:21:59,923 [WARNING] /data/models/riva_detokenize already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva_detokenize\n",
      "2022-06-22 13:21:59,923 [WARNING] /data/models/riva_ner already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva_ner\n",
      "+ echo\n",
      "\n",
      "+ echo 'Riva initialization complete. Run ./riva_start.sh to launch services.'\n",
      "Riva initialization complete. Run ./riva_start.sh to launch services.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Riva\n",
    "!cd $RIVA_QS && bash riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.\n",
      "Waiting for Riva server to load all models...retrying in 10 seconds\n",
      "Waiting for Riva server to load all models...retrying in 10 seconds\n",
      "Riva server is ready...\n"
     ]
    }
   ],
   "source": [
    "# Run Riva Start. This will deploy the model(s).\n",
    "!cd $RIVA_QS && bash riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Riva NLP service should be running when you get `Riva server is ready...` (about 30 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.3 Riva Available Services Check\n",
    "\n",
    "Check available NLP models. You should see this:\n",
    "\n",
    "<img src=\"images/ner/riva_ner_logs.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker logs riva-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8.4 Riva NLP Service Request\n",
    "\n",
    "After the Riva server is up and running with your models, you can query the server. \n",
    "To send gRPC requests, Riva Python API bindings for the client must be installed. This is available as a pip wheel with the quick start directory.  For this class, the API is already installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.1 Python Client Demo\n",
    "The following cell creates a Python file that queries the Riva server (using gRPC) to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /dli/task/riva/riva_quickstart/ner_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $RIVA_QS/ner_client.py\n",
    "\n",
    "import grpc\n",
    "import os\n",
    "import argparse\n",
    "import riva_api.riva_nlp_pb2 as rnlp\n",
    "import riva_api.riva_nlp_pb2_grpc as rnlp_srv\n",
    "\n",
    "# use the NER network to return top-1 classes for entities\n",
    "def postprocess_labels_server(tokens_response):\n",
    "    results = []\n",
    "    for i in range(0, len(tokens_response.results)):\n",
    "        slots = []\n",
    "        slot_scores = []\n",
    "        tokens = []\n",
    "        for j in range(0, len(tokens_response.results[i].results)):\n",
    "          entity = tokens_response.results[i].results[j]\n",
    "          tokens.append(entity.token)\n",
    "          slots.append(entity.label[0].class_name)\n",
    "          slot_scores.append(entity.label[0].score)\n",
    "        results.append((slots, tokens, slot_scores))\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_ner(grpc_server, query):\n",
    "    channel = grpc.insecure_channel(grpc_server)\n",
    "    riva_nlp = rnlp_srv.RivaLanguageUnderstandingStub(channel)\n",
    "    req = rnlp.AnalyzeEntitiesRequest()\n",
    "    req.query = query\n",
    "    resp = riva_nlp.AnalyzeEntities(req)\n",
    "    print(\"Query:\", query)\n",
    "    print(postprocess_labels_server(resp))\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Client app to test named entity recognition on Riva\")\n",
    "    parser.add_argument(\"--server\", default=\"localhost:50051\", type=str, help=\"URI to GRPC server endpoint\")\n",
    "    parser.add_argument(\"--model\", default=\"riva_ner\", type=str, help=\"Model on Riva Server to execute\")\n",
    "    parser.add_argument(\"--query\", default=\"NVIDIA is located at Santa Clara\", type=str, help=\"Input Query\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def run_ner_client():\n",
    "    args = get_args()\n",
    "    run_ner(args.server, query=args.query)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_ner_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the NLP service for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  I would like to order a pizza at 8pm\n",
      "[(['Dish', 'Hours'], ['pizza', '8pm'], [0.9604489803314209, 0.8666989803314209])]\n"
     ]
    }
   ],
   "source": [
    "!python3 $RIVA_QS/ner_client.py --query \" I would like to order a pizza at 8pm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8.5 Restaurant NER with the Riva Contact App\n",
    "The Riva Contact app requires both ASR and NER to run, and we need to let the app know what entities we are tracking with our custom model.  Therefore, we need to:\n",
    "1. Riva Server:\n",
    "    - Stop Riva Services\n",
    "    - Copy the ASR models we need into the same directory as our custom NER model\n",
    "    - Reconfigure `config.sh` to include ASR\n",
    "    - Start Riva Services\n",
    "2. Riva Contact App\n",
    "    - Modify [env.txt](contact-app/env.txt) for the Restaurant entities\n",
    "    - Re-install the contact app\n",
    "    - Start the app server\n",
    "    - Open the app!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.1 Stop, Reconfigure, and Restart Riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down docker containers...\n"
     ]
    }
   ],
   "source": [
    "# Run Riva Stop. \n",
    "!bash $RIVA_QS/riva_stop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the ASR models to our model repo\n",
    "!cp -rf $WORKSPACE/riva/models/quartznet-asr-trt-ensemble-vad-streaming $RIVA_MODEL_LOC/models/\n",
    "!cp -rf $WORKSPACE/riva/models/quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming $RIVA_MODEL_LOC/models/\n",
    "!cp -rf $WORKSPACE/riva/models/riva-trt-quartznet $RIVA_MODEL_LOC/models/\n",
    "!cp -rf $WORKSPACE/riva/models/quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming $RIVA_MODEL_LOC/models/\n",
    "!cp -rf $WORKSPACE/riva/models/quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming $RIVA_MODEL_LOC/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quartznet-asr-trt-ensemble-vad-streaming\n",
      "quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming\n",
      "quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming\n",
      "quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming\n",
      "riva-trt-quartznet\n",
      "riva-trt-riva_ner-nn-bert-base-uncased\n",
      "riva_detokenize\n",
      "riva_ner\n",
      "riva_ner_label_tokens\n",
      "riva_tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Check optimized models in repo\n",
    "!ls $RIVA_MODEL_LOC/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1.1 Exercise: Enable ASR in `config.sh`\n",
    "You are good at this by now!  Modify [config.sh](riva/riva_quickstart/config.sh) to enable ASR services in addition to NLP services.  The model repo is the same as before since we copied the ASR models to that directory.  Check your work against the [solution](solutions/ex8.5.1.1.sh) before restarting Riva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modify config.sh so that this cell verifies changes are correct\n",
    "# There should be no output if the files match\n",
    "!diff $RIVA_QS/config.sh solutions/ex8.5.1.1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1.2 Restart Riva Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging into NGC docker registry if necessary...\n",
      "Pulling required docker images if necessary...\n",
      "Note: This may take some time, depending on the speed of your Internet connection.\n",
      "> Pulling Riva Speech Server images.\n",
      "  > Image nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-server exists. Skipping.\n",
      "  > Image nvcr.io/nvidia/riva/riva-speech-client:1.4.0-beta exists. Skipping.\n",
      "  > Image nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-servicemaker exists. Skipping.\n",
      "\n",
      "Downloading models (RMIRs) from NGC...\n",
      "Note: this may take some time, depending on the speed of your Internet connection.\n",
      "To skip this process and use existing RMIRs set the location and corresponding flag in config.sh.\n",
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "/data/artifacts /opt/riva\n",
      "/opt/riva\n",
      "\n",
      "Converting RMIRs at /dli/task/riva/riva_quickstart/models_repo_NER/rmir to Riva Model repository.\n",
      "+ docker run --init -it --rm --gpus '\"device=0\"' -v /dli/task/riva/riva_quickstart/models_repo_NER:/data -e MODEL_DEPLOY_KEY=tlt_encode --name riva-service-maker nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-servicemaker deploy_all_models /data/rmir /data/models\n",
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "2022-06-22 13:26:20,764 [INFO] Writing Riva model repository to '/data/models'...\n",
      "2022-06-22 13:26:20,764 [INFO] The riva model repo target directory is /data/models\n",
      "2022-06-22 13:26:21,927 [WARNING] /data/models/riva_tokenizer already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva_tokenizer\n",
      "2022-06-22 13:26:21,927 [WARNING] /data/models/riva-trt-riva_ner-nn-bert-base-uncased already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva-trt-riva_ner-nn-bert-base-uncased\n",
      "2022-06-22 13:26:21,927 [WARNING] /data/models/riva_ner_label_tokens already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva_ner_label_tokens\n",
      "2022-06-22 13:26:21,927 [WARNING] /data/models/riva_detokenize already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva_detokenize\n",
      "2022-06-22 13:26:21,927 [WARNING] /data/models/riva_ner already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva_ner\n",
      "+ echo\n",
      "\n",
      "+ echo 'Riva initialization complete. Run ./riva_start.sh to launch services.'\n",
      "Riva initialization complete. Run ./riva_start.sh to launch services.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Riva\n",
    "!cd $RIVA_QS && bash riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.\n",
      "Waiting for Riva server to load all models...retrying in 10 seconds\n",
      "Waiting for Riva server to load all models...retrying in 10 seconds\n",
      "Riva server is ready...\n"
     ]
    }
   ],
   "source": [
    "# Run Riva Start. This will deploy the model(s).\n",
    "!cd $RIVA_QS && bash riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release 21.07 (build 25292380)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0622 13:26:36.869200 73 metrics.cc:228] Collecting metrics for GPU 0: Tesla T4\n",
      "I0622 13:26:36.872480 73 onnxruntime.cc:1722] TRITONBACKEND_Initialize: onnxruntime\n",
      "I0622 13:26:36.872507 73 onnxruntime.cc:1732] Triton TRITONBACKEND API version: 1.0\n",
      "I0622 13:26:36.872515 73 onnxruntime.cc:1738] 'onnxruntime' TRITONBACKEND API version: 1.0\n",
      "I0622 13:26:37.056442 73 pinned_memory_manager.cc:206] Pinned memory pool is created at '0x7f0f90000000' with size 268435456\n",
      "I0622 13:26:37.056840 73 cuda_memory_manager.cc:103] CUDA memory pool is created on device 0 with size 1000000000\n",
      "I0622 13:26:37.066019 73 model_repository_manager.cc:1066] loading: quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming:1\n",
      "I0622 13:26:37.166351 73 model_repository_manager.cc:1066] loading: quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming:1\n",
      "I0622 13:26:37.166797 73 custom_backend.cc:201] Creating instance quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming_0_0_gpu0 on GPU 0 (7.5) using libtriton_riva_asr_features.so\n",
      "I0622 13:26:37.266691 73 model_repository_manager.cc:1066] loading: quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming:1\n",
      "I0622 13:26:37.267042 73 custom_backend.cc:198] Creating instance quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming_0_0_cpu on CPU using libtriton_riva_asr_decoder_cpu.so\n",
      "W:parameter_parser.cc:106: Parameter forerunner_start_offset_ms could not be set from parameters\n",
      "W:parameter_parser.cc:107: Default value will be used\n",
      "W:parameter_parser.cc:106: Parameter voc_string could not be set from parameters\n",
      "W:parameter_parser.cc:107: Default value will be used\n",
      "I0622 13:26:37.367049 73 model_repository_manager.cc:1066] loading: riva-trt-quartznet:1\n",
      "I0622 13:26:37.367523 73 custom_backend.cc:198] Creating instance quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming_0_0_cpu on CPU using libtriton_riva_asr_vad.so\n",
      "I0622 13:26:37.388893 73 model_repository_manager.cc:1240] successfully loaded 'quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming' version 1\n",
      "I0622 13:26:37.467637 73 model_repository_manager.cc:1066] loading: riva-trt-riva_ner-nn-bert-base-uncased:1\n",
      "I0622 13:26:37.567944 73 model_repository_manager.cc:1066] loading: riva_detokenize:1\n",
      "I0622 13:26:37.668578 73 model_repository_manager.cc:1066] loading: riva_ner_label_tokens:1\n",
      "I0622 13:26:37.669299 73 custom_backend.cc:198] Creating instance riva_detokenize_0_0_cpu on CPU using libtriton_riva_nlp_detokenizer.so\n",
      "I0622 13:26:37.670648 73 model_repository_manager.cc:1240] successfully loaded 'riva_detokenize' version 1\n",
      "I0622 13:26:37.769214 73 model_repository_manager.cc:1066] loading: riva_tokenizer:1\n",
      "I0622 13:26:37.769959 73 custom_backend.cc:198] Creating instance riva_ner_label_tokens_0_0_cpu on CPU using libtriton_riva_nlp_seqlabel.so\n",
      "I0622 13:26:37.771470 73 model_repository_manager.cc:1240] successfully loaded 'riva_ner_label_tokens' version 1\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0622 13:26:37.870449 73 custom_backend.cc:198] Creating instance riva_tokenizer_0_0_cpu on CPU using libtriton_riva_nlp_tokenizer.so\n",
      "I0622 13:26:37.903726 73 model_repository_manager.cc:1240] successfully loaded 'riva_tokenizer' version 1\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0622 13:26:43.102116 73 model_repository_manager.cc:1240] successfully loaded 'quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming' version 1\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0622 13:26:50.964178 73 model_repository_manager.cc:1240] successfully loaded 'quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming' version 1\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0622 13:26:51.497773 73 plan_backend.cc:384] Creating instance riva-trt-quartznet_0_0_gpu0 on GPU 0 (7.5) using model.plan\n",
      "I0622 13:26:51.777687 73 plan_backend.cc:768] Created instance riva-trt-quartznet_0_0_gpu0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "I0622 13:26:51.779507 73 model_repository_manager.cc:1240] successfully loaded 'riva-trt-quartznet' version 1\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0622 13:26:53.089873 73 plan_backend.cc:384] Creating instance riva-trt-riva_ner-nn-bert-base-uncased_0_0_gpu0 on GPU 0 (7.5) using model.plan\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0622 13:26:54.446961 73 plan_backend.cc:768] Created instance riva-trt-riva_ner-nn-bert-base-uncased_0_0_gpu0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "I0622 13:26:54.453862 73 model_repository_manager.cc:1240] successfully loaded 'riva-trt-riva_ner-nn-bert-base-uncased' version 1\n",
      "I0622 13:26:54.454440 73 model_repository_manager.cc:1066] loading: quartznet-asr-trt-ensemble-vad-streaming:1\n",
      "I0622 13:26:54.554685 73 model_repository_manager.cc:1066] loading: riva_ner:1\n",
      "I0622 13:26:54.654967 73 model_repository_manager.cc:1240] successfully loaded 'quartznet-asr-trt-ensemble-vad-streaming' version 1\n",
      "I0622 13:26:54.655202 73 model_repository_manager.cc:1240] successfully loaded 'riva_ner' version 1\n",
      "I0622 13:26:54.655304 73 server.cc:504] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0622 13:26:54.655362 73 server.cc:543] \n",
      "+-------------+-----------------------------------------------------------------+--------+\n",
      "| Backend     | Path                                                            | Config |\n",
      "+-------------+-----------------------------------------------------------------+--------+\n",
      "| tensorrt    | <built-in>                                                      | {}     |\n",
      "| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {}     |\n",
      "+-------------+-----------------------------------------------------------------+--------+\n",
      "\n",
      "I0622 13:26:54.655505 73 server.cc:586] \n",
      "+--------------------------------------------------------------------------------+---------+--------+\n",
      "| Model                                                                          | Version | Status |\n",
      "+--------------------------------------------------------------------------------+---------+--------+\n",
      "| quartznet-asr-trt-ensemble-vad-streaming                                       | 1       | READY  |\n",
      "| quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming             | 1       | READY  |\n",
      "| quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming           | 1       | READY  |\n",
      "| quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming | 1       | READY  |\n",
      "| riva-trt-quartznet                                                             | 1       | READY  |\n",
      "| riva-trt-riva_ner-nn-bert-base-uncased                                         | 1       | READY  |\n",
      "| riva_detokenize                                                                | 1       | READY  |\n",
      "| riva_ner                                                                       | 1       | READY  |\n",
      "| riva_ner_label_tokens                                                          | 1       | READY  |\n",
      "| riva_tokenizer                                                                 | 1       | READY  |\n",
      "+--------------------------------------------------------------------------------+---------+--------+\n",
      "\n",
      "I0622 13:26:54.655616 73 tritonserver.cc:1658] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                  |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                 |\n",
      "| server_version                   | 2.9.0                                                                                                                                                                                  |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\n",
      "| model_repository_path[0]         | /data/models                                                                                                                                                                           |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                              |\n",
      "| strict_model_config              | 1                                                                                                                                                                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\n",
      "| cuda_memory_pool_byte_size{0}    | 1000000000                                                                                                                                                                             |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                      |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                     |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0622 13:26:54.656597 73 grpc_server.cc:4028] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0622 13:26:54.656822 73 http_server.cc:2761] Started HTTPService at 0.0.0.0:8000\n",
      "I0622 13:26:54.698150 73 http_server.cc:2780] Started Metrics Service at 0.0.0.0:8002\n",
      "  > Triton server is ready...\n",
      "I0622 13:26:55.025348   188 grpc_health.cc:27] RivaHealthService initialized with server: localhost:8001\n",
      "I0622 13:26:55.026227   188 grpc_riva_asr.cc:148] Setting uri for ASRServiceImpl\n",
      "I0622 13:26:55.026237   188 grpc_riva_asr.cc:149] Initializing different models\n",
      "I0622 13:26:55.026576   188 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0622 13:26:55.027493   188 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0622 13:26:55.027781   188 model_registry.cc:86] Our model repository has a total of: 10 models\n",
      "I0622 13:26:55.027791   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming, Model version: 1\n",
      "I0622 13:26:55.030337   188 model_registry.cc:104] 'Successfully registering quartznet-asr-trt-ensemble-vad-streaming'\n",
      "I0622 13:26:55.030391   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0622 13:26:55.031455   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming, Model version: 1\n",
      "I0622 13:26:55.032441   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0622 13:26:55.033169   188 model_registry.cc:91] Model names: riva-trt-quartznet, Model version: 1\n",
      "I0622 13:26:55.033787   188 model_registry.cc:91] Model names: riva-trt-riva_ner-nn-bert-base-uncased, Model version: 1\n",
      "I0622 13:26:55.034415   188 model_registry.cc:91] Model names: riva_detokenize, Model version: 1\n",
      "I0622 13:26:55.035081   188 model_registry.cc:91] Model names: riva_ner, Model version: 1\n",
      "I0622 13:26:55.035878   188 model_registry.cc:91] Model names: riva_ner_label_tokens, Model version: 1\n",
      "I0622 13:26:55.036480   188 model_registry.cc:91] Model names: riva_tokenizer, Model version: 1\n",
      "I0622 13:26:55.037124   188 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0622 13:26:55.037138   188 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0622 13:26:55.037353   188 client.cc:54] Our model repository has: 10 models.\n",
      "W0622 13:26:55.038241   188 client.cc:78] Registration of 'quartznet-asr-trt-ensemble-vad-streaming' failed with unknown service type\n",
      "I0622 13:26:55.042809   188 client.cc:72] Registering 'riva_ner' with service '/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyTokens'\n",
      "I0622 13:26:55.047684   188 grpc_riva_asr.cc:173] Punctuation model does not exist on server\n",
      "I0622 13:26:55.047698   188 grpc_riva_asr.cc:177] Seeding RNG used for correlation id with time: 1655904415\n",
      "I0622 13:26:55.094355   188 grpc_riva_asr.cc:148] Setting uri for ASRServiceImpl\n",
      "I0622 13:26:55.094377   188 grpc_riva_asr.cc:149] Initializing different models\n",
      "I0622 13:26:55.094385   188 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0622 13:26:55.094823   188 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0622 13:26:55.095059   188 model_registry.cc:86] Our model repository has a total of: 10 models\n",
      "I0622 13:26:55.095070   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming, Model version: 1\n",
      "I0622 13:26:55.096079   188 model_registry.cc:104] 'Successfully registering quartznet-asr-trt-ensemble-vad-streaming'\n",
      "I0622 13:26:55.096138   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0622 13:26:55.097075   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming, Model version: 1\n",
      "I0622 13:26:55.098024   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0622 13:26:55.098757   188 model_registry.cc:91] Model names: riva-trt-quartznet, Model version: 1\n",
      "I0622 13:26:55.099411   188 model_registry.cc:91] Model names: riva-trt-riva_ner-nn-bert-base-uncased, Model version: 1\n",
      "I0622 13:26:55.100049   188 model_registry.cc:91] Model names: riva_detokenize, Model version: 1\n",
      "I0622 13:26:55.100680   188 model_registry.cc:91] Model names: riva_ner, Model version: 1\n",
      "I0622 13:26:55.101444   188 model_registry.cc:91] Model names: riva_ner_label_tokens, Model version: 1\n",
      "I0622 13:26:55.102015   188 model_registry.cc:91] Model names: riva_tokenizer, Model version: 1\n",
      "I0622 13:26:55.102658   188 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0622 13:26:55.102672   188 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0622 13:26:55.102891   188 client.cc:54] Our model repository has: 10 models.\n",
      "W0622 13:26:55.103864   188 client.cc:78] Registration of 'quartznet-asr-trt-ensemble-vad-streaming' failed with unknown service type\n",
      "I0622 13:26:55.108458   188 client.cc:72] Registering 'riva_ner' with service '/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyTokens'\n",
      "I0622 13:26:55.109587   188 grpc_riva_asr.cc:173] Punctuation model does not exist on server\n",
      "I0622 13:26:55.109599   188 grpc_riva_asr.cc:177] Seeding RNG used for correlation id with time: 1655904415\n",
      "I0622 13:26:55.140316   188 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0622 13:26:55.140762   188 client.cc:54] Our model repository has: 10 models.\n",
      "W0622 13:26:55.141770   188 client.cc:78] Registration of 'quartznet-asr-trt-ensemble-vad-streaming' failed with unknown service type\n",
      "I0622 13:26:55.146430   188 client.cc:72] Registering 'riva_ner' with service '/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyTokens'\n",
      "I0622 13:26:55.147621   188 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0622 13:26:55.147949   188 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0622 13:26:55.148207   188 model_registry.cc:86] Our model repository has a total of: 10 models\n",
      "I0622 13:26:55.148221   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming, Model version: 1\n",
      "I0622 13:26:55.149060   188 model_registry.cc:104] 'Successfully registering quartznet-asr-trt-ensemble-vad-streaming'\n",
      "I0622 13:26:55.149101   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0622 13:26:55.149938   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming, Model version: 1\n",
      "I0622 13:26:55.150890   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0622 13:26:55.151665   188 model_registry.cc:91] Model names: riva-trt-quartznet, Model version: 1\n",
      "I0622 13:26:55.152323   188 model_registry.cc:91] Model names: riva-trt-riva_ner-nn-bert-base-uncased, Model version: 1\n",
      "I0622 13:26:55.152931   188 model_registry.cc:91] Model names: riva_detokenize, Model version: 1\n",
      "I0622 13:26:55.153523   188 model_registry.cc:91] Model names: riva_ner, Model version: 1\n",
      "I0622 13:26:55.154265   188 model_registry.cc:104] 'Successfully registering riva_ner'\n",
      "I0622 13:26:55.154297   188 model_registry.cc:91] Model names: riva_ner_label_tokens, Model version: 1\n",
      "I0622 13:26:55.154819   188 model_registry.cc:91] Model names: riva_tokenizer, Model version: 1\n",
      "I0622 13:26:55.155508   188 model_registry.cc:109] Successfully registered: 2 models.\n",
      "I0622 13:26:55.155526   188 grpc_riva_nlp.cc:33] NLPService GRPC service started\n",
      "I0622 13:26:55.155532   188 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0622 13:26:55.155776   188 client.cc:54] Our model repository has: 10 models.\n",
      "W0622 13:26:55.156623   188 client.cc:78] Registration of 'quartznet-asr-trt-ensemble-vad-streaming' failed with unknown service type\n",
      "I0622 13:26:55.161147   188 client.cc:72] Registering 'riva_ner' with service '/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyTokens'\n",
      "I0622 13:26:55.162238   188 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0622 13:26:55.162545   188 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0622 13:26:55.162763   188 model_registry.cc:86] Our model repository has a total of: 10 models\n",
      "I0622 13:26:55.162775   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming, Model version: 1\n",
      "I0622 13:26:55.163662   188 model_registry.cc:104] 'Successfully registering quartznet-asr-trt-ensemble-vad-streaming'\n",
      "I0622 13:26:55.163717   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0622 13:26:55.164669   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming, Model version: 1\n",
      "I0622 13:26:55.165565   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0622 13:26:55.166332   188 model_registry.cc:91] Model names: riva-trt-quartznet, Model version: 1\n",
      "I0622 13:26:55.166954   188 model_registry.cc:91] Model names: riva-trt-riva_ner-nn-bert-base-uncased, Model version: 1\n",
      "I0622 13:26:55.167649   188 model_registry.cc:91] Model names: riva_detokenize, Model version: 1\n",
      "I0622 13:26:55.168278   188 model_registry.cc:91] Model names: riva_ner, Model version: 1\n",
      "I0622 13:26:55.169029   188 model_registry.cc:104] 'Successfully registering riva_ner'\n",
      "I0622 13:26:55.169070   188 model_registry.cc:91] Model names: riva_ner_label_tokens, Model version: 1\n",
      "I0622 13:26:55.169620   188 model_registry.cc:91] Model names: riva_tokenizer, Model version: 1\n",
      "I0622 13:26:55.170255   188 model_registry.cc:109] Successfully registered: 2 models.\n",
      "I0622 13:26:55.170269   188 grpc_riva_nlp.cc:33] NLPService GRPC service started\n",
      "I0622 13:26:55.170275   188 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0622 13:26:55.170482   188 client.cc:54] Our model repository has: 10 models.\n",
      "W0622 13:26:55.171416   188 client.cc:78] Registration of 'quartznet-asr-trt-ensemble-vad-streaming' failed with unknown service type\n",
      "I0622 13:26:55.176173   188 client.cc:72] Registering 'riva_ner' with service '/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyTokens'\n",
      "I0622 13:26:55.177296   188 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0622 13:26:55.177618   188 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0622 13:26:55.177814   188 model_registry.cc:86] Our model repository has a total of: 10 models\n",
      "I0622 13:26:55.177824   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming, Model version: 1\n",
      "I0622 13:26:55.178650   188 model_registry.cc:104] 'Successfully registering quartznet-asr-trt-ensemble-vad-streaming'\n",
      "I0622 13:26:55.178696   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0622 13:26:55.179598   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming, Model version: 1\n",
      "I0622 13:26:55.180573   188 model_registry.cc:91] Model names: quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0622 13:26:55.181284   188 model_registry.cc:91] Model names: riva-trt-quartznet, Model version: 1\n",
      "I0622 13:26:55.181916   188 model_registry.cc:91] Model names: riva-trt-riva_ner-nn-bert-base-uncased, Model version: 1\n",
      "I0622 13:26:55.182514   188 model_registry.cc:91] Model names: riva_detokenize, Model version: 1\n",
      "I0622 13:26:55.183147   188 model_registry.cc:91] Model names: riva_ner, Model version: 1\n",
      "I0622 13:26:55.183925   188 model_registry.cc:104] 'Successfully registering riva_ner'\n",
      "I0622 13:26:55.183964   188 model_registry.cc:91] Model names: riva_ner_label_tokens, Model version: 1\n",
      "I0622 13:26:55.184576   188 model_registry.cc:91] Model names: riva_tokenizer, Model version: 1\n",
      "I0622 13:26:55.185225   188 model_registry.cc:109] Successfully registered: 2 models.\n",
      "I0622 13:26:55.185240   188 grpc_riva_nlp.cc:33] NLPService GRPC service started\n",
      "I0622 13:26:55.185488   188 riva_server.cc:91] NLP Service connected to Triton at localhost:8001\n",
      "I0622 13:26:55.185501   188 riva_server.cc:93] ASR Service connected to Triton at localhost:8001\n",
      "I0622 13:26:55.185505   188 riva_server.cc:96] Riva Conversational AI Server listening on 0.0.0.0:50051\n"
     ]
    }
   ],
   "source": [
    "# Check available services\n",
    "!docker logs riva-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.2 Update and Start Riva Contact App\n",
    "Take a look at the configuration file for the contact app.  Note the NER entities listed by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Configuration environment vars for Riva Contact\n",
      "\n",
      "# Replace the IP and port with your hosted RIVA endpoint\n",
      "RIVA_API_URL=\"0.0.0.0:50051\"\n",
      "# NER model to use. This one is the default from the Riva Quick Start setup\n",
      "RIVA_NER_MODEL=\"riva_ner\"\n",
      "# NER entities to use from the above model (can be a subset of what is offered)\n",
      "RIVA_NER_ENTITIES=\"per,loc,org,time,misc\"\n",
      "#RIVA_NER_ENTITIES=\"amenity,dish,hours,location,price,rating,restaurant_name\"\n",
      "\n",
      "# The port your Node.js app will be hosted at\n",
      "PORT=\"8009\"\n",
      "# Port for the peer-js server, to be used for negotiating the peer-to-peer chat connection\n",
      "PEERJS_PORT=\"9000\"\n"
     ]
    }
   ],
   "source": [
    "!cat contact-app/env.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2.1 Exercise: Update `env.txt`\n",
    "\n",
    "The first step is to update the entities that will be visually identified in the application.\n",
    "Modify the [contact-app/env.txt](contact-app/env.txt) file section on entities by changing the comments to the correct restaurant context model.  Check your work against the [solution](solutions/ex8.5.2.1.txt) before starting the web app server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modify env.txt so that this cell verifies changes are correct\n",
    "# There should be no output if the files match\n",
    "!diff contact-app/env.txt solutions/ex8.5.2.1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2.2 Start the Contact Web Server\n",
    "To start the web service, open a JupyterLab terminal.  You can do this by first opening the JupyterLab Launcher (small '+' sign at the top of the file browser) and clicking the \"Terminal\" icon.  Next, enter the following in the terminal to start the app server:  \n",
    "\n",
    "```sh\n",
    "cd /dli/task/contact-app\n",
    "npm install\n",
    "npm run start\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"\", data-commandlinker-command=\"terminal:create-new\">Open Terminal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell, then click the link to open a terminal\n",
    "# Enter the commands provided above in the terminal window to start the web server\n",
    "from IPython.display import HTML\n",
    "HTML('<a href=\"\", data-commandlinker-command=\"terminal:create-new\">Open Terminal</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal window, you should see that the server has started running on port 8009:\n",
    "\n",
    "<img src=\"images/asr/webserver_running.png\">\n",
    "\n",
    "After you have started the server, execute the following cell to create a link to open the app! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const href = window.location.hostname + '/app/';\n",
       "let a = document.createElement('a');\n",
       "let link = document.createTextNode('Open Riva Contact!');\n",
       "a.appendChild(link);\n",
       "a.href = \"http://\" + href;\n",
       "a.style.color = \"navy\"\n",
       "a.target = \"_blank\"\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "const href = window.location.hostname + '/app/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Riva Contact!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Browser Restrictions Reminder\n",
    "\n",
    "To use the web app, access to your microphone is required (camera is optional).<br>\n",
    "Several browsers restrict camera/microphone access to applications served from a secure origin (HTTPS or local IP).\n",
    "For your own development purposes, you can set up a self-signed certificate or proxy.  \n",
    "\n",
    "Some browsers provide a way to treat specific URLs as secure:\n",
    "- Chrome browser: <br>\n",
    "Configure the \"treat insecure origin as secure\" flag by adding the application URL on the following page: <br>\n",
    "   ***chrome://flags/#unsafely-treat-insecure-origin-as-secure***<br>\n",
    "(Copy and paste this \"chrome://\" link to a tab on your browser to open the page) <br>\n",
    "You'll see the flag with a text window at the top of the page.  Add your own course URL to the box.  Here is an example (your URL is different).<br>\n",
    "More discussion can be found in [this blog](https://medium.com/@Carmichaelize/enabling-the-microphone-camera-in-chrome-for-local-unsecure-origins-9c90c3149339).\n",
    "\n",
    "<img src=\"images/asr/chrome_override_example.png\">\n",
    "\n",
    "- Safari browser: <br>\n",
    "Enable in the menu: Develop > WebRTC > Allow Media Capture on Insecure Sites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ner/riva_contact_ner2.PNG\" width=800px>\n",
    "\n",
    "Go to the application URL. Once the page is loaded, you're welcome to start the Riva transcription.\n",
    "After you open the app, you may see a \"Lost connection to server\" alert.  Just click \"Ok\".  Other than that, your initial view should look like the following:\n",
    "In the box titled \"Riva transcription,\" hit the Start button, then start speaking. You'll see in-progress transcripts in the text field at the bottom. As those transcripts are finalized, they'll appear, with NER annotations, in the transcription box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.3 Stop Riva Services "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down docker containers...\n"
     ]
    }
   ],
   "source": [
    "# Shut down Riva \n",
    "!bash $RIVA_QS/riva_stop.sh\n",
    "# Shut down web app\n",
    "!pkill -9 node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Exported the NER model to a RMIR format\n",
    "- Configured and exposed NLP Riva services \n",
    "- Requested NLP service using a Python client API\n",
    "- Demonstrated a custom NER model with ASR streaming yourself!\n",
    "\n",
    "This concludes the NER portion of the course.<br>\n",
    "Next, you'll work with deployment of Riva at scale using Kubernetes, starting with [Enabling GPU within Kubernetes](009_K8s_Enable.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
