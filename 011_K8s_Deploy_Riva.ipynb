{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9d1f94",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b51e5",
   "metadata": {},
   "source": [
    "# 11.0 Deploying Riva Services within a Kubernetes Cluster and Further Riva API Examples \n",
    "## (part of Lab 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4fece5",
   "metadata": {},
   "source": [
    "In this notebook, you'll deploy NVIDIA Riva within Kubernetes, and try some API queries for text-to-speech (TTS) and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15adfb4",
   "metadata": {},
   "source": [
    "**[11.1 Deploy NVIDIA Riva](#11.1-Deploy-NVIDIA-Riva)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.1.1 Exercise: Configure Helm Values and Deploy](#11.1.1-Exercise:-Configure-Helm-Values-and-Deploy)<br>\n",
    "**[11.2 Riva Services](#11.2-Riva-Services)<br>**\n",
    "**[11.3 Riva TTS Example](#11.3-Riva-TTS-Example)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.3.1 Exercise: Pod IP with Port 50051](#11.3.1-Exercise:-Pod-IP-with-Port-50051)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.3.2 Exercise: LoadBalancer IP with Port 50051](#11.3.2-Exercise:-LoadBalancer-IP-with-Port-50051)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.3.3 Exercise: Localhost with Mapped Port](#11.3.3-Exercise:-Localhost-with-Mapped-Port)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.3.4 Upgrade the Service with Helm](#11.3.4-Upgrade-the-Service-with-Helm)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[11.3.4.1 Exercise: Upgrade the Service Type to NodePort](#11.3.4.1-Exercise:-Upgrade-the-Service-Type-to-NodePort)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[11.3.4.2 Verify the Upgrade](#11.3.4.2-Verify-the-Upgrade)<br>\n",
    "**[11.4 Riva NLP Examples](#11.4-Riva-NLP-Examples)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.4.1 `AnalyzeIntent` API](#11.4.1-AnalyzeIntent-API)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.4.2 `TextTransform` API](#11.4.2-TextTransform-API)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.4.3 Shutdown](#11.4.3-Shutdown)<br>\n",
    "\n",
    "In the previous parts of the class, you have deployed Riva using very basic shell commands. \n",
    "You have also deployed a basic CUDA application to a Kubernetes cluster.\n",
    "Now it is time to put it all together and deploy Riva into production!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479fed1",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "1. The steps in this notebook assume that you are starting with a K8s cluster that is GPU enabled with feature discovery.  Let's ensure that by stopping and restarting the a cluster and bringing it to a known state. \n",
    "2. As with earlier NVIDIA Riva deployments, you need NGC API credentials.  In this case, you'll also need your email address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f86eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete and restart K8s\n",
    "!minikube delete\n",
    "!minikube start --driver=none\n",
    "# Install the GPU device plugin with Helm\n",
    "!helm repo add nvdp https://nvidia.github.io/k8s-device-plugin \\\n",
    "    && helm repo update\n",
    "!helm install \\\n",
    "    --generate-name nvdp/nvidia-device-plugin\n",
    "# Install GPU feature discovery with Helm\n",
    "!helm repo add nvgfd https://nvidia.github.io/gpu-feature-discovery \\\n",
    "    && helm repo update\n",
    "!helm install \\\n",
    "    --version=0.4.1 \\\n",
    "    --generate-name nvgfd/gpu-feature-discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your personal API key and email address (valid in the scope of this notebook)\n",
    "NGC_API_KEY = \"YOUR_NGC_API_KEY\"\n",
    "NGC_EMAIL = \"YOUR_EMAIL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49127bbe",
   "metadata": {},
   "source": [
    "---\n",
    "# 11.1 Deploy NVIDIA Riva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675e083",
   "metadata": {},
   "source": [
    "To deploy NVIDIA Riva on Kubernetes, start by fetching `riva-api` with Helm, and examining the assets downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43313048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch riva-api with Helm\n",
    "# You may already have fetched this earlier in the course through Riva\n",
    "!helm fetch https://helm.ngc.nvidia.com/nvidia/riva/charts/riva-api-1.4.0-beta.tgz \\\n",
    "    --username='$oauthtoken' --password=$NGC_API_KEY --untar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc43ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l riva-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b468dd8",
   "metadata": {},
   "source": [
    "The configuration file, `values.yaml` contains a number of settings for the service including image details, credentials, and service type.  It also contains a list of ASR, NLP, and TTS models that will be downloaded and optimized upon initialization under `ngcModelConfigs:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6661e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat riva-api/values.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62b721",
   "metadata": {},
   "source": [
    "The Helm Chart starts two containers:\n",
    "* `riva-model-init` - Responsible for fetching all of the model assets configured in `values.yaml` and their optimization for the target platform (appropriate TensorRT optimization will be executed).  After initialization is complete, this container will self-terminate.\n",
    "* `riva-speech-api` - Hosts Riva services after initialization is complete. \n",
    "\n",
    "Before proceeding, we'll need to make some edits to set the configurations in `values.yaml` to match our environment and limit the models deployed.  If we deploy all the possible models, we may run out of memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b867cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where Riva models are located in our class environment\n",
    "!ls -al /dli/task/riva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8bc593",
   "metadata": {},
   "source": [
    "## 11.1.1 Exercise: Configure Helm Values and Deploy\n",
    "Modify the YAML file for our environment and deploy `riva-api` with Helm.  For our environment, the host path location for Riva `models`, `rmir`, and `artifacts` is `/dli/task/riva`.  We also need to comment out all of the models listed to avoid unnecessary deployments as we already have our models we need in the `/dli/task/riva` directory.\n",
    "\n",
    "Exercise:\n",
    "* Open the [values.yaml](riva-api/values.yaml) config file\n",
    "* Comment out all uncommented models under `ngcModelConfigs:`\n",
    "* Modify the `modelDeployVolume.hostPath.path` to reflect our environment\n",
    "* Modify `artifactDeployVolume.hostPath.path` to reflect our environment\n",
    "* Save the file\n",
    "* Check your work against the [solution](solutions/ex11.1.1.yaml) before moving on\n",
    "* Deploy it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modify values.yaml so that this cell verifies changes are correct\n",
    "# Check your work - your file should have the same uncommented models (none!) and folder paths as the solution!\n",
    "print(\"YOUR SETTINGS\\n=============\")\n",
    "!cat riva-api/values.yaml | grep -v \"^\\s*[#;]\" | sed -n '/ngcModelConfigs:/,/modelDeployVolume:/p' | sed ';$d'\n",
    "!cat riva-api/values.yaml | grep -v \"^\\s*[#;]\" | grep -A 20 modelDeployVolume: | grep 'DeployVolume\\|path'\n",
    "print(\"\\nSOLUTION SETTINGS\\n=================\")\n",
    "!cat solutions/ex11.1.1.yaml | grep -v \"^\\s*[#;]\" | sed -n '/ngcModelConfigs:/,/modelDeployVolume:/p' | sed ';$d'\n",
    "!cat solutions/ex11.1.1.yaml | grep -v \"^\\s*[#;]\" | grep -A 20 modelDeployVolume: | grep 'DeployVolume\\|path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3da3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env model_key_string=tlt_encode\n",
    "\n",
    "!helm install riva-api \\\n",
    "    --generate-name \\\n",
    "    --set ngcCredentials.password=`echo -n $NGC_API_KEY | base64 -w0` \\\n",
    "    --set ngcCredentials.email=$NGC_EMAIL \\\n",
    "    --set modelRepoGenerator.modelDeployKey=`echo -n model_key_string | base64 -w0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378619fb",
   "metadata": {},
   "source": [
    "---\n",
    "# 11.2 Riva Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfa2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kubectl describe pods riva-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b5745",
   "metadata": {},
   "source": [
    "At first, the models are downloading (this is reflected in the status), so we have to wait. Wait a minute and look at the status again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf45fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe pods riva-api | grep -A 2 'Containers:\\|State:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d154bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kubectl describe pods riva-api "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58bafad",
   "metadata": {},
   "source": [
    "We need to wait until the status of the `riva-model-init` container changes from \"Waiting\" to \"Running\". You can keep executing the previous command to check as many times as needed.  Once `riva-model-init` is \"Running\", we should be able to view the Docker container logs. We need the name of the pod to view the logs, which we'll grab with a Linux `grep` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13ecff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Grab the name\n",
    "RIVA_API_LONGNAME=$(kubectl describe pods riva-api | grep \"Name:         riva-api-\" | awk '{print $2}')\n",
    "echo \"The pod name is $RIVA_API_LONGNAME\"\n",
    "# Check the logs\n",
    "kubectl logs $RIVA_API_LONGNAME --container=riva-model-init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed6b3ca",
   "metadata": {},
   "source": [
    "The logs should say that the models are already deployed and optimized and that the initialization has finished.  For example, they should consist of lines like:\n",
    "\n",
    "```\n",
    "    Directory rmir_text_classification_v1.0.0-b.1 already exists, skipping. Use '--force' option to override.\n",
    "    Directory rmir_named_entity_recognition_v1.0.0-b.1 already exists, skipping. Use '--force' option to override.\n",
    "    Directory rmir_riva_tts_ljspeech_v1.0.0-b.1 already exists, skipping. Use '--force' option to override.\n",
    "    /opt/riva\n",
    "    2021-05-17 17:25:37,336 [INFO] Writing Riva model repository to ...\n",
    "    2021-05-17 17:25:37,336 [INFO] The riva model repo target directory is /data/models\n",
    "    2021-05-17 17:25:39,892 [WARNING] /data/models/riva_tokenizer already exists, skipping deployment.\n",
    "    2021-05-17 17:25:39,892 [WARNING] /data/models/riva-trt-riva_intent_weather-nn-bert-base-uncased already exists, skipping deployment. \n",
    "```\n",
    "    \n",
    "Troubleshooting note:<br>\n",
    "If there is a mistake in the path configuration, then the initialization container will attempt to download all of the assets. The models take aproximately 6GB of space and their target-specific optimization is a non-trivial task.  Therefore, this step can take 45+ minutes. If the logs are saying that Riva is downloading models, you can uninstall this helm deployment by executing `!helm uninstall riva-api`, correct the [values.yaml](riva-api/values.yaml) file, and try deploying again. \n",
    "\n",
    "When Riva model initialization is complete, Riva services will initialize. This can also take a while as all models need to be loaded to memory and verified, and there are quite a few models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184aad6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -l riva/models\n",
    "!du -sh riva/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2353f8",
   "metadata": {},
   "source": [
    "Check to see if the service container, `riva-speech-api` is running yet.<br>\n",
    "Once it is, take a look at the logs for the container.  The logs should list all the models loaded and confirm that \"Riva Conversational AI Server listening on 0.0.0.0:50051\" in the last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ee77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat execution of this cell until riva-speech-api is \"Running\" and \"Ready\"\n",
    "!kubectl describe pods riva-api | grep -A 2 'Containers:\\|State:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ade210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Grab the name\n",
    "RIVA_API_LONGNAME=$(kubectl describe pods riva-api | grep \"Name:         riva-api-\" | awk '{print $2}')\n",
    "echo \"The pod name is $RIVA_API_LONGNAME\"\n",
    "# Check the logs\n",
    "kubectl logs $RIVA_API_LONGNAME --container=riva-speech-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdbad30",
   "metadata": {},
   "source": [
    "---\n",
    "# 11.3 Riva TTS Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7302e54",
   "metadata": {},
   "source": [
    "If you have observed \"Riva Conversational AI Server listening on 0.0.0.0:50051\" in the logs, we are ready to run an application. We will query the API with a TTS example. <br>\n",
    "First, import the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933680d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import librosa\n",
    "from time import time\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "import requests\n",
    "\n",
    "# NLP proto\n",
    "import riva_api.riva_nlp_pb2 as rnlp\n",
    "import riva_api.riva_nlp_pb2_grpc as rnlp_srv\n",
    "\n",
    "# ASR proto\n",
    "import riva_api.riva_asr_pb2 as rasr\n",
    "import riva_api.riva_asr_pb2_grpc as rasr_srv\n",
    "\n",
    "# TTS proto\n",
    "import riva_api.riva_tts_pb2 as rtts\n",
    "import riva_api.riva_tts_pb2_grpc as rtts_srv\n",
    "import riva_api.riva_audio_pb2 as ra "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139640f",
   "metadata": {},
   "source": [
    "Configure the connection to our server. As you might recall, the service is listening on port 50051. Lets try configuring localhost:50051.  Call the app and output an audio file to listen to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5fd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = grpc.insecure_channel('localhost:50051')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f265a8",
   "metadata": {},
   "source": [
    "Next, we'll create a little function that sets the channel and submits a line of text to the `SynthesizeSpeech` model and returns an audio sample.  Then run the audio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984bde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tts(input_channel, input_text):\n",
    "    riva_tts = rtts_srv.RivaSpeechSynthesisStub(input_channel)\n",
    "    \n",
    "    req = rtts.SynthesizeSpeechRequest()\n",
    "    req.text = input_text\n",
    "    req.language_code = \"en-US\"                    # currently required to be \"en-US\"\n",
    "    req.encoding = ra.AudioEncoding.LINEAR_PCM     # Supports LINEAR_PCM, FLAC, MULAW and ALAW audio encodings\n",
    "    req.sample_rate_hz = 22050                     # ignored, audio returned will be 22.05KHz\n",
    "    req.voice_name = \"ljspeech\"                    # ignored\n",
    "\n",
    "    resp = riva_tts.Synthesize(req)\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.float32)\n",
    "    return audio_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461343b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipd.Audio(test_tts(channel, \"Is it recognize speech or wreck a nice beach?\"), rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861925c",
   "metadata": {},
   "source": [
    "Well, that didn't work... Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0223a6e8",
   "metadata": {},
   "source": [
    "## 11.3.1 Exercise: Pod IP with Port 50051\n",
    "\n",
    "When running Riva from within Kubernetes, our \"localhost\" IP (127.0.0.1) is not connected to the Riva services.  There are a few different pathways we could use to send our request.  The first is to select the Riva API pod IP address and send our requests there. The IP is listed in the pod description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pod -o wide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051487e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get service --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329bac8",
   "metadata": {},
   "source": [
    "Replace the `POD_IP` with the actual IP value in the next cell and try it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO replace the POD_IP\n",
    "channel = grpc.insecure_channel('POD_IP:50051')\n",
    "\n",
    "ipd.Audio(test_tts(channel, \"Is it recognize speech or wreck a nice beach?\"), rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234702b0",
   "metadata": {},
   "source": [
    "Did that work?  There is another way as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa22a48",
   "metadata": {},
   "source": [
    "## 11.3.2 Exercise: LoadBalancer IP with Port 50051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d87c92",
   "metadata": {},
   "source": [
    "Alternatively, we could use the load balancer IP that is set up with a 50051 port mapping for requests.  \n",
    "\n",
    "Replace the `LOADBALANCER_IP` with the actual value in the next cell and try it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO replace the LOADBALANCER_IP\n",
    "channel = grpc.insecure_channel('LOADBALANCER_IP:50051')\n",
    "\n",
    "ipd.Audio(test_tts(channel, \"Is it recognize speech or wreck a nice beach?\"), rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edf52b",
   "metadata": {},
   "source": [
    "## 11.3.3 Exercise: Localhost with Mapped Port\n",
    "Connect to the external facing port mapped from the load balancer to localhost. In this case, this port is assigned randomly, so lets check what it is by looking at the port mapped to 50051 in the services list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e52618",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3954b9e",
   "metadata": {},
   "source": [
    "Replace the `MAPPED_PORT` with the actual value in the next cell and try it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a527cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO replace the MAPPED_PORT\n",
    "channel = grpc.insecure_channel('localhost:MAPPED_PORT')\n",
    "\n",
    "ipd.Audio(test_tts(channel, \"Is it recognize speech or wreck a nice beach?\"), rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60176754",
   "metadata": {},
   "source": [
    "## 11.3.4 Upgrade the Service with Helm\n",
    "Load balancing is used to distribute tasks over a set of compute resources.  Since we have just one GPU and pod in our example, we do not need the load balancer.  We can turn it off by changing the service type in the `values.yaml` file executing the upgrade command. Here's what we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accabca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./riva-api/values.yaml | grep -A 3 service:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ef570",
   "metadata": {},
   "source": [
    "The [`helm upgrade` command](https://helm.sh/docs/helm/helm_upgrade/) has the form:\n",
    "\n",
    "```\n",
    "helm upgrade [RELEASE] [CHART] [flags]\n",
    "```\n",
    "\n",
    "   * CHART is the archive location of the `chart.yaml` file, `riva-api`\n",
    "   * RELEASE is be the specific name of the riva-api service deployed. \n",
    "   \n",
    "RELEASE is listed in the services names, so we can grab it from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf333be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Show the RELEASE value\n",
    "RELEASE=$(kubectl get svc -A | grep \"riva-api\"| awk '{print $2}')\n",
    "echo $RELEASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca15c81",
   "metadata": {},
   "source": [
    "### 11.3.4.1 Exercise: Upgrade the Service Type to NodePort\n",
    "Modify the YAML file for to change the service type from `LoadBalancer` to `NodePort` and upgrade it with Helm.\n",
    "\n",
    "Exercise:\n",
    "* Open the [values.yaml](riva-api/values.yaml) config file\n",
    "* Modify the \"service.type\" to \"NodePort\"\n",
    "* Save the file\n",
    "* Check your work against the [solution](solutions/ex11.3.4.1.yaml) before moving on\n",
    "* Upgrade the service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modify values.yaml so that this cell verifies changes are correct\n",
    "# Check your work - your file should have the values as the solution!\n",
    "print(\"YOUR SETTING\")\n",
    "!cat ./riva-api/values.yaml | grep -A 3 service:\n",
    "print(\"\\nSOLUTION SETTING\")\n",
    "!cat solutions/ex11.3.4.1.yaml | grep -A 3 service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904dad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "RELEASE=$(kubectl get svc -A | grep \"riva-api\"| awk '{print $2}')\n",
    "helm upgrade $RELEASE riva-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b55ba",
   "metadata": {},
   "source": [
    "### 11.3.4.2 Verify the Upgrade\n",
    "Since we have configured port 32222 as our NodePort, we should see the change now in our live service list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699d256",
   "metadata": {},
   "source": [
    "As a consequence, we have a known IP:PORT value to reliably expose the Riva server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = grpc.insecure_channel('localhost:32222')\n",
    "\n",
    "ipd.Audio(test_tts(channel, \"Is it recognize speech or wreck a nice beach?\"), rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0f98f",
   "metadata": {},
   "source": [
    "What did the code actually do? It executed a request to a TTS service transcribing the sentence provided, then generated an audio file with the transcript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9f308",
   "metadata": {},
   "source": [
    "---\n",
    "# 11.4 Riva NLP Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35adf3e2",
   "metadata": {},
   "source": [
    "In the TTS example, we used the `SynthesizeSpeechRequest` API to synthesize speech.  We can similarly make a requests with other APIs and we'll try a couple of the NLP examples.  You can find more in the [Riva Speech Skills documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/notebooks/Riva_speech_API_demo.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c30bed",
   "metadata": {},
   "source": [
    "## 11.4.1 `AnalyzeIntent` API\n",
    "The `AnalyzeIntent` API can be used to query an \"intent slot\" classifier. If we don't have a specific domain, this API can be leveraged with an additional text classification model to classify the domain of the input query before routing the text to the appropriate intent slot model.\n",
    "\n",
    "We'll keep things simple and use an example where the domain is known. This example skips execution of the domain classifier\n",
    "and proceeds directly to the intent slot model for the requested domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = grpc.insecure_channel('localhost:32222')\n",
    "riva_nlp = rnlp_srv.RivaLanguageUnderstandingStub(channel)\n",
    "req = rnlp.AnalyzeIntentRequest()\n",
    "req.query = \"How is the humidity in San Francisco?\"\n",
    "req.options.domain = \"weather\"  # The <domain_name> is appended to \"riva_intent_\" to look for a\n",
    "                                # model \"riva_intent_<domain_name>\". So in this e.g., the model \"riva_intent_weather\"\n",
    "                                # needs to be preloaded in riva server. If you would like to deploy your\n",
    "                                # custom Joint Intent and Slot model use the `--domain_name` parameter in\n",
    "                                # ServiceMaker's `riva-build intent_slot` command.\n",
    "\n",
    "resp = riva_nlp.AnalyzeIntent(req)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some weather Intent queries\n",
    "queries = [\n",
    "    \"Is it currently cloudy in Tokyo?\",\n",
    "    \"What is the annual rainfall in Pune?\",\n",
    "    \"What is the humidity going to be tomorrow?\"\n",
    "]\n",
    "for q in queries:\n",
    "    req = rnlp.AnalyzeIntentRequest()\n",
    "    req.query = q\n",
    "    start = time()\n",
    "    resp = riva_nlp.AnalyzeIntent(req)\n",
    "\n",
    "    print(f\"[{resp.intent.class_name}]\\t{req.query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e49525",
   "metadata": {},
   "source": [
    "## 11.4.2 `TextTransform` API\n",
    "We can use this API to run the punctuation and capitalization model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026fc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TextTransform API to run the punctuation and capitalization model\n",
    "channel = grpc.insecure_channel('localhost:32222')\n",
    "riva_nlp = rnlp_srv.RivaLanguageUnderstandingStub(channel)\n",
    "\n",
    "req = rnlp.TextTransformRequest()\n",
    "req.model.model_name = \"riva_punctuation\"\n",
    "req.text.append(\"add punctuation to this sentence\")\n",
    "req.text.append(\"do you have any red nvidia shirts\")\n",
    "req.text.append(\"i need one cpu four gpus and lots of memory \"\n",
    "                \"for my new computer it's going to be very cool\")\n",
    "\n",
    "nlp_resp = riva_nlp.TransformText(req)\n",
    "print(\"TransformText Output:\")\n",
    "print(\"\\n\".join([f\" {x}\" for x in nlp_resp.text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cb171",
   "metadata": {},
   "source": [
    "## 11.4.3 Shutdown\n",
    "Clean up your environment by shutting down Riva and K8s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down K8s\n",
    "!minikube delete\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8feaa",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Deployed Riva on K8s\n",
    "- Queried the TTS API, `SynthesizeSpeechRequest`\n",
    "- Learned how to access the Riva server from various IP:Port combinations\n",
    "- Queried the `AnalyzeIntent` and `TextTransform` NLP APIs\n",
    "\n",
    "Now that you've finished the hands-on portion of the course, you can work on the assessments to test your understanding and obtain a certificate!  Move on to the assessment questions in the course dashboard or the [coding assessment notebook](assessment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2fbc9",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
